<!DOCTYPE html>
<html lang="en"><head>

  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Bernoulli Diffusion Derivations | The Horenberger Zone</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Bernoulli Diffusion Derivations" />
<meta name="author" content="Beau Horenberger" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What is Bernoulli Diffusion? What is Bernoulli Diffusion?" />
<meta property="og:description" content="What is Bernoulli Diffusion? What is Bernoulli Diffusion?" />
<link rel="canonical" href="http://localhost:4000/2022/10/03/bernoulliderivations.html" />
<meta property="og:url" content="http://localhost:4000/2022/10/03/bernoulliderivations.html" />
<meta property="og:site_name" content="The Horenberger Zone" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-10-03T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bernoulli Diffusion Derivations" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Beau Horenberger"},"headline":"Bernoulli Diffusion Derivations","dateModified":"2022-10-03T00:00:00-04:00","datePublished":"2022-10-03T00:00:00-04:00","description":"What is Bernoulli Diffusion? What is Bernoulli Diffusion?","url":"http://localhost:4000/2022/10/03/bernoulliderivations.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/10/03/bernoulliderivations.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="The Horenberger Zone" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  
</head>
<body><header class="site-header">
  <div class="wrapper"><a class="site-title" rel="author" href="/">The Horenberger Zone</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/">Home</a><a class="page-link" href="/about.html">Bio</a><a class="page-link" href="/contact.html">Contact</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bernoulli Diffusion Derivations</h1>
    <p class="post-meta"><time class="dt-published" datetime="2022-10-03T00:00:00-04:00" itemprop="datePublished">
        Oct 3, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="markdown-toc">
  <li><a href="#what-is-bernoulli-diffusion" id="markdown-toc-what-is-bernoulli-diffusion">What is Bernoulli Diffusion?</a></li>
  <li><a href="#who-is-this-article-for" id="markdown-toc-who-is-this-article-for">Who is this article for?</a></li>
  <li><a href="#prependix" id="markdown-toc-prependix">Prependix</a></li>
  <li><a href="#kl-divergence-of-multivariate-bernoulli-distributions" id="markdown-toc-kl-divergence-of-multivariate-bernoulli-distributions">KL divergence of Multivariate Bernoulli Distributions</a></li>
  <li><a href="#tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right" id="markdown-toc-tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right">$\tilde{\beta}_t$ for Calculating $q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)$</a></li>
  <li><a href="#calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right" id="markdown-toc-calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right">Calculating the Posterior $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$</a></li>
  <li><a href="#entropy-of-multivariate-bernoulli-distributions" id="markdown-toc-entropy-of-multivariate-bernoulli-distributions">Entropy of Multivariate Bernoulli Distributions</a></li>
</ul>
<h2 id="what-is-bernoulli-diffusion">What is Bernoulli Diffusion?</h2>

<p>Short answer: <a href="https://github.com/horenbergerb/BernoulliDiffusion">a Github repo that I made.</a></p>

<p>Long answer:</p>

<p>Diffusion originated from the classic publication <a href="https://arxiv.org/abs/1503.03585">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a> by Sohl-Dickstein et al.</p>

<p>In the original paper–as well as most practical applications of diffusion (see <a href="https://arstechnica.com/information-technology/2022/09/with-stable-diffusion-you-may-never-believe-what-you-see-online-again/">stable-diffusion</a>)–a Gaussian Markov diffusion kernel is used. This means the algorithm iteratively applies Gaussian noise to samples from the target distribution and then learns to reverse the noising process.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/notesonbernoullidiffusion/gaussiandiffusionillustration.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>An illustration of Gaussian diffusion stolen from <a href="https://arxiv.org/abs/2006.11239">this paper</a>. You’re legally required to put this in any technical article about diffusion.</em></td>
    </tr>
  </tbody>
</table>

<p>However, the original paper offhandedly mentions that you can also implement diffusion for binary-valued data using a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> as your Markov diffusion kernel.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><img src="/images/notesonbernoullidiffusion/originalpaperbernoullidiffusion.png" alt="" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><em>A segment of the original diffusion paper which mentions using Bernoulli diffusion to learn “heartbeat data”.</em></td>
    </tr>
  </tbody>
</table>

<p>The authors of the paper <a href="https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/tree/master">published the code for the Gaussian diffusion</a>, but for some reason chose not to publish the Bernoulli diffusion model. I have yet to find an implementation of Bernoulli diffusion anywhere on the internet.</p>

<p><a href="https://github.com/horenbergerb/BernoulliDiffusion">So I decided to implement it myself</a>. It was very hard. I calculated a lot of things and learned a lot of fancy new words.</p>

<p>Here I will be detailing some of the more interesting derivations that I haven’t seen documented anywhere else.</p>

<h2 id="who-is-this-article-for">Who is this article for?</h2>

<p>Me. I need to dump this all somewhere so that I can finally forget it. However, it might help if you are trying to understand the code in the Bernoulli Diffusion repo. One day in the distant future, these notes might even help college students cheat on their homework.</p>

<h2 id="prependix">Prependix</h2>

<p>Most of these calculations have to do with the terms involved in this the loss approximation (equation 14 in the paper):</p>

\[K = -\sum_{t=2}^T\int d\mathbf{x}^{(0)} d\mathbf{x}^{(t)} q\left(\mathbf{x}^{(0)}, \mathbf{x}^{(t)} \right)\cdot \\
D_{KL}\left(q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \vert \vert p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)\right) \\
+ H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(1)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(T)})\]

<p>So I figured I’d prepend it here for reference later.</p>

<h2 id="kl-divergence-of-multivariate-bernoulli-distributions">KL divergence of Multivariate Bernoulli Distributions</h2>

<p>In order to calculate $K$, we will need to calculate</p>

\[D_{KL}\left(q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \vert \vert p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)\right)\]

<p>Where $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$ and $p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)$ are both essentially a bunch of Bernoulli distributions.</p>

<p>I actually found the answer <a href="https://math.stackexchange.com/questions/2604566/kl-divergence-between-two-multivariate-bernoulli-distribution">here</a>, so I’m not going to elaborate on this one.</p>

<h2 id="tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right">$\tilde{\beta}_t$ for Calculating $q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)$</h2>

<p>Okay, so in <a href="https://arxiv.org/abs/2006.11239">this paper</a> the authors casually remark that the forward process in Gaussian diffusion has a really nice computational property. Specifically, if</p>

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)} \right) =\mathcal{N}\left(\mathbf{x}^{(t)};\sqrt{1-\beta_t}\mathbf{x}^{(t-1)}, \beta_t\mathbf{I}\right)\]

<p>then you can define $\alpha_t := 1-\beta_t$ and $\bar{\alpha_t} := \prod_{s=1}^{t} \alpha_{s}$, and it follows that</p>

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right) =\mathcal{N}\left(\mathbf{x}^{(t)};\sqrt{\bar{\alpha}_t}\mathbf{x}^{(0)}, (1-\bar{\alpha}_t)\mathbf{I}\right)\]

<p>So, in other words, you don’t have to iterate the forward noising process $t$ times to get a sample $\mathbf{x}^{(t)}$. You can just sample it directly.</p>

<p>I figured something similar held for the Bernoulli case. However, the authors didn’t actually demonstrate how this relationship is derived. I’ll leave the Gaussian case as an exercise and show the Bernoulli case instead.</p>

<p>We can focus on an individual digit $\mathbf{x}^{(t)}_i$. The way I derived this was by denoting</p>

\[\mathbf{x}^{(t)}_i = \mathbf{x}^{(t-1)}_i(1-\epsilon_t) + (1-\mathbf{x}^{(t-1)}_i)\epsilon_t = \mathbf{x}^{(t-1)}_i + \epsilon_t - 2\epsilon_t \mathbf{x}^{(t-1)}_i\]

<p>Where $\epsilon_t = \mathcal{B}(0.5\beta_t)$ is a Bernoulli random variable. This is just another way of representing the typical noising process as a probability of flipping bits.</p>

<p>I used this recursively to represent $\mathbf{x}^{(t+1)}_i$ in terms of $\mathbf{x}^{(t-1)}_i$ and then simplified to find that:</p>

\[\mathbf{x}^{(t+1)}_i = \mathbf{x}^{(t-1)}_i + \epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1} - 2(\epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1})\mathbf{x}^{(t-1)}_i\]

<p>denoting $\tilde{\epsilon}_ {t+1} := \epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1}$, we see that</p>

\[\mathbf{x}^{(t+1)}_i = \mathbf{x}^{(t-1)}_i + \tilde{\epsilon}_{t+1} - 2\tilde{\epsilon}_{t+1}\mathbf{x}^{(t-1)}_i\]

<p>Which is the same relationship as the original case! If we wanted to skip even further, say from $\mathbf{x}^{(t-1)}$ to $\mathbf{x}^{(t+2)}$, then we would derive that</p>

\[\mathbf{x}^{(t+2)}_i = \mathbf{x}^{(t-1)}_i + \tilde{\epsilon}_{t+1} + \epsilon_{t+2} - {\epsilon}_{t+1}\epsilon_{t+2} - 2(\tilde{\epsilon}_{t+1} + \epsilon_{t+2} - {\epsilon}_{t+1}\epsilon_{t+2})\mathbf{x}^{(t-1)}_i\]

<p>So generally we have that</p>

\[\tilde{\epsilon}_ {t+1} = \tilde{\epsilon}_{t} + \epsilon_{t+1} - \tilde{\epsilon}_{t}\epsilon_{t+1}\]

<p>Then the corresponding Bernoulli distribution is</p>

\[\mathcal{B}(0.5(\tilde{\beta}_{t} + \beta_{t+1} - \beta_{t+1}\tilde{\beta}_{t})) := \mathcal{B}(0.5\tilde{\beta}_{t+1})\]

<p>And we have a recursive relationship for our desired values,</p>

\[\tilde{\beta}_{t+1} = \tilde{\beta}_{t} + \beta_{t+1} - \beta_{t+1}\tilde{\beta}_{t}\]

<p>Then</p>

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right) =\mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{t}) + 0.5\tilde{\beta}_{t}\right)\]

<p>Badda bing, badda boom.</p>

<h2 id="calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right">Calculating the Posterior $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$</h2>

<p>Another obstacle while computing the loss $K$ is the posterior,</p>

\[q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)\]

<p>Where $\mathbf{x}$ and $\mathbf{x}^{(t)}$ are given. The first time I saw this, I had no idea how to even interpret it, and the authors never even mention it beyond “oh, yeah, that’s the posterior.”</p>

<p>But contextually, it seems like it should be analytically computable, and in fact you can find a <a href="https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/blob/master/model.py#L203">computation of the posterior in the source code of the Gaussian diffusion repo</a>.</p>

<p>I eventually realized that the authors don’t mention it because it’s trivial to a keen Bayesian statistican. Essentially, you can think of this as a tiny Bayesian model within the larger system. The inspiration came to me while reading <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis, by Andrew Gelman</a>, particularly section 2.5. This is coincidentally almost exactly the solution for the Gaussian posterior.</p>

<p>Usually with a Bayesian system, you construct a sampling model $p(y\vert \theta)$ dependent on the parameters $\theta$. Then you establish a prior distribution on the parameters $p(\theta)$.</p>

<p>When you finally collect data $y$ and want to determine the proper parameters $\theta$, you’ll need to calculate the posterior distribution (or something proportional to it):</p>

\[p(\theta \vert y) = \frac{p(y\vert\theta)p(\theta)}{p(y)} \propto p(y\vert\theta)p(\theta)\]

<p>we can translate this into our current case as follows:</p>

\[y = \mathbf{x}^{(t)} \\
\theta = \mathbf{x}^{(t-1)} \\
p(\theta) = q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{t-1}) + 0.5\tilde{\beta}_{t-1}\right) \\
p(y\vert \theta) = q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)},\mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(t-1)}(1-\beta_{t}) + 0.5\beta_{t}\right) \\
p(\theta\vert y) = q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)\]

<p>Now the given variables, $\mathbf{x}^{(t)}$ and $\mathbf{x}^{(0)}$, determine a prior distribution and a set of observations, so we can get to work calculating the posterior</p>

\[p(\theta \vert y) \propto p(y\vert\theta)p(\theta) = q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)},\mathbf{x}^{(0)} \right)q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(0)} \right)\]

<p>Since we parameterize the Bernoulli distributions using the probability of observing 1, want to calculate $p(\theta =1\vert y)$ and thus we set $\mathbf{x}^{(t-1)} = 1$.</p>

<p>I also ended up calculating the cases for $\mathbf{x}^{(t)} = 1$ and $\mathbf{x}^{(t)} = 0$ separately because it seemed easier. Then I combined them into a single equation:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  posterior = x_0*(1-self.beta_tilde_t[t-1]) + 0.5*self.beta_tilde_t[t-1]
  posterior *= x_t * (1-0.5*beta_t) + (1 - x_t) * (1.5*beta_t)
</code></pre></div></div>

<p>Honestly, I’m still not totally sure if this needs to be normalized or something. I should probably put that on my long list of to-dos.</p>

<h2 id="entropy-of-multivariate-bernoulli-distributions">Entropy of Multivariate Bernoulli Distributions</h2>

<p>This one was a doozy. While calculating that darn $K$ value, we find ourselves in need of the following entropies:</p>

\[H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(1)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(T)})\]

<p>These are entropies of multivariate Bernoulli distributions. So, first thing’s first, the definition of the entropy of a discrete distribution $p(x)$ over $\mathcal{X}$ is</p>

\[H(p(x)) = -\sum_{x\in\mathcal{X}}p(x)\log p(x)\]

<p>Now, for a single Bernoulli distribution with probability $0\leq c \leq 1$, it works out that</p>

\[H(\mathcal{B}(c)) = -(c\log c + (1-c) \log c)\]

<p>But life is not so simple when you get to the multivariate case. If we have $n$ Bernoulli distributions, each with probability $p_i$, then the probability of an outcome $x\in\{0,1\}^n$ is given by</p>

\[p(x) = \prod_{i=1}^n x_i p_i + (1-x_i)(1-p_i)\]

<p>and there are $2^n$ values of $x$ in our entropy summation… If we are learning sequences of length 20, each loss calculation would involve summing $2^{20}=1048576$ products. That is not efficient.</p>

<p>However! There are additional constraints at play which give us hope.</p>

<p>Let’s consider $H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)})$. Recall that</p>

\[q\left(\mathbf{x}^{(T)}\vert \mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{T}) + 0.5\tilde{\beta}_{T}\right)\]

<p>Consequently, each Bernoulli distribution can only have one of two probabilities:</p>

\[p_i =
\begin{cases}
1-0.5\tilde{\beta}_T, &amp; \text{if} &amp; \mathbf{x}^{(0)}_i = 1 \\
0.5\tilde{\beta}_T, &amp; \text{if} &amp; \mathbf{x}^{(0)}_i = 0
\end{cases}\]

<p>It follows almost immediately that $\mathbf{x}^{(T)}_i = \mathbf{x}^{(0)}_i$, then the probability must be $1-0.5\tilde{\beta}_T$, and otherwise the probability of $\mathbf{x}^{(T)}_i$ is $0.5\tilde{\beta}_T$.</p>

<p>In other words, the probability of any outcome $\mathbf{x}^{(T)}$ is totally determined by the number of digits it has in common with $\mathbf{x}^{(0)}$. If they agree in $k$ digits and disagree for the other $k-n$, then the probability of $\mathbf{x}^{(T)}$ is</p>

\[(1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\]

<p>So how many samples will have each probability? a bit of thought will convince you that there is $n$ choose $k$ ways to agree in $k$ digits.</p>

<p>Now, how does this all tie back into entropy? Well, all three entropies are similar, but wrapping up the case of $H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)})$, we can calculate that</p>

\[H(p(x)) = -\sum_{x\in\mathcal{X}}p(x)\log p(x) \\
=-\sum_{k=0}^n {n\choose k}(1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\log \left((1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\right)\]

<p>Easy as.</p>

  </div>

  <div>
        <center> <h3> Share this article: </h3> </center>
    <ul class="social-media-list">
      <li>
        <a rel="me" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2022/10/03/bernoulliderivations.html" target="_blank"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#facebook"></use></svg>
        </a>
      </li>
      <li>
        <a rel="me" href="http://news.ycombinator.com/submitlink?u=http://localhost:4000/2022/10/03/bernoulliderivations.html&t=Bernoulli%20Diffusion%20Derivations" target="_blank"><svg class="svg-icon grey" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414">
            <path d="M 0,0 V 16 H 16 V 0 Z M 1.4545454,1.4545454 H 14.545454 V 14.545454 H 1.4545454 Z m 3.2727273,2.909091 2.5454546,4.3636364 V 12.363636 H 8.7272728 V 8.7272728 L 11.272727,4.3636364 H 9.8181816 L 8,7.4772727 6.1818182,4.3636364 Z"/>
          </svg>
        </a>
      </li>
      <li>
        <a rel="me" href="https://twitter.com/intent/tweet?via=BeauHorenberger&url=http://localhost:4000/2022/10/03/bernoulliderivations.html&text=Bernoulli%20Diffusion%20Derivations" target="_blank"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg>
        </a>
      </li>
    </ul>
    </div><script src="https://giscus.app/client.js"
    data-repo="horenbergerb/horenbergerb.github.io"
    data-repo-id="MDEwOlJlcG9zaXRvcnkyNzc4OTI2NzY="
    data-category="Announcements"
    data-category-id="DIC_kwDOEJBORM4CQ6OK"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="light"
    data-lang="en"
    crossorigin="anonymous"
    async>
  </script>

  <a class="u-url" href="/2022/10/03/bernoulliderivations.html" hidden></a>
</article>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Beau Horenberger</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>Cite me in your thesis</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/horenbergerb" target="_blank" title="horenbergerb"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/beau-horenberger-875487164" target="_blank" title="beau-horenberger-875487164"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/BeauHorenberger" target="_blank" title="BeauHorenberger"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul></div>

  </div>

</footer>
</body>

</html>
