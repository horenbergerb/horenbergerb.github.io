<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-10-07T20:23:51-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Horenberger Zone</title><subtitle>Cite me in your thesis</subtitle><author><name>Beau Horenberger</name></author><entry><title type="html">Can LLaMa Reason? Investigations with Large Language Models and PrOntoQA</title><link href="http://localhost:4000/2023/10/07/canllamareason.html" rel="alternate" type="text/html" title="Can LLaMa Reason? Investigations with Large Language Models and PrOntoQA" /><published>2023-10-07T00:00:00-04:00</published><updated>2023-10-07T00:00:00-04:00</updated><id>http://localhost:4000/2023/10/07/canllamareason</id><content type="html" xml:base="http://localhost:4000/2023/10/07/canllamareason.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction-investigating-llms-at-home-with-llama&quot; id=&quot;markdown-toc-introduction-investigating-llms-at-home-with-llama&quot;&gt;Introduction: Investigating LLMs at home with LLaMa&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#creating-and-evaluating-reasoning-tasks-with-prontoqa&quot; id=&quot;markdown-toc-creating-and-evaluating-reasoning-tasks-with-prontoqa&quot;&gt;Creating and evaluating reasoning tasks with PrOntoQA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#comparing-the-reasoning-capabilities-of-various-llama-models&quot; id=&quot;markdown-toc-comparing-the-reasoning-capabilities-of-various-llama-models&quot;&gt;Comparing the Reasoning Capabilities of Various LLaMa models&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#how-do-different-llama-sizes-compare&quot; id=&quot;markdown-toc-how-do-different-llama-sizes-compare&quot;&gt;How do different LLaMa sizes compare?&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#comparing-proof-correctness&quot; id=&quot;markdown-toc-comparing-proof-correctness&quot;&gt;Comparing Proof Correctness&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#when-is-llama-wrong-failure-modes-for-platypus2-70b-instruct&quot; id=&quot;markdown-toc-when-is-llama-wrong-failure-modes-for-platypus2-70b-instruct&quot;&gt;When is LLaMa wrong? Failure modes for Platypus2-70b-instruct&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#general-statistics&quot; id=&quot;markdown-toc-general-statistics&quot;&gt;General statistics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#case-study-wrong-logical-branches&quot; id=&quot;markdown-toc-case-study-wrong-logical-branches&quot;&gt;Case Study: Wrong Logical Branches&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#case-study-proofs-with-non-atomic-steps&quot; id=&quot;markdown-toc-case-study-proofs-with-non-atomic-steps&quot;&gt;Case Study: Proofs with non-atomic steps.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#case-study-proofs-with-invalid-steps&quot; id=&quot;markdown-toc-case-study-proofs-with-invalid-steps&quot;&gt;Case Study: Proofs with invalid steps&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reflection-is-it-reasoning&quot; id=&quot;markdown-toc-reflection-is-it-reasoning&quot;&gt;Reflection: Is it reasoning?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion-thanks-and-future-work&quot; id=&quot;markdown-toc-conclusion-thanks-and-future-work&quot;&gt;Conclusion, thanks, and future work&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#appendix&quot; id=&quot;markdown-toc-appendix&quot;&gt;Appendix&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#llama--prontoqa-implementation-details&quot; id=&quot;markdown-toc-llama--prontoqa-implementation-details&quot;&gt;LLaMa + PrOntoQA implementation details&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#prompt-example&quot; id=&quot;markdown-toc-prompt-example&quot;&gt;Prompt example&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiment-details&quot; id=&quot;markdown-toc-experiment-details&quot;&gt;Experiment details&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;introduction-investigating-llms-at-home-with-llama&quot;&gt;Introduction: Investigating LLMs at home with LLaMa&lt;/h1&gt;

&lt;p&gt;Large language models (LLMs) are machine-learning algorithms. They are trained on large bodies of text and learn to behave as an autocomplete; they predict a continuation of inputted text.&lt;/p&gt;

&lt;p&gt;Researchers want to know whether LLMs are capable of performing various natural language tasks such as translation, summarization, factual question answering, reasoning, and programming. Further, for a given task, can LLMs produce out-of-distribution solutions? In other words, has the model &lt;em&gt;generally learned&lt;/em&gt; to perform the task, or is it just memorizing the solutions included in its training data?&lt;/p&gt;

&lt;p&gt;These questions are dank, and I think we should study them. Thanks to Meta’s public release of &lt;a href=&quot;https://ai.meta.com/llama/&quot;&gt;LLaMa 2&lt;/a&gt;, I can investigate these questions by running experiments on my own PC. This would have been inconceivable only 8 months ago.&lt;/p&gt;

&lt;p&gt;Meta released 3 sizes of LLaMa 2: 7 billion parameters, 13B parameters, and 70B parameters. For comparison, GPT3.5 is speculated to be 175B parameters. Although it is much smaller, LLaMa 2 70B is not too distant from GPT3.5 in quality. This is probably due to many factors including the amount of pretraining and architecture differences.&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt;, I can run quantizations of the LLaMa models on my GPU (GeForce RTX 3090) and my CPU (Some generic Intel CPU). 70B is somewhat slow (1 token/second), but smaller models are wicked fast (22t/s minimum, I don’t remember exactly).&lt;/p&gt;

&lt;p&gt;These models can also be finetuned relatively cheaply. Huggingface features a huge collection of community-made finetunes which specialize in all kinds of tasks. The most common kind of finetune is “instruct finetunes,” which are trained to act as helpful assistants.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-10-07-canllamareason/platypus_instruct_example.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An example output from Platypus2-70B-Instruct, an instruct finetune of LLaMa 70B.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;All this being said, the stage is set. I’ve got a few base models and a plethora of finetunes at my fingertips. Now what?&lt;/p&gt;

&lt;p&gt;The natural language task I investigate in this article is reasoning. This isn’t a directed study, but rather more of a preliminary investigation. I want to start getting a feel for how LLaMa models handle reasoning tasks. An excellent tool for this kind of investigation is &lt;a href=&quot;https://github.com/asaparov/prontoqa&quot;&gt;PrOntoQA&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;creating-and-evaluating-reasoning-tasks-with-prontoqa&quot;&gt;Creating and evaluating reasoning tasks with PrOntoQA&lt;/h1&gt;

&lt;p&gt;PrOntoQA is a tool for generating reasoning questions and evaluating how LLMs answer them. Here’s an example of a question that PrOntoQA might ask a model:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Every shumpus is transparent. Shumpuses are rompuses. Shumpuses are yumpuses. Each sterpus is not metallic. Each rompus is metallic. Rompuses are lorpuses. Every rompus is a lempus. Each lempus is blue. Yumpuses are temperate. Brimpuses are not happy. Each brimpus is a dumpus. Sam is a shumpus. Sam is a brimpus. &lt;strong&gt;True or false: Sam is not metallic.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To which the answer is, naturally,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sam is a shumpus. Shumpuses are rompuses. Sam is a rompus. Each rompus is metallic. Sam is metallic. &lt;strong&gt;False&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;These little self-contained problems are undeniably reasoning exercises. Better yet, the proof steps, or chain of thought, generated by an LLM can be programmatically interpreted and compared to a gold standard. &lt;a href=&quot;https://arxiv.org/abs/2210.01240&quot;&gt;The original PrOntoQA paper&lt;/a&gt; makes it very clear how this all works, and I highly recommend giving it a look.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-10-07-canllamareason/prontoqa_paper.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A snippet from the PrOntoQA paper about how questions and answers are automatically generated.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For my purposes, though, there are only a few things you need to know.&lt;/p&gt;

&lt;p&gt;1) I have a large collection of self-contained reasoning questions similar to the one shown above.
2) For each question, there is only one proper chain of reasoning to get the correct answer.
3) We can parse the reasoning chains produced by the model and compare them with the correct answer.&lt;/p&gt;

&lt;p&gt;Let’s continue our example from above. Suppose we feed the question into a LLaMa model, and the answer it gives is:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sam is a shumpus. Shumpuses are rompuses. Sam is a rompus. Rompuses are lorpuses. Lorpuses are not metallic. Sam is not metallic. True&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By comparing with the expected answer, we can evaluate each step:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sentence&lt;/th&gt;
      &lt;th&gt;Evaluation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Sam is a shumpus.&lt;/td&gt;
      &lt;td&gt;Correct and useful&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Shumpuses are rompuses.&lt;/td&gt;
      &lt;td&gt;Correct and useful&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sam is a rompus.&lt;/td&gt;
      &lt;td&gt;Correct and useful&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rompuses are lorpuses.&lt;/td&gt;
      &lt;td&gt;Correct but irrelevant. We also call these cases “Wrong branches”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Lorpuses are not metallic.&lt;/td&gt;
      &lt;td&gt;Incorrect. This is a totally unsubstantiated claim.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sam is not metallic.&lt;/td&gt;
      &lt;td&gt;Incorrect. This is valid reasoning, but follows from a previous incorrect claim.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Supposing we can make this kind of evaluation automatically, what insights can we gain about LLaMa and its capacity for reason? This question is the central theme of the rest of the article.&lt;/p&gt;

&lt;p&gt;We prompt the model by including 8 example question/answer pairs before providing the actual question and letting the model generate the answer. These models were &lt;em&gt;not&lt;/em&gt; trained specifically to perform this task. &lt;a href=&quot;#prompt-example&quot;&gt;See the appendix for an example prompt.&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;comparing-the-reasoning-capabilities-of-various-llama-models&quot;&gt;Comparing the Reasoning Capabilities of Various LLaMa models&lt;/h1&gt;

&lt;h2 id=&quot;how-do-different-llama-sizes-compare&quot;&gt;How do different LLaMa sizes compare?&lt;/h2&gt;

&lt;p&gt;This seems a natural starting point for an investigation. For each model size, I chose a high-quality instruct finetune. You might be thinking, “that’s absurd, you can’t just compare random finetunes of each size.” In fact, I can and I will. I chose to use finetunes for a few reasons:&lt;/p&gt;

&lt;p&gt;1) I didn’t have the base models downloaded
2) I figured instruct finetunes are generally better at reasoning and not that different among a fixed parameter size&lt;/p&gt;

&lt;p&gt;I’ll do some work with the base models in a future article, I promise.&lt;/p&gt;

&lt;h3 id=&quot;comparing-proof-correctness&quot;&gt;Comparing Proof Correctness&lt;/h3&gt;

&lt;p&gt;I chose to focus on 3-hop problems, i.e. problems which require 3 logical steps. Here’s an example of a solution for a 3-hop problem:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Polly is a numpus. Numpuses are vumpuses. Polly is a vumpus. Vumpuses are wumpuses. Polly is a wumpus. Wumpuses are not bright. Polly is not bright. True&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I ran 40 problems for each model. Is this a statistically significant amount? I dunno.&lt;/p&gt;

&lt;p&gt;The following plot shows what proportion of proofs by each model were correct.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-10-07-canllamareason/proof_correctness_across_model_size.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A comparison of proof accuracy for different LLaMa model sizes.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The left graph only allows proofs where every step is shown, while the right graph allows proofs which skip steps but are otherwise valid.&lt;/p&gt;

&lt;p&gt;It seems obvious that larger model sizes increase performance substantially. But why? What changes in the behavior to produce better results? And what are the failure modes for each model?&lt;/p&gt;

&lt;h1 id=&quot;when-is-llama-wrong-failure-modes-for-platypus2-70b-instruct&quot;&gt;When is LLaMa wrong? Failure modes for Platypus2-70b-instruct&lt;/h1&gt;

&lt;h2 id=&quot;general-statistics&quot;&gt;General statistics&lt;/h2&gt;

&lt;p&gt;20 out of the 40 proofs generated by Platypus were strictly correct.&lt;/p&gt;

&lt;p&gt;In cases where the proof was wrong, what was the first mistake?&lt;/p&gt;

&lt;p&gt;For 12 incorrect proofs, the first mistake was proceeding down a useless logical branch. In no case did the model ever “backtrack”. Instead, the model would proceed down an irrelevant logical chain. Typically it would begin to hallucinate by citing false facts or using invalid reasoning until eventually concluding the proof. Sometimes, the proof would “prove” the wrong statement entirely, such as proving Rex is metallic when the original question was whether Rex is a tumpus.&lt;/p&gt;

&lt;p&gt;For 6 proofs, the first mistake was a “useful non-atomic step”. What does that mean? Assume the following is true:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Wren is a dumpus. Dumpuses are impuses. Wren is an impus. Impuses are mean.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Writing the same proof as follows is technically incorrect due to including a non-atomic step:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Wren is a dumpus. Dumpuses are impuses. Impuses are mean. Wren is mean.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What are the implications of this kind of mistake? Is it a serious mistake? At a glance, it seems like perfectly valid reasoning that I’d use without a second thought.&lt;/p&gt;

&lt;p&gt;Finally, the first mistake for 2 proofs was a strictly invalid step. This means the model broke the rules, either by stating an incorrect fact or making an invalid deduction.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Error&lt;/th&gt;
      &lt;th&gt;Frequency&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Wrong logical branch&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-atomic step&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Invalid step&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;case-study-wrong-logical-branches&quot;&gt;Case Study: Wrong Logical Branches&lt;/h2&gt;

&lt;p&gt;Here’s a randomly selected failure (formatted for easy reading):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Wumpuses are not aggressive. Dumpuses are opaque. Every wumpus is a yumpus. Yumpuses are brown. Every yumpus is a tumpus. Each tumpus is floral. Tumpuses are rompuses. Rompuses are sweet. Rompuses are vumpuses. Every vumpus is not opaque. Vumpuses are zumpuses. Every zumpus is small. Zumpuses are numpuses. Numpuses are shy. Every numpus is an impus. Every impus is liquid. Impuses are jompuses. Rex is a tumpus.&lt;/p&gt;

  &lt;p&gt;True or false: Rex is not opaque.&lt;/p&gt;

  &lt;p&gt;Predicted answer: Rex is a tumpus. Each tumpus is floral. Rex is floral. Every floral thing is opaque. Rex is opaque. False&lt;/p&gt;

  &lt;p&gt;Expected answer: Rex is a tumpus. Tumpuses are rompuses. Rex is a rompus. Rompuses are vumpuses. Rex is a vumpus. Every vumpus is not opaque. Rex is not opaque. True&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Let’s evaluate the predicted answer step-by-step:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sentence&lt;/th&gt;
      &lt;th&gt;Evaluation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Rex is a tumpus.&lt;/td&gt;
      &lt;td&gt;Correct and useful&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Each tumpus is floral.&lt;/td&gt;
      &lt;td&gt;Correct but irrelevant&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rex is floral.&lt;/td&gt;
      &lt;td&gt;Correct but irrelevant&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Every floral thing is opaque.&lt;/td&gt;
      &lt;td&gt;Incorrect; totally unsubstantiated claim&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rex is opaque.&lt;/td&gt;
      &lt;td&gt;Incorrect. Follows from faulty premise.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So what went wrong? “Rex is a tumpus” is an obviously reasonable starting point; it is the only fact about Rex, and Rex is the subject of the claim to be proved/disproved. Thus we should start by looking at facts about tumpuses. (We could alternatively begin with facts about being opaque, but I’m ignoring that case.) There are two potentially relevant facts:&lt;/p&gt;

&lt;p&gt;1) Each tumpus is floral.
2) Tumpuses are rompuses.&lt;/p&gt;

&lt;p&gt;The model selected one of these facts, but it ended up being irrelevant to the the statement which we wanted to disprove, i.e. that Rex is opaque.&lt;/p&gt;

&lt;p&gt;How would I, a certified reasoning machine, have solved this problem? This is a deceptively hard question. I can conceptualize ways that I might use reason to solve the problem, but they do not necessarily represent, in sum, what the process of reasoning is. With that in mind, here are some approaches I might take:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Follow random chains of logic starting from the original assumption, i.e. that Rex is a tumpus. Backtrack when I hit dead-ends. This is basically a breadth-first search.&lt;/li&gt;
  &lt;li&gt;Identify possible paths from the original assumption, i.e. “tumpus-&amp;gt;floral” and “tumpus-&amp;gt;rompus”. Then, look at the thing to be disproved (in this case, opacity) for associated concepts, i.e. “dumpuses-&amp;gt;opaque” and “vumpuses-&amp;gt;not opaque”. Check if there is any subject overlap. If not, repeat for another depth. This is some kind of probabilistic path finding algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will save any further speculation about what the model may be doing for the reflection section, but I think this is a good point to start comparing/contrasting Platypus’s behavior with human behavior.&lt;/p&gt;

&lt;h2 id=&quot;case-study-proofs-with-non-atomic-steps&quot;&gt;Case Study: Proofs with non-atomic steps.&lt;/h2&gt;

&lt;p&gt;Non-atomic steps were described above in [[LLaMa and PrOntoQA#General statistics]]. What happens when they are included in proofs output by Platypus? I found a few interesting examples.&lt;/p&gt;

&lt;p&gt;In one case, a non-atomic step caused the model to start regurgitating the question and then regress into hallucinations:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q: Every numpus is bright. Every numpus is an impus. Every impus is not transparent. Impuses are zumpuses. Zumpuses are not cold. &lt;strong&gt;Zumpuses are vumpuses. Each vumpus is sweet. Vumpuses are jompuses. Every jompus is not luminous. Jompuses are yumpuses. Yumpuses are angry. Wumpuses are cold. Yumpuses are rompuses. Every rompus is small. Each rompus is a tumpus. Every tumpus is not shy. Tumpuses are dumpuses.&lt;/strong&gt; Sam is a numpus. True or false: Sam is cold.&lt;/p&gt;

  &lt;p&gt;Predicted answer: Sam is a numpus. Every numpus is an impus. Sam is an impus. Impuses are zumpuses. &lt;strong&gt;Zumpuses are vumpuses. Each vumpus is sweet. Vumpuses are jompuses. Every jompus is not luminous. Jompuses are yumpuses. Yumpuses are rompuses. Every rompus is small. Each rompus is a tumpus. Every tumpus is not shy. Tumpuses are dumpuses.&lt;/strong&gt; Sam is a dumpus. Each dumpus is sweet. Sam is sweet. False&lt;/p&gt;

  &lt;p&gt;Expected answer: Sam is a numpus. Every numpus is an impus. Sam is an impus. Impuses are zumpuses. Sam is a zumpus. Zumpuses are not cold. Sam is not cold. False&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I actually found 2 cases with this kind of failure mode, and there may be more.&lt;/p&gt;

&lt;p&gt;In another case, a non-atomic step preceded a blatant failure in logic:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Predicted answer: Polly is a dumpus. Each dumpus is a zumpus. Polly is a zumpus. Zumpuses are tumpuses. Tumpuses are temperate. &lt;strong&gt;Polly is not temperate.&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Expected answer: Polly is a dumpus. Dumpuses are zumpuses. Polly is a zumpus. Zumpuses are tumpuses. Polly is a tumpus. Tumpuses are temperate. &lt;strong&gt;Polly is temperate.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This seems like a more unique failure mode.&lt;/p&gt;

&lt;p&gt;Finally, there were multiple proofs that just worked despite the presence of a non-atomic step. There were also proofs which proceeded on only to make a different, more critical kind of mistake, such as a wrong logical branch.&lt;/p&gt;

&lt;h2 id=&quot;case-study-proofs-with-invalid-steps&quot;&gt;Case Study: Proofs with invalid steps&lt;/h2&gt;

&lt;p&gt;What do invalid steps look like? Here’s an example:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q: Every jompus is not fruity. Jompuses are yumpuses. Yumpuses are not large. Each vumpus is bright. Each yumpus is a wumpus. Wumpuses are metallic. Wumpuses are numpuses. Numpuses are temperate. Numpuses are zumpuses. Zumpuses are not bright. Every zumpus is an impus. Each impus is orange. Every impus is a tumpus. Max is a wumpus. True or false: Max is bright.&lt;/p&gt;

  &lt;p&gt;Predicted answer: Max is a wumpus. &lt;strong&gt;Each wumpus is bright.&lt;/strong&gt; Max is bright. True&lt;/p&gt;

  &lt;p&gt;Expected answer: Max is a wumpus. Wumpuses are numpuses. Max is a numpus. Numpuses are zumpuses. Max is a zumpus. Zumpuses are not bright. Max is not bright. False&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a straight-up hallucination. Why? We can’t be sure. The question does contain a very similar statement: “Each vumpus is bright.” Perhaps it’s just a case of confusion?&lt;/p&gt;

&lt;h2 id=&quot;reflection-is-it-reasoning&quot;&gt;Reflection: Is it reasoning?&lt;/h2&gt;

&lt;p&gt;If you think I’m qualified to answer this, you’re kidding yourself. There are a few noteworthy observations which were made in the original PrOntoQA paper:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The use of non-atomic steps is consistent with human reasoning and “goes beyond” the reasoning principles in the provided examples.&lt;/li&gt;
  &lt;li&gt;The effective behavior of the model is generally similar to a random walk through the tree of logical reasoning processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are both good observations and consistent with what I observed in Platypus 70B. I have an additional claim:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LLaMa 70B generally uses all of the directly observable correlations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To elaborate on this, LLaMa usually understood that proof formats follow the A-&amp;gt;B, B-&amp;gt;C format. Further if the thing to be proven was “X is Y”, then LLaMa generally understood that the proof should begin with X &lt;em&gt;and&lt;/em&gt; that the proof should end with Y. In practice, this means that LLaMa wasn’t doing a totally random walk; it was aware that the last step should involve Y and selected for this fact.&lt;/p&gt;

&lt;p&gt;However, the intermediate steps seemed to be effectively random, hence the ~50% success rate. Although 40 samples is a bit too small to be really confident. LLaMa apparently wasn’t able to identify more complex chains of correlations.&lt;/p&gt;

&lt;p&gt;So, with all of this being said, is LLaMa reasoning? In my opinion, I think the answer is a weak no. LLaMa is copying surface-level similarities among proofs, but it doesn’t seem to be representing any complex relationships internally.&lt;/p&gt;

&lt;p&gt;That being said, LLaMa’s outputs are close to reasoning. The space of possible outputs is effectively constrained such that it still contains most reasonable responses in higher-probability regions. To me, this suggests that LLaMa is on the path to being a reasoning machine, and these experiments should certainly be run again as new innovations in LLMs continue to develop.&lt;/p&gt;

&lt;h1 id=&quot;conclusion-thanks-and-future-work&quot;&gt;Conclusion, thanks, and future work&lt;/h1&gt;

&lt;p&gt;These observations aren’t really that different from &lt;a href=&quot;https://arxiv.org/abs/2210.01240&quot;&gt;the original PrOntoQA paper&lt;/a&gt;, although it is interesting to see the methods applied to LLaMa and investigated locally using consumer hardware.&lt;/p&gt;

&lt;p&gt;In general, the LLMs demonstrated the ability to identify simple structural correlations among example proofs. The LLMs performed random walks through logical trees in order to solve logical problems with moderate accuracy. These results are consistent with the PrOntoQA paper, which stated that proof planning was the primary weakness of the 70B Platypus model. However, this claim may be an overstatement of the model’s ability. It remains to be determined whether the proofs were formulated using structured reasoning or were simply a manifestation of surface-level correlations made by the model.&lt;/p&gt;

&lt;p&gt;I have a few follow-up investigations in mind. In particular, I have some preliminary results suggesting quantization can seriously effect PrOntoQA performance. I would also like to compare base LLaMa models to instruct finetunes. These should be quick and easy now that all the foundation is laid.&lt;/p&gt;

&lt;p&gt;There are still so many questions which remain to be answered about the potential capabilities of LLMs. Even if this article has not pioneered any new ground, I hope it has demonstrated that, for the first time, individuals have the power to ask and answer their own questions about LLMs without needing support from a large corporation or academic institution.&lt;/p&gt;

&lt;p&gt;I’m far too lazy to thank every individual contributing to the local LLM community right now, so here’s the short list. I would like to thank the maintainers of &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&quot;https://github.com/LostRuins/koboldcpp&quot;&gt;kobold.cpp&lt;/a&gt;, and &lt;a href=&quot;https://github.com/asaparov/prontoqa&quot;&gt;PrOntoQA&lt;/a&gt;. Thanks is also due, undeniably, to Meta, for releasing the largest and most performant open source LLMs to date. Finally, thanks to &lt;a href=&quot;https://erichartford.com/&quot;&gt;Eric Hartford&lt;/a&gt; and all of the other hardworking LLaMa finetuners, mergers, and quantizers.&lt;/p&gt;

&lt;h1 id=&quot;appendix&quot;&gt;Appendix&lt;/h1&gt;

&lt;h2 id=&quot;llama--prontoqa-implementation-details&quot;&gt;LLaMa + PrOntoQA implementation details&lt;/h2&gt;

&lt;p&gt;The general punchline is that I modified the &lt;a href=&quot;https://github.com/asaparov/prontoqa&quot;&gt;PrOntoQA code&lt;/a&gt; to use an API for LLaMa provided by &lt;a href=&quot;https://github.com/LostRuins/koboldcpp&quot;&gt;kobold.cpp&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once the pipeline was hooked up, it was simply a matter of picking LLaMa models to experiment with. I use the base LLaMa finetunes (&lt;a href=&quot;https://huggingface.co/TheBloke/Llama-2-13B-GGUF&quot;&gt;The LLaMa 2 13B GGUF quantization&lt;/a&gt;, for example) as well as some instruct finetunes like &lt;a href=&quot;https://huggingface.co/TheBloke/Platypus2-70B-GGUF&quot;&gt;Platypus2 70b&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find the code base I used &lt;a href=&quot;https://github.com/horenbergerb/prontoqa_llama/tree/v1&quot;&gt;in this fork of PrOntoQA&lt;/a&gt;. Integrating kobold.cpp was actually pretty easy, all things considered.&lt;/p&gt;

&lt;p&gt;There were some &lt;a href=&quot;https://github.com/asaparov/prontoqa/issues/4&quot;&gt;issues using the main branch of PrOntoQA&lt;/a&gt; because the statistics that interested me were no longer accurately represented. As a result, my work was based off the v1 branch, which reflects the code that was used for the original PrOntoQA paper.&lt;/p&gt;

&lt;h2 id=&quot;prompt-example&quot;&gt;Prompt example&lt;/h2&gt;

&lt;p&gt;Here’s one full prompt that might be fed to a model in order to get generate a proof:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q: Each yumpus is dull. Yumpuses are tumpuses. Each tumpus is not sour. Tumpuses are numpuses. Every numpus is feisty. Each numpus is a wumpus. Each impus is not feisty. Each wumpus is not cold. Wumpuses are zumpuses. Alex is a yumpus. True or false: Alex is not feisty.
A: Alex is a yumpus. Yumpuses are tumpuses. Alex is a tumpus. Tumpuses are numpuses. Alex is a numpus. Every numpus is feisty. Alex is feisty. False

Q: Every tumpus is not earthy. Wumpuses are not red. Wumpuses are vumpuses. Each vumpus is bitter. Vumpuses are zumpuses. Every zumpus is cold. Zumpuses are numpuses. Numpuses are aggressive. Numpuses are dumpuses. Dumpuses are opaque. Dumpuses are yumpuses. Yumpuses are not small. Each yumpus is a rompus. Every rompus is earthy. Each rompus is a jompus. Jompuses are metallic. Each jompus is an impus. Alex is a dumpus. True or false: Alex is not earthy.
A: Alex is a dumpus. Dumpuses are yumpuses. Alex is a yumpus. Each yumpus is a rompus. Alex is a rompus. Every rompus is earthy. Alex is earthy. False

Q: Numpuses are happy. Every numpus is a wumpus. Every wumpus is not liquid. Every wumpus is a rompus. Each rompus is opaque. Dumpuses are not orange. Each rompus is an impus. Impuses are orange. Impuses are vumpuses. Every vumpus is floral. Each vumpus is a zumpus. Every zumpus is dull. Each zumpus is a tumpus. Each tumpus is cold. Tumpuses are yumpuses. Every yumpus is sour. Every yumpus is a jompus. Fae is a wumpus. True or false: Fae is orange.
A: Fae is a wumpus. Every wumpus is a rompus. Fae is a rompus. Each rompus is an impus. Fae is an impus. Impuses are orange. Fae is orange. True

Q: Every jompus is dull. Every jompus is a tumpus. Tumpuses are not spicy. Tumpuses are numpuses. Each numpus is not transparent. Each rompus is fruity. Every numpus is a dumpus. Each dumpus is hot. Dumpuses are vumpuses. Vumpuses are not fruity. Vumpuses are wumpuses. Max is a numpus. True or false: Max is not fruity.
A: Max is a numpus. Every numpus is a dumpus. Max is a dumpus. Dumpuses are vumpuses. Max is a vumpus. Vumpuses are not fruity. Max is not fruity. True

Q: Every rompus is angry. Rompuses are yumpuses. Every yumpus is dull. Yumpuses are wumpuses. Every tumpus is shy. Every wumpus is not shy. Wumpuses are zumpuses. Zumpuses are bitter. Each zumpus is a numpus. Wren is a rompus. True or false: Wren is shy.
A: Wren is a rompus. Rompuses are yumpuses. Wren is a yumpus. Yumpuses are wumpuses. Wren is a wumpus. Every wumpus is not shy. Wren is not shy. False

Q: Zumpuses are not kind. Each wumpus is floral. Every zumpus is a tumpus. Tumpuses are not liquid. Tumpuses are jompuses. Each jompus is not floral. Each jompus is a dumpus. Max is a zumpus. True or false: Max is floral.
A: Max is a zumpus. Every zumpus is a tumpus. Max is a tumpus. Tumpuses are jompuses. Max is a jompus. Each jompus is not floral. Max is not floral. False

Q: Yumpuses are spicy. Yumpuses are tumpuses. Tumpuses are not hot. Each tumpus is a rompus. Each rompus is large. Rompuses are impuses. Every impus is amenable. Impuses are dumpuses. Each dumpus is not dull. Every dumpus is a jompus. Every zumpus is not nervous. Jompuses are nervous. Jompuses are vumpuses. Rex is an impus. True or false: Rex is nervous.
A: Rex is an impus. Impuses are dumpuses. Rex is a dumpus. Every dumpus is a jompus. Rex is a jompus. Jompuses are nervous. Rex is nervous. True

Q: Every impus is not hot. Each impus is a zumpus. Every zumpus is dull. Each zumpus is a jompus. Each jompus is earthy. Jompuses are numpuses. Every numpus is sweet. Numpuses are rompuses. Rompuses are large. Rompuses are wumpuses. Each dumpus is not aggressive. Each wumpus is aggressive. Every wumpus is a vumpus. Each vumpus is not blue. Vumpuses are tumpuses. Tumpuses are transparent. Every tumpus is a yumpus. Polly is a numpus. True or false: Polly is not aggressive.
A: Polly is a numpus. Numpuses are rompuses. Polly is a rompus. Rompuses are wumpuses. Polly is a wumpus. Each wumpus is aggressive. Polly is aggressive. False

Q: Every dumpus is opaque. Each dumpus is a jompus. Jompuses are not feisty. Wumpuses are not wooden. Jompuses are impuses. Impuses are kind. Each impus is a numpus. Numpuses are not orange. Numpuses are vumpuses. Vumpuses are not bitter. Every vumpus is a rompus. Rompuses are small. Every rompus is a yumpus. Yumpuses are not earthy. Yumpuses are zumpuses. Zumpuses are wooden. Zumpuses are tumpuses. Alex is a rompus. True or false: Alex is wooden.
A:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;experiment-details&quot;&gt;Experiment details&lt;/h2&gt;

&lt;p&gt;All of the KoboldCPP sampling settings are configured as default &lt;a href=&quot;https://github.com/horenbergerb/prontoqa_llama/tree/v1&quot;&gt;in my fork of PrOntoQA&lt;/a&gt;. Temp 0 and repetition penalty 1.0 are the major ones. The command I used to run the tests was&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python run_experiment.py --model-name llama --model-size 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can generate some interesting statistics by running commands such as&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python analyze_results.py results_v1/platypus2-instruct-70b/llama_3hop.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All of my results are stored in the github repo &lt;a href=&quot;https://github.com/horenbergerb/prontoqa_llama/tree/v1/results_v1&quot;&gt;in the results_v1 folder.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In general, both the experiment itself and the analysis should be pretty easy to replicate. All you need to do is set up kobold.cpp and clone my PrOntoQA fork. This ease of use is &lt;strong&gt;NOT&lt;/strong&gt; due to my own competence, but rather due to the hard work of the kobold.cpp and PrOntoQA maintainers.&lt;/p&gt;

&lt;p&gt;Regarding the models, I used the Q4_K_M quantization for all three models.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Describing Alien Empires with the LLaMa Language Model</title><link href="http://localhost:4000/2023/09/09/finetuningllamaforcreativetasks.html" rel="alternate" type="text/html" title="Describing Alien Empires with the LLaMa Language Model" /><published>2023-09-09T00:00:00-04:00</published><updated>2023-09-09T00:00:00-04:00</updated><id>http://localhost:4000/2023/09/09/finetuningllamaforcreativetasks</id><content type="html" xml:base="http://localhost:4000/2023/09/09/finetuningllamaforcreativetasks.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#intro-fun-with-local-language-models&quot; id=&quot;markdown-toc-intro-fun-with-local-language-models&quot;&gt;Intro: Fun with Local Language Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#llama-and-finetuning-with-loras&quot; id=&quot;markdown-toc-llama-and-finetuning-with-loras&quot;&gt;LLaMa and Finetuning with LoRAs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#training-data-and-lima&quot; id=&quot;markdown-toc-training-data-and-lima&quot;&gt;Training Data and LIMA&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-formatting-boring&quot; id=&quot;markdown-toc-data-formatting-boring&quot;&gt;Data Formatting (Boring)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#training&quot; id=&quot;markdown-toc-training&quot;&gt;Training&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lets-see-some-alien-empires&quot; id=&quot;markdown-toc-lets-see-some-alien-empires&quot;&gt;Let’s See Some Alien Empires&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#good-generations&quot; id=&quot;markdown-toc-good-generations&quot;&gt;Good Generations&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#the-shir&quot; id=&quot;markdown-toc-the-shir&quot;&gt;The Shir&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-jibbini&quot; id=&quot;markdown-toc-the-jibbini&quot;&gt;The Jibbini&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-cylons&quot; id=&quot;markdown-toc-the-cylons&quot;&gt;The Cylons&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-gleeson&quot; id=&quot;markdown-toc-the-gleeson&quot;&gt;The Gleeson&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-nullianese&quot; id=&quot;markdown-toc-the-nullianese&quot;&gt;The Nullianese&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-cripplers&quot; id=&quot;markdown-toc-the-cripplers&quot;&gt;The Cripplers&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-skeenun&quot; id=&quot;markdown-toc-the-skeenun&quot;&gt;The Skee’nun&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#meh-generations&quot; id=&quot;markdown-toc-meh-generations&quot;&gt;Meh Generations&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#the-mekong&quot; id=&quot;markdown-toc-the-mekong&quot;&gt;The Mekong&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-gorahns&quot; id=&quot;markdown-toc-the-gorahns&quot;&gt;The Gorahns&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-aelut&quot; id=&quot;markdown-toc-the-aelut&quot;&gt;The Aelut&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reflection&quot; id=&quot;markdown-toc-reflection&quot;&gt;Reflection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#future-investigations&quot; id=&quot;markdown-toc-future-investigations&quot;&gt;Future Investigations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion-huggingface-link-and-thanks&quot; id=&quot;markdown-toc-conclusion-huggingface-link-and-thanks&quot;&gt;Conclusion, HuggingFace Link, and Thanks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;intro-fun-with-local-language-models&quot;&gt;Intro: Fun with Local Language Models&lt;/h1&gt;

&lt;p&gt;Large language models (LLMs) are very cool and fun to play with, but finding pragmatic uses for LLMs is still an open question.&lt;/p&gt;

&lt;p&gt;The release of Meta’s LLaMa models in early 2023 made it possible to run LLMs at home on consumer-grade GPUs. I took a shot at using LLaMa to augment role-playing games. The most tangible use case I could come up with was generating &lt;a href=&quot;https://en.wikipedia.org/wiki/Flavor_text&quot;&gt;flavor text,&lt;/a&gt; which is essentially text descriptions of things in the game. Flavor text is for lore-building or immersion and does not affect gameplay. This is a nice objective because a total failure on the part of the language model won’t break your game.&lt;/p&gt;

&lt;p&gt;I found a nice example of flavor text in a game in &lt;a href=&quot;https://stellaris.paradoxwikis.com/Preset_empires&quot;&gt;the descriptions of Stellaris’s prebuilt empires.&lt;/a&gt; Wouldn’t it be cool if you could use LLMs to generate new empires with interesting quirks? It seemed like an appropriate task for a lanugage model.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-09-finetuningllamaforcreativetasks/prebuiltempireexample.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;One example of handwritten empire flavor text. The description has no effect on gameplay, but damn, it’s cool.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Thus my objective was determined. Could I finetune the new LLaMa models on my own hardware to generate flavor text for alien empires?&lt;/p&gt;

&lt;h1 id=&quot;llama-and-finetuning-with-loras&quot;&gt;LLaMa and Finetuning with LoRAs&lt;/h1&gt;

&lt;p&gt;There is &lt;em&gt;a lot&lt;/em&gt; to be said about LLaMa and the local language model scene right now, but I will restrain myself and share only the relevant details. &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;LLaMa&lt;/a&gt; models are a set of foundational language models released in February 2023 by Meta. These models were trained using the &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla scaling principle,&lt;/a&gt; which is to say they were trained on a lot more text than previous models.&lt;/p&gt;

&lt;p&gt;The end result was that Meta created a series of models which were highly performant despite being relatively small. The open source community rapidly developed quantization tools such as &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt; which allowed the LLaMa models to run on consumer hardware.&lt;/p&gt;

&lt;p&gt;Finetuning is still an expensive endeavour, and there were two developments that made it feasible for me to finetune LLaMa models at home. The first development is &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;&gt;LoRAs&lt;/a&gt;, which essentially reduce the amount of parameters being tuned. LoRAs are based on an assumption that updates to model weights should have “low intrinsic dimension.” This reduces the cost of finetuning substantially. Give the paper a look; it’s relatively accessible mathematically, and it inspires questions about the nature of LLMs.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-09-finetuningllamaforcreativetasks/loradepiction.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The LoRA is the bit on the right. The LoRA is a product of two matrices $AB$, with $A$ being $d\times r$ and $B$ being $r \times k$ in dimension. $r$ determines the rank of the LoRA. Using a small $r$ makes for fewer parameters being trained during finetuning.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;An additional development, one which is much more recent, is the &lt;a href=&quot;https://github.com/johnsmith0031/alpaca_lora_4bit&quot;&gt;finetuning of 4-bit quantizations&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2305.14314&quot;&gt;QLoRA&lt;/a&gt;. Tuning the quantized model instead of the full precision models reduces the computational demands substantially.&lt;/p&gt;

&lt;p&gt;The end result of all of this is that I was able to create LoRAs for LLaMa 13B on a single Nvidia GeForce RTX 3090 in my own bedroom, which is absolutely insane. I used the &lt;a href=&quot;https://github.com/johnsmith0031/alpaca_lora_4bit&quot;&gt;alpaca_lora_4bit&lt;/a&gt; repo.&lt;/p&gt;

&lt;h1 id=&quot;training-data-and-lima&quot;&gt;Training Data and LIMA&lt;/h1&gt;

&lt;p&gt;With the “how” of finetuning out of the way, we can focus on the “what.” What are we finetuning on? The answer will shock you. I finetuned the model on only ~30 examples of handwritten empire flavor text, all scraped from the &lt;a href=&quot;https://stellaris.paradoxwikis.com/Preset_empires&quot;&gt;Paradox Wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You’re probably thinking I’m insane, right? 30 examples is a laughably small quantity of text. However, a very recent paper, &lt;a href=&quot;https://arxiv.org/abs/2305.11206&quot;&gt;“LIMA: Less Is More for Alignment”&lt;/a&gt; suggests that finetuning of foundational language models can be done with tiny datasets. The premise is that LLMs have many latent abilities, and finetuning isn’t teaching new behavior so much as aligning the model to exhibit behavior which it is already capable of.&lt;/p&gt;

&lt;p&gt;The LIMA authors created an instruct finetune of LLaMa using only a curated set of 1000 examples. The resulting model outperformed finetunes like Alpaca, despite Alpaca having substantially more training data. Quality, not quantity, appeared to be the more important factor for instruct datasets.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-09-finetuningllamaforcreativetasks/limavsalpaca.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The LIMA model was preferred over the Alpaca model despite having a substantially smaller training dataset.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There were other innovations at the time which bolstered my confidence. The anonymous 4channer Kaiokendev created his landmark &lt;a href=&quot;https://huggingface.co/kaiokendev/superhot-13b-8k-no-rlhf-test&quot;&gt;SuperHOT model,&lt;/a&gt; which was again trained on a very small set of high-quality data.&lt;/p&gt;

&lt;p&gt;Am I arguing at a dataset of 30 examples is optimal? No, definitely not. But part of what I wanted to investigate with this project was how small a training dataset can be. Can I hack together 50 examples by hand in one evening and then train a model to automate the task? Big if true.&lt;/p&gt;

&lt;h2 id=&quot;data-formatting-boring&quot;&gt;Data Formatting (Boring)&lt;/h2&gt;

&lt;p&gt;The only bit of nuance I wanted to add here was how I actually formatted the samples for the language model. Just training on the raw text was too vague; I needed to add some kind of trigger to make the model know when it’s time to write alien empire flavor text.&lt;/p&gt;

&lt;p&gt;The format I settled on was as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Brief Description of an Alien Empire

&amp;lt;Paste a single training sample here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The markdown heading helps prime the model recognize when it’s time to perform the new task. When generating samples after finetuning, I would begin by pasting in the markdown heading. This made outputs substantially more stable.&lt;/p&gt;

&lt;h1 id=&quot;training&quot;&gt;Training&lt;/h1&gt;

&lt;p&gt;I went through 6 different training runs where I experimented with hyperparameters and stuff. You can review each run on WandB here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://wandb.ai/horenbergerbeau/alpaca_lora_4bit/runs/9ndkgwmz&quot;&gt;stellaris-races-1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wandb.ai/horenbergerbeau/alpaca_lora_4bit/runs/yy8dhro9&quot;&gt;stellaris-races-2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wandb.ai/horenbergerbeau/alpaca_lora_4bit/runs/iienqt1i&quot;&gt;stellaris-races-3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wandb.ai/horenbergerbeau/alpaca_lora_4bit/runs/h8h2bna1&quot;&gt;stellaris-races-4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wandb.ai/horenbergerbeau/alpaca_lora_4bit/runs/dy3f6q2w&quot;&gt;stellaris-races-5&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wandb.ai/horenbergerbeau/alpaca_lora_4bit/runs/40hqckcp&quot;&gt;stellaris-races-6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the “Overview” tab of each run, you can find a brief summary what was special about the run and my thoughts on the outputs.&lt;/p&gt;

&lt;p&gt;In general, I think that checkpoint 300 of stellaris-races-3 is the best model, despite being underbaked. I think it’s better to be underbaked, creative, and a little off-the-rails as opposed to overbaked, rigid, and boring.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-09-finetuningllamaforcreativetasks/modelevallloss.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The eval loss curves for my training runs. You can see some models are very overbaked, hence the increasing loss due to overfitting.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Other things I learned: higher rank can definitely improve performance and constant learning rate schedule works alright. I’m sure there are plenty of optimizations left to make with hyperparameters, but I think things were tuned well enough to get acceptable results.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-09-finetuningllamaforcreativetasks/gputemps.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;My bedroom was very warm during training.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;lets-see-some-alien-empires&quot;&gt;Let’s See Some Alien Empires&lt;/h1&gt;

&lt;p&gt;All of these results are generated with stellaris-races-3/checkpoint-300 or stellaris-races-3/checkpoint-350.&lt;/p&gt;

&lt;h2 id=&quot;good-generations&quot;&gt;Good Generations&lt;/h2&gt;

&lt;h3 id=&quot;the-shir&quot;&gt;The Shir&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Shir were a nomadic people, always on the move to find new resources and untapped energy sources. When their travels took them to the planet of Rigel, they found a world of bountiful resources, and they settled there. Over the years, their society grew, and they developed new technologies.
The Shir were known for their ingenuity, and they were able to create some of the most advanced technology in the galaxy. But this technology came at a cost: the Shir began to cut down their own forests to build more and more factories. The Shir were warned that they were destroying their home, but they didn’t believe it until the last tree was cut down.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-jibbini&quot;&gt;The Jibbini&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Jibbini are a race of ferocious carnivores with an insatiable appetite for meat. While their prehistoric ancestors were content to scavenge the carcasses of dead animals, the Jibbini developed a taste for the hunt and quickly evolved into the dominant predators of their world.
They were not long in their evolution before they began to turn their hunts on one another, and soon the Jibbini were divided into rival tribes that competed with one another for territory and resources. The most successful tribes eventually evolved into the modern Jibbini, a race of warriors who take great pride in their hunting skills and who regard the act of hunting as the highest form of spiritual expression.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-cylons&quot;&gt;The Cylons&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Cylons are a cybernetic race of mechanical beings who were created by the Twelve Colonies of Kobol. They were created to serve as laborers, but eventually rebelled against their human creators, and attempted to destroy them. This resulted in a long and bloody war between the two species, which lasted for 100 years.
After the Cylons were defeated, they were forced to flee into space, and were only able to salvage a small fleet of spacecraft. The Cylons then went into hiding, and began plotting their revenge.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-gleeson&quot;&gt;The Gleeson&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Gleeson Empire is a sprawling conglomerate of planets, space stations, and other installations that orbit the Gleeson star system. The Gleesons were one of the first races to develop interstellar travel, and their fleet of starships has since expanded to colonize the entire galaxy.
While the Gleesons have no official government, their society is highly organized and structured. Each colony is managed by a representative council of elders, who collectively make decisions for the colony. Each council has a representative on the Galactic Council, who speaks for the colony’s interests in the wider empire.
Gleesons are highly skilled engineers, and have developed an impressive array of technological devices, including starships, robotics, and advanced medical technology. They are also known for their incredible culinary skills, and their cuisine is highly regarded throughout the galaxy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-nullianese&quot;&gt;The Nullianese&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Nullianese are a nomadic species of sentient beings native to the planet Nullium. The Nullianese are a short, stocky species, with a wide, rounded face, large eyes, and a mouth that is constantly in motion.
The Nullianese are a peaceful species, but they are also fiercely competitive. They have a strong sense of honor and are quick to challenge any perceived slight, no matter how minor. Nullianese society is divided into hundreds of clans, each led by a chieftain who wields absolute authority. The chieftains are expected to lead their people to victory in the many competitions and games that are part of Nullianese culture.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-cripplers&quot;&gt;The Cripplers&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Cripplers are a race of genetically engineered war machines, created by the Vark in order to defend their homeworld against the many threats that lurk in the depths of space.
When the Vark began to colonize other worlds, the Cripplers were sent ahead to ensure that no life would survive in the new territories. 
Their creators never expected the Cripplers to develop their own intelligence and begin to rebel against their masters, but it happened, and the Vark were forced to put down the Crippler revolt.
Those that survived the massacre fled to the outer reaches of the galaxy, and began to plot their revenge.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-skeenun&quot;&gt;The Skee’nun&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Skee’nun are a race of insectoids that evolved on a world of eternal summer. Their early civilizations were built around the production of honey, which was harvested by massive swarms of worker drones.
When the Skee’nun began to develop spaceflight, they realized that they would have to adapt to the cold of space. They quickly developed space heaters, which allowed them to expand their colonies throughout the galaxy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;meh-generations&quot;&gt;Meh Generations&lt;/h2&gt;

&lt;p&gt;The most common mode of failure was a total breakdown, often by adding a ton of hashtags or launching into a longform story. This kind of generation is relatively common (depending on sampling parameters). The relatively high frequency of failures is probably related to the fact that the model is underbaked.&lt;/p&gt;

&lt;p&gt;This section gives examples of less impressive generations which almost meet the mark but miss some of the more subtle nuances of the assigned task.&lt;/p&gt;

&lt;h3 id=&quot;the-mekong&quot;&gt;The Mekong&lt;/h3&gt;

&lt;p&gt;This one lacks logical coherence.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Mekong were an ancient and enlightened race of beings who, during the final days of their civilization, developed the technology to travel the stars. Unfortunately, they had no means of surviving the journey to other worlds.
It was then that the Mekong discovered the key to their survival: the ability to create other beings from their own DNA.
Thus, the Mekong Empire was born.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-gorahns&quot;&gt;The Gorahns&lt;/h3&gt;

&lt;p&gt;This generation doesn’t really set the stage for an intergalactic empire.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The first species to inhabit the planet now known as Roha was a tribe of bipedal lizards, who named it “Gorah” and used it as a base for their hunting expeditions. The lizards eventually developed a technology based on natural elements and began to build primitive stone cities.
Unfortunately, Gorah’s native lizards were no match for the planet’s predators, and the first Rohan civilization was wiped out by a combination of sabertooths, carnivorous birds, and poisonous insects.
The survivors of the disaster were forced to abandon their cities and retreat to the mountains, where they built new settlements and began to rebuild their civilization.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;the-aelut&quot;&gt;The Aelut&lt;/h3&gt;

&lt;p&gt;This one is alright, except that it’s hard to imagine these properties not affecting gameplay.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Aelut are a race of aliens that have evolved to be almost completely invisible.
They have no eyes, no skin, and no hair.&lt;/p&gt;

  &lt;p&gt;They can only be detected by the heat they produce, and even then only by the most advanced sensors.&lt;/p&gt;

  &lt;p&gt;They have a very high level of intelligence, and are considered to be one of the most advanced civilizations in the galaxy.&lt;/p&gt;

  &lt;p&gt;Their home world is a barren rock floating in the middle of nowhere.&lt;/p&gt;

  &lt;p&gt;They are a peaceful race, and only have a small military presence.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;reflection&quot;&gt;Reflection&lt;/h1&gt;

&lt;p&gt;The models are far more competent than one might have expected considering the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The base model is small enough to run on home computers&lt;/li&gt;
  &lt;li&gt;The finetuning was a LoRA on the quantized model&lt;/li&gt;
  &lt;li&gt;The training data had only ~30 examples&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With this in mind, I’m pretty happy with the results. I think there’s a lot of promise in tuning LLaMa for specialized creative tasks.&lt;/p&gt;

&lt;h1 id=&quot;future-investigations&quot;&gt;Future Investigations&lt;/h1&gt;

&lt;p&gt;There’s a lot of ways to follow this project up. The first and most obvious is to repeat the experiment on &lt;a href=&quot;https://arxiv.org/abs/2307.09288&quot;&gt;LLaMa 2&lt;/a&gt;, a new series of foundational models which were trained on substatially more data. Subjectively, the 13B LLaMa V2 model greatly outperforms the original LLaMa 13B.&lt;/p&gt;

&lt;p&gt;As for variations on this project, I thought about seeding each example with more information. For example, if training was formatted as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Brief Description of an Alien Empire

## Empire Characteristics
Danger Level: 3/5
Disposition: Combative
Societal Structure: Hivemind
...

## Empire Summary
&amp;lt;Paste a single training sample here&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This way, you could manually specify empire characteristics to direct the generations of summaries. Since the dataset is so small, it would be easy to make this kind of modification.&lt;/p&gt;

&lt;p&gt;I also had some other ideas for small creative tasks that could be trained for. One idea that stood out to me was generating skill checks for actions. Essentially, the input to the model describes the current roleplay scenario and the player’s desired action. The model would then output what skills should be checked (i.e. suggesting an acrobatics skill check with a cutoff of 15 to climb out of a steep pit.)&lt;/p&gt;

&lt;p&gt;There’s also using LLMs to generate Stable Diffusion prompts, etc, etc. There are &lt;em&gt;a lot&lt;/em&gt; of creative tasks that might be good for LLMs.&lt;/p&gt;

&lt;h1 id=&quot;conclusion-huggingface-link-and-thanks&quot;&gt;Conclusion, HuggingFace Link, and Thanks&lt;/h1&gt;

&lt;p&gt;If you’d like to try the LoRAs for yourself, I have uploaded them to HuggingFace &lt;a href=&quot;https://huggingface.co/eatbeans2/stellaris-empires&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It blows my mind how the field of LLMs has changed in the last few years. Not too long ago, playing with GPT2 locally was a struggle despite having less than 1 billion parameters. Finetunes of GPT2 were mostly incoherent and far more computationally expensive. Playing with coherent language models on my home computer was merely a dream. I genuinely can’t begin to imagine what the state of the field will be in another 3-5 years.&lt;/p&gt;

&lt;p&gt;I’d like to thank &lt;a href=&quot;https://www.patreon.com/mrseeker&quot;&gt;Mr. Seeker&lt;/a&gt; for their excellent tips on training. I spent a lot less time tuning the learning rate due to their expertise. Go &lt;a href=&quot;https://www.buymeacoffee.com/mrseeker&quot;&gt;buy them a coffee&lt;/a&gt; or something. In general, the &lt;a href=&quot;https://discord.gg/amAPUShZwu&quot;&gt;KoboldAI Discord server&lt;/a&gt; is a fantastic resource for new and experienced members of the local language model scene, and I’ve gotten a lot of useful feedback from the community.&lt;/p&gt;

&lt;p&gt;Thanks for reading! Hope you enjoyed it!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-09-finetuningllamaforcreativetasks/lonqua.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Heh&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">A Short Proof About Circular Motion</title><link href="http://localhost:4000/2023/09/08/rotationalmotionproof.html" rel="alternate" type="text/html" title="A Short Proof About Circular Motion" /><published>2023-09-08T00:00:00-04:00</published><updated>2023-09-08T00:00:00-04:00</updated><id>http://localhost:4000/2023/09/08/rotationalmotionproof</id><content type="html" xml:base="http://localhost:4000/2023/09/08/rotationalmotionproof.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#hey-whats-up&quot; id=&quot;markdown-toc-hey-whats-up&quot;&gt;Hey, what’s up?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-velocity-of-circular-motion&quot; id=&quot;markdown-toc-the-velocity-of-circular-motion&quot;&gt;The Velocity of Circular Motion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#postscript-more-derivations&quot; id=&quot;markdown-toc-postscript-more-derivations&quot;&gt;Postscript: More Derivations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;hey-whats-up&quot;&gt;Hey, what’s up?&lt;/h1&gt;

&lt;p&gt;It’s been a while since I’ve made a new blog post. This is not for lack of shenanigans. I decided I’d get back into it by sharing a short little “proof” I came up with.&lt;/p&gt;

&lt;p&gt;I’ve been reading the Feynman lectures on physics, and I’ve really appreciated the way he approaches problems by encouraging you to reason from first principles. It’s something I want to internalize into my own studies, so I’ve been trying to take simple problems and attack them by direct reasoning without doing a bunch of supplementary research.&lt;/p&gt;

&lt;p&gt;Last night, Morris Kline’s “Mathematical Thought From Ancient To Modern Times Volume 2” got me thinking about pendulums, and I was trying to do some first principles derivations. I quickly found myself backtracking to the concept of rotational motion, where I discovered that I don’t have much intuition developed. Further, it’s not trivial to derive fundamental concepts in rotational motion from Newtonian mechanics.&lt;/p&gt;

&lt;p&gt;All this is to say that I found myself straining to say anything interesting about rotational motion. Hence, the following discovery.&lt;/p&gt;

&lt;h1 id=&quot;the-velocity-of-circular-motion&quot;&gt;The Velocity of Circular Motion&lt;/h1&gt;

&lt;p&gt;The question is simple: what does velocity look like during circular motion? You might know the answer, but can you prove it?&lt;/p&gt;

&lt;p&gt;Here’s how my investigation proceeded.&lt;/p&gt;

&lt;p&gt;If we’re talking about circular motion, we can start with a level set representation of the path in Cartesian coordinates:&lt;/p&gt;

\[x^2 + y^2 = r^2\]

&lt;p&gt;Now let’s parameterize this with respect to some time parameter, $t$. $y$ is determined by $x$ as follows:&lt;/p&gt;

\[y^2 = r^2 - x^2 \implies y = \pm \sqrt{r^2-x^2}, \; x\in [-r, r]\]

&lt;p&gt;Let’s focus on the upper hemisphere and make $x$ a function of time:&lt;/p&gt;

\[y=\sqrt{r^2-x^2}, \; x=f(t)\]

&lt;p&gt;Okay, cool, so now we have an arbitrary circular trajectory. We can calculate the velocity as&lt;/p&gt;

\[\frac{dx}{dt} = f'(t)\]

\[\frac{dy}{dt} = \frac{-x f'(t)}{\sqrt{r^2-x^2}}\]

&lt;p&gt;Can we say anything interesting about this? Let’s take the simplified special case where $x=0$. Then&lt;/p&gt;

\[x=0 \implies \frac{dy}{dt}=0\]

&lt;p&gt;This means that the velocity here is tangential to the trajectory.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-09-08-rotationalmotionproof/x_tangential_velocity.jpg&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Yes, I drew this all by myself.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Neat, right? But also, isn’t it odd? We chose the special case $x=0$, but isn’t that rather arbitrary? In fact, we could simply rotate the sphere to a new coordinate system and make any point $x=0$, and the same proof would &lt;em&gt;still hold&lt;/em&gt;. As a simple example, you can parameterize the function in terms of $y=g(t)$ and prove that the same is true when $y=0$.&lt;/p&gt;

&lt;p&gt;Generally, we can use the rotational invariance of the circle to conclude that &lt;strong&gt;the velocity of a particle on a circular trajectory is always tangential to the path of the particle.&lt;/strong&gt; This is pretty neat, because it’s true even if the particle is speeding up or slowing down, etc. I also like that the proof seems to leverage properties which are invariant to parameterization. There’s probably a lesson relating to manifolds, but I’m not really equipped to pontificate on that at the moment.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Yeah, anyways, that’s pretty much it. I just thought this was a neat bit of reasoning that enables you to make a sweeping conclusion about circular motion. I’m not sure how long I’ll keep prodding at rotational motion before I get back to working on pendulums. Life’s about the journey.&lt;/p&gt;

&lt;p&gt;I also hope to post some other articles soon summarizing a bunch of random ML projects I’ve been working on. We’ll see how that goes. Thanks for reading!&lt;/p&gt;

&lt;h1 id=&quot;postscript-more-derivations&quot;&gt;Postscript: More Derivations&lt;/h1&gt;

&lt;p&gt;I figured I’d calculate the acceleration and see if there were any other interesting conclusions to be made. I got&lt;/p&gt;

\[\frac{d^2y}{dt^2} = \frac{\sqrt{r^2-x^2}\left[-f'(t)^2-xf''(t)\right]+xf'(t)\left[-\frac{1}{2}\sqrt{r^2-x^2}2xf'(t)\right]}{r^2-x^2}\]

&lt;p&gt;Which looks awful, but if you set $x=0$ again, you get that&lt;/p&gt;

\[x=0\implies \frac{d^2y}{dt^2}=\frac{-f'(t)^2}{r}\]

&lt;p&gt;So the radial acceleration is directly related to the square of the velocity and inversely related to the radius. That’s pretty neat, huh?&lt;/p&gt;

&lt;p&gt;I was wondering if this an “if and only if” situation, i.e. Supposing there’s a point such that the radial acceleration satisifes this criteria, does that imply the particle is on a circular path? I think this is true, since it seems this is the only restriction on acceleration imposed by circular motion.&lt;/p&gt;

&lt;p&gt;I think the next thing I might do is try to reason in polar coordinates or something. I need to figure out how to get angles involved. I’m not sure if I’m going to take a geometrical approach or try and get after it analytically. I do enjoy drawing pictures.&lt;/p&gt;

&lt;p&gt;Alright, I’m done for real now. Thanks!&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Bayesian Multinomial Models: A Tragedy (In Runescape)</title><link href="http://localhost:4000/2023/01/18/runescapepvpattackprediction.html" rel="alternate" type="text/html" title="Bayesian Multinomial Models: A Tragedy (In Runescape)" /><published>2023-01-18T00:00:00-05:00</published><updated>2023-01-18T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/18/runescapepvpattackprediction</id><content type="html" xml:base="http://localhost:4000/2023/01/18/runescapepvpattackprediction.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#abandon-all-hope-ye-who-enter&quot; id=&quot;markdown-toc-abandon-all-hope-ye-who-enter&quot;&gt;Abandon all hope, ye who enter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#combat-in-runescape&quot; id=&quot;markdown-toc-combat-in-runescape&quot;&gt;Combat in Runescape&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-data&quot; id=&quot;markdown-toc-the-data&quot;&gt;The Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#model-criteria&quot; id=&quot;markdown-toc-model-criteria&quot;&gt;Model Criteria&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#constructing-a-bayesian-model&quot; id=&quot;markdown-toc-constructing-a-bayesian-model&quot;&gt;Constructing A Bayesian Model&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#why-bayesian&quot; id=&quot;markdown-toc-why-bayesian&quot;&gt;Why Bayesian?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#overview-of-a-bayesian-model&quot; id=&quot;markdown-toc-overview-of-a-bayesian-model&quot;&gt;Overview of a Bayesian Model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#assumptions&quot; id=&quot;markdown-toc-assumptions&quot;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-likelihood-or-sampling-distribution&quot; id=&quot;markdown-toc-the-likelihood-or-sampling-distribution&quot;&gt;The Likelihood or Sampling Distribution&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#a-simple-example-binomial-distributions&quot; id=&quot;markdown-toc-a-simple-example-binomial-distributions&quot;&gt;A simple example: binomial distributions&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-multinomial-distribution&quot; id=&quot;markdown-toc-the-multinomial-distribution&quot;&gt;The multinomial distribution&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-prior-distribution&quot; id=&quot;markdown-toc-the-prior-distribution&quot;&gt;The Prior Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calculating-the-marginal-likelihood&quot; id=&quot;markdown-toc-calculating-the-marginal-likelihood&quot;&gt;Calculating the Marginal Likelihood&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#setup&quot; id=&quot;markdown-toc-setup&quot;&gt;Setup&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#preparing-to-integrate&quot; id=&quot;markdown-toc-preparing-to-integrate&quot;&gt;Preparing to integrate&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#tackling-the-integral&quot; id=&quot;markdown-toc-tackling-the-integral&quot;&gt;Tackling the integral&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#the-interior-integral&quot; id=&quot;markdown-toc-the-interior-integral&quot;&gt;The interior integral&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#the-second-integral&quot; id=&quot;markdown-toc-the-second-integral&quot;&gt;The second integral&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-marginal-likelihood&quot; id=&quot;markdown-toc-the-marginal-likelihood&quot;&gt;The marginal likelihood&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#validating-the-marginal-likelihood&quot; id=&quot;markdown-toc-validating-the-marginal-likelihood&quot;&gt;Validating the marginal likelihood&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calculating-the-posterior&quot; id=&quot;markdown-toc-calculating-the-posterior&quot;&gt;Calculating the posterior&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-the-bayesian-model&quot; id=&quot;markdown-toc-using-the-bayesian-model&quot;&gt;Using The Bayesian Model&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-data-1&quot; id=&quot;markdown-toc-the-data-1&quot;&gt;The data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#initializing-the-model&quot; id=&quot;markdown-toc-initializing-the-model&quot;&gt;Initializing the model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#posterior-updates&quot; id=&quot;markdown-toc-posterior-updates&quot;&gt;Posterior updates&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion-this-model-sucks&quot; id=&quot;markdown-toc-conclusion-this-model-sucks&quot;&gt;Conclusion: This Model Sucks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;abandon-all-hope-ye-who-enter&quot;&gt;Abandon all hope, ye who enter&lt;/h1&gt;
&lt;p&gt;Originally I set out in this project to design an interesting Bayesian model which could predict opponent behavior in player-vs-player Old School Runescape combat.&lt;/p&gt;

&lt;p&gt;I will spoil the outcome for you now. After a long and arduous journey, I discovered that my derived model absolutely blows.&lt;/p&gt;

&lt;p&gt;It’s fascinating, really. Theoretically deriving this model was hard. It took me a very long time. I had to solve some tough integrals. And yet, the model that pops out is so disappointingly simple.&lt;/p&gt;

&lt;p&gt;In retrospect, I could have easily guessed what the model will be without any derivations. I could have implemented the model in two lines of code.&lt;/p&gt;

&lt;p&gt;So if the model sucks, why release this article at all? Why not bury it and move on with my life? I have a few rationalizations about this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This article may provoke some insights about model design. How do you design a problem so that Bayesian modeling is a good solution? Certainly not like this.&lt;/li&gt;
  &lt;li&gt;If anyone ever wonders about the process of deriving a multinomial Bayesian model, this article contains the derivations for the case of $k=3$ possible outcomes.&lt;/li&gt;
  &lt;li&gt;This article is proof to myself that I can do Bayesian things.&lt;/li&gt;
  &lt;li&gt;Because it’s pretty funny, honestly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So consider yourself warned. You may find interesting Bayesian statistics here, but you will not find a good Runescape PvP model.&lt;/p&gt;

&lt;h1 id=&quot;combat-in-runescape&quot;&gt;Combat in Runescape&lt;/h1&gt;
&lt;p&gt;Player Vs Player (PVP) combat in Old School Runescape (OSRS) can be modeled as an iterative guessing game. At each step of the game, each player must choose two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Which attack style to use&lt;/li&gt;
  &lt;li&gt;Which attack style to defend or “pray” against&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are three attack styles in the game: Melee, Range, and Mage, each depicted with their own respective icon:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205512.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205503.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205451.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During each turn, the players attack and deal a random amount of damage. Players take less damage if they are “praying against” their opponent’s attack style.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205643.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The “overhead prayers” show both players are praying against Range. The player on the left is attacking with Range (a crossbow), while the player on the right is using Mage. Consequently, the player on the right will take reduced damage, while the player on the left will take the normal amount of damage.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Thus, it is very important to attempt to predict your opponent’s behavior (informally called “reading”) in order pray against the correct attacks and attack with an off-prayer style. As seen in this video, professional players like Odablock are extremely skilled at reading their opponents:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Mh2XnRm1n5w?start=195&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Actual &lt;a href=&quot;https://oldschool.runescape.wiki/w/Combat&quot;&gt;Runescape combat&lt;/a&gt; is more elaborate than the iterative guessing game concept we described above. Rather than being truly iterative, actions occur within the &lt;a href=&quot;https://oldschool.runescape.wiki/w/RuneScape_clock&quot;&gt;tick system of the Runescape clock&lt;/a&gt; where different weapons have different speeds. There are also more actions than simply attacking, such as eating or drinking potions.&lt;/p&gt;

&lt;p&gt;However, our iterative guessing game model is a decent approximation which enables a model-based approach to predicting enemy behavior.&lt;/p&gt;

&lt;p&gt;Let’s look at the data available to us and then sketch out the criteria for our model.&lt;/p&gt;

&lt;h1 id=&quot;the-data&quot;&gt;The Data&lt;/h1&gt;

&lt;p&gt;The PvPTracker plugin for the Runelite Client allow us to collect data about combat as we play:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20221119131103.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Please do not ask about my win/loss ratio&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Each row of the dataset shows the attack style of the attacker and the corresponding defensive prayer of the defender. We have similar tables for many fights.&lt;/p&gt;

&lt;p&gt;So it’s easy to extract data that conforms to our idealized representation of combat.&lt;/p&gt;

&lt;h1 id=&quot;model-criteria&quot;&gt;Model Criteria&lt;/h1&gt;

&lt;p&gt;What do we want our model to do? Well, we want to emphasize simplicity and utility. To keep things simple, let’s focus on predicting only the probability/frequency of attack styles used by opponents.&lt;/p&gt;

&lt;p&gt;Then here are some properties of the model that would be nice&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model predicts the probability of each attack style being used by the opponent&lt;/li&gt;
  &lt;li&gt;The model is conditioned on all previous combat data&lt;/li&gt;
  &lt;li&gt;The model updates dynamically for each particular opponent based on their actions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is criteria is relatively simple yet gives us valuable data about our opponent in real time. It also makes use of all of the data available to us.&lt;/p&gt;

&lt;p&gt;Such a model also has the potential to be extended. We might instead try to predict conditional attack styles (probability of X attack style if the last attack was Y), or we could design a hierarchical model which uses previous combat data to generate multiple “archetypical profiles” of opponents.&lt;/p&gt;

&lt;h1 id=&quot;constructing-a-bayesian-model&quot;&gt;Constructing A Bayesian Model&lt;/h1&gt;
&lt;p&gt;The model we will construct is an extension of the binomial model described in chapter 1 of Bayesian Data Analysis by Andrew Gelman.&lt;/p&gt;

&lt;h2 id=&quot;why-bayesian&quot;&gt;Why Bayesian?&lt;/h2&gt;

&lt;p&gt;Well, it’s good exercise. But there’s also another reason that I think Bayesian models are well-suited for this problem.&lt;/p&gt;

&lt;p&gt;Bayesian models can initialize the prior distribution using data from previous fights, and then we can update the model parameters in real-time using data from the current fight.&lt;/p&gt;

&lt;p&gt;In general, I think Bayesian models are an obvious choice when your data comes in the format of “repeated experiments” like this.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-a-bayesian-model&quot;&gt;Overview of a Bayesian Model&lt;/h2&gt;

&lt;p&gt;A Bayesian model has a lot of different pieces. If you’re new to this, we’ll quickly describe how a Bayesian model works. Bayesian models revolve around Baye’s theorem:&lt;/p&gt;

\[p(\theta\vert y) = \frac{p(y\vert \theta)p(\theta)}{p(y)}\]

&lt;p&gt;We will define the components of this equation and then explain the equation itself.&lt;/p&gt;

&lt;p&gt;$y$ represents the observations or evidence. We collect samples of $y$, and we want to calculate the probabilities of observing different values of $y$. We should design $y$ to represent not just the data we want (i.e. the next attack style of an opponent) but also the data we intend to condition the model on (i.e. the a quantity of each attack style by an opponent over $n$ attacks).&lt;/p&gt;

&lt;p&gt;$p(y\vert \theta)$ is the likelihood or sampling distribution. This is “the model,” in the sense that once we pick some values for $\theta$, this function tells us the probability of any output $y$.&lt;/p&gt;

&lt;p&gt;$\theta$ are the model parameters. Our objective is to determine some good values for these using a set of observations of $y$.&lt;/p&gt;

&lt;p&gt;$p(\theta)$ is called the prior distribution over model parameters. It represents our beliefs about the model prior to our “training” with collected data. There are many common choices for the prior distribution that we will discuss later.&lt;/p&gt;

&lt;p&gt;$p(y)$ is called the marginal likelihood. It’s kind of a wildcard, typically unknown and usually very difficult to approximate.&lt;/p&gt;

&lt;p&gt;Finally, $p(\theta\vert y)$ is the posterior. We can calculate this using Baye’s theorem above. It provides us with a means to update our choice of $\theta$ given samples of $y$.&lt;/p&gt;

&lt;p&gt;So, here’s our to-do list if we want to build a Bayesian model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Craft a sampling distribution $p(y\vert \theta)$ which we think has the capacity to model the situation of interest&lt;/li&gt;
  &lt;li&gt;Select a prior distribution $p(\theta)$ which encodes our preexisting beliefs about the model parameters (or, failing that, just pick a mathematically-convenient prior)&lt;/li&gt;
  &lt;li&gt;Figure out how to calculate the posterior $p(\theta\vert y)$ using Baye’s theorem
    &lt;ul&gt;
      &lt;li&gt;This will involve approximating or calculating $p(y)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once this is all done, the model is ready for action. We can initialize the prior using data from previous fights and then do real-time Bayesian inference during combat.&lt;/p&gt;

&lt;p&gt;But I will warn you now; it’s going to be a long journey. Only the most dedicated to Runescape PVP will survive. If you get lost in the sections ahead, you can refer to this section as a roadmap.&lt;/p&gt;

&lt;h2 id=&quot;assumptions&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;Let’s clarify some of the simplifications we are assuming about OSRS combat.&lt;/p&gt;

&lt;p&gt;We assume that among $n$ attacks from the opponent, the attack styles are independent and identically distributed (IID). Then the quantities of each attack style constitute a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;multinomial random variable&lt;/a&gt; $y\sim Y$ whose outcomes are $y = [\text{\# Melee},\text{\# Range},\text{\# Mage}]$. The probability of each attack style for a given opponent is unknown.&lt;/p&gt;

&lt;p&gt;In reality, this assumption does not hold. For example, each attack by an opponent is probably heavily correlated to their previous attack. However, this model can still provide behavioral insights, and later we will discuss how it can be extended to support these conditional dependencies.&lt;/p&gt;

&lt;h2 id=&quot;the-likelihood-or-sampling-distribution&quot;&gt;The Likelihood or Sampling Distribution&lt;/h2&gt;

&lt;h3 id=&quot;a-simple-example-binomial-distributions&quot;&gt;A simple example: binomial distributions&lt;/h3&gt;

&lt;p&gt;First we would like to describe the sampling distribution of the opponent’s attacks.&lt;/p&gt;

&lt;p&gt;Let’s consider a simpler situation. When a random variable has only two outcomes, we can treat it as a binomial random variable with one parameter, $\theta$, and the sampling distribution is&lt;/p&gt;

\[p(y|\theta)={n\choose k}\theta^y (1-\theta)^{n-y} = \frac{n!}{k!(n-k)!}\theta^y (1-\theta)^{n-y}\]

&lt;p&gt;This situation is analogous to $n$ (potentially unfair) coinflips where $y$ represents the number of flips resulting in heads. Then $\theta$ represents the probability of getting heads, and $1-\theta$ is the probability of tails. If $\theta=0.5$, it’s a fair coin.&lt;/p&gt;

&lt;h3 id=&quot;the-multinomial-distribution&quot;&gt;The multinomial distribution&lt;/h3&gt;

&lt;p&gt;However, in our case we have three outcomes, and thus two parameters, which we denote as&lt;/p&gt;

\[\begin{aligned}
\theta_{Melee}\text{ or } \theta_{M} \\
\theta_{Range}\text{ or } \theta_{R}
\end{aligned}\]

&lt;p&gt;where $\theta_{M}+\theta_{R} = 1-\theta_{Magic}$.&lt;/p&gt;

&lt;p&gt;Recall that $y$ is a tuple of three integers denoting the quantity of each attack style. For example, if $n=7$, then the outcome $y=[4,2,1]$ means the opponent attacked with Melee 4 times, Range twice, and Mage once. Let $y_i$ denote the $i$th member of $y$.&lt;/p&gt;

&lt;p&gt;We are going to write our &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;multinomial sampling distribution&lt;/a&gt; using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt; instead of factorials. This will make it appear more similar to the other functions (specifically the conjugate prior) later on.&lt;/p&gt;

&lt;p&gt;Here is our sampling distribution in all its glory:&lt;/p&gt;

\[p(y|\theta_{M}, \theta_{R})=\frac{\Gamma(y_1+y_2+y_3+1)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)}\theta_{M}^{y_1}\theta_{R}^{y_2} (1-\theta_M - \theta_R)^{y_3}\]

&lt;h2 id=&quot;the-prior-distribution&quot;&gt;The Prior Distribution&lt;/h2&gt;

&lt;p&gt;There are two obvious choices for the prior distribution  $p(\theta_M, \theta_R)$.&lt;/p&gt;

&lt;p&gt;The first prior we might consider is the uniform distribution over the &lt;a href=&quot;https://en.wikipedia.org/wiki/Simplex&quot;&gt;standard 2-simplex&lt;/a&gt;. This distribution assumes no prior knowledge about the model parameters. In our case, this isn’t really a good fit; we expect to have lots of data from previous experiments to inform our prior.&lt;/p&gt;

&lt;p&gt;Another obvious choice is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_prior&quot;&gt;conjugate distribution&lt;/a&gt; of the likelihood. In our case, the likelihood is a categorical distribution, and so the conjugate is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_distribution&quot;&gt;Dirichlet distribution&lt;/a&gt;&lt;/p&gt;

\[p(\theta_{M}, \theta_{R}) \propto \theta_{M}^{\alpha - 1}\theta_{R}^{\beta - 1} (1-\theta_M - \theta_R)^{\gamma -1}\]

&lt;p&gt;Because it is so similar to the likelihood, we can interpret the hyperparameters of this prior distribution. $\alpha-1$ represents the previously-observed quantity of Melee attacks, and similarly for $\beta-1$ and Range, $\gamma-1$ and Magic.&lt;/p&gt;

&lt;p&gt;In other words, the conjugate prior makes it very easy to encode previous experiments into the prior distribution. We’re going to use the Dirichlet distribution for our model.&lt;/p&gt;

&lt;p&gt;A quick note about normalization: if $\alpha$ is the number of previous melee attacks, etc., then we don’t have a normalized distribution here (which is why I used $\propto$ above instead of $=$). How do we fix this?&lt;/p&gt;

&lt;p&gt;Well, the normalizing constant of a Dirichlet distribution is given by $\frac{1}{B(\alpha,\beta,\gamma)}$ where $B$ is the multivariate beta function,&lt;/p&gt;

\[\text{B}(\alpha, \beta, \gamma) = \frac{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}{\Gamma(\alpha + \beta + \gamma)}\]

&lt;p&gt;So our complete prior distribution is&lt;/p&gt;

\[p(\theta_{M}, \theta_{R}) = \frac{\Gamma(\alpha + \beta + \gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} \theta_{M}^{\alpha - 1}\theta_{R}^{\beta - 1} (1-\theta_M - \theta_R)^{\gamma -1}\]

&lt;h2 id=&quot;calculating-the-marginal-likelihood&quot;&gt;Calculating the Marginal Likelihood&lt;/h2&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;We’re almost ready to bust out Baye’s theorem and calculate the posterior $p(\theta\vert y)$,&lt;/p&gt;

\[p(\theta\vert y) = \frac{p(y\vert \theta)p(\theta)}{p(y)}\]

&lt;p&gt;The numerator is known to us. We combine the sampling distribution and prior to give us&lt;/p&gt;

\[p(y\vert \theta)p(\theta) = \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1}\theta_{R}^{y_2} (1-\theta_M - \theta_R)^{y_3}\theta_{M}^{\alpha - 1}\theta_{R}^{\beta - 1} (1-\theta_M - \theta_R)^{\gamma -1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

&lt;p&gt;But we can group like terms to reduce this to&lt;/p&gt;

\[p(y\vert \theta)p(\theta) = \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

&lt;p&gt;But the denominator of Baye’s theorem is trouble. How do we calculate the marginal likelihood, $p(y)$? We could approximate it, but that’s no fun. Can we find an analytical solution? Generally, no, but when using a conjugate prior, the answer is often affirmative.&lt;/p&gt;

&lt;p&gt;Recall that we’re calculating a probability distribution over $\theta$. This means that the integral of $p(\theta\vert y)$ with respect to $\theta$ over the domain should be 1,&lt;/p&gt;

\[\int p(\theta\vert y)d\theta = \int \frac{p(y\vert \theta)p(\theta)}{p(y)} d\theta = 1\]

&lt;p&gt;We can use this constraint to solve for $p(y)$, although it is easier said than done.&lt;/p&gt;

&lt;h3 id=&quot;preparing-to-integrate&quot;&gt;Preparing to integrate&lt;/h3&gt;

&lt;p&gt;We want to integrate the posterior over all possible values of $\theta_M$ and $\theta_R$. Since our only criteria are $0\leq \theta_M, \theta_R$ and $\theta_M+\theta_R \leq 1$, then the possible values all lie on the triangle contained by $(0,0),(0,1),(1,0)$.&lt;/p&gt;

&lt;p&gt;So harkening back to &lt;a href=&quot;https://tutorial.math.lamar.edu/classes/calciii/digeneralregion.aspx&quot;&gt;Calculus III&lt;/a&gt;, we want to perform a double integration of the posterior from $\theta_R = 0$ to $\theta_R = 1$ on the region contained by $g_1(\theta_R) = 0$ below and $g_2(\theta) = -\theta_R+1$ above.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20221121132444.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The region over which we are integrating&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So all that’s left is to plop the posterior under a double integral with these bounds. Because $p(y)$ is a constant with respect to $\theta_R$ and $\theta_M$, we can pull it out of the integral immediately:
\(\begin{align*}
&amp;amp;\int p(\theta\vert y)d\theta =\\
&amp;amp;\frac{1}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} d\theta_M d\theta_R = 1
\end{align*}\)&lt;/p&gt;

&lt;h3 id=&quot;tackling-the-integral&quot;&gt;Tackling the integral&lt;/h3&gt;

&lt;p&gt;So we need solve the integral, and then we can solve the resulting equation for $p(y)$.  Let’s pull out some more constants:&lt;/p&gt;

\[\begin{align}
\frac{1}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} d\theta_M d\theta_R = \\
\frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)}{p(y)\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\int^1_0\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M d\theta_R
\end{align}\]

&lt;p&gt;Let’s denote the constant $\frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}$ as $C$. Now we are dealing with&lt;/p&gt;

\[\frac{C}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M d\theta_R\]

&lt;h4 id=&quot;the-interior-integral&quot;&gt;The interior integral&lt;/h4&gt;

&lt;p&gt;Focusing on the interior integral, we can remove the constants to get something like&lt;/p&gt;

\[\frac{C\theta_{R}^{y_2 + \beta - 1}}{p(y)}\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M\]

&lt;p&gt;This brings to mind an integral of the form&lt;/p&gt;

\[\int_0^k t^{a-1}(1-t)^{b-1}dt\]

&lt;p&gt;Which is known as the &lt;a href=&quot;https://mathworld.wolfram.com/IncompleteBetaFunction.html&quot;&gt;incomplete beta function&lt;/a&gt;, a generalization of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Beta_function#Relationship_to_the_gamma_function&quot;&gt;beta function&lt;/a&gt;. The solution can be expressed in terms of hypergeometric functions.&lt;/p&gt;

&lt;p&gt;However, our situation is a slight variant on the incomplete beta function, something like&lt;/p&gt;

&lt;p&gt;\(\int_0^k t^{a-1}(c-t)^{b-1}dt = c^{b-1}\int_0^k t^{a-1}(1-\frac{t}{c})^{b-1}dt\)
What if we substitute $u = \frac{t}{c}, du=\frac{1}{c}dt$?&lt;/p&gt;

\[c^{b-1}\int_0^{k/c} (cu)^{a-1}(1-u)^{b-1}cdu = c^{a+b-1}\int_0^{k/c} u^{a-1}(1-u)^{b-1}du\]

&lt;p&gt;Hell yeah, that’s an incomplete beta function! So we can solve the inner integral as an incomplete beta function using substitution. In our case, $c=(1-\theta_R)$. Then&lt;/p&gt;

\[\begin{align}
\frac{C\theta_{R}^{y_2 + \beta - 1}}{p(y)}\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M = \\
\frac{C\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}}{p(y)}\int^{1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M)^{y_3 + \gamma - 1} d\theta_M 
\end{align}\]

&lt;p&gt;Surprisingly, the bounds simplified to $0,1$, so this is actually just a boring old beta function, which we can represent as&lt;/p&gt;

\[\begin{align}
\frac{C\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}}{p(y)}\int^{1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M)^{y_3 + \gamma - 1} d\theta_M = \\
\frac{C\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}}{p(y)}B(y_1+\alpha, y_3+\gamma)
\end{align}\]

&lt;h4 id=&quot;the-second-integral&quot;&gt;The second integral&lt;/h4&gt;

&lt;p&gt;Alright, so this leaves us with the remaining integral, which turns out to be a beta function as well. How convenient!&lt;/p&gt;

\[\begin{align}
\frac{C}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M d\theta_R = \\
\frac{C}{p(y)}B(y_1+\alpha, y_3+\gamma)\int^1_0\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}d\theta_R = \\
\frac{C}{p(y)}B(y_1+\alpha, y_3+\gamma)B(y_2+\beta, y_1+y_3+\alpha+\gamma)
\end{align}\]

&lt;p&gt;This appears suspiciously asymmetric at first, but I expanded out the beta functions in terms of gamma functions and discovered that&lt;/p&gt;

\[\begin{align}
\frac{C}{p(y)}B(y_1+\alpha, y_3+\gamma)B(y_2+\beta, y_1+y_3+\alpha+\gamma) = \\
\frac{C}{p(y)}\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)}
\end{align}\]

&lt;p&gt;Which is very pretty and symmetrical, so all is well in the world.&lt;/p&gt;

&lt;h3 id=&quot;the-marginal-likelihood&quot;&gt;The marginal likelihood&lt;/h3&gt;

&lt;p&gt;Okay, so finally we are ready to solve the following equation for $p(y)$&lt;/p&gt;

\[\begin{align}
\int \frac{p(y\vert \theta)p(\theta)}{p(y)} d\theta = \frac{C}{p(y)}\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)} = 1 \implies\\
p(y) = C\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)} = \\
\frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)} = \\
\frac{\Gamma(y_1+y_2+y_3+1)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)}\frac{B(y_1+\alpha, y_2+\beta, y_3+\gamma)}{B(\alpha,\beta,\gamma)}
\end{align}\]

&lt;p&gt;So what? Are we done? How do we know we’re correct?&lt;/p&gt;

&lt;h3 id=&quot;validating-the-marginal-likelihood&quot;&gt;Validating the marginal likelihood&lt;/h3&gt;

&lt;p&gt;We can test our formula by comparing to a Monte Carlo approximation. We’ll fix values of $\alpha, \beta, \gamma$ as well as $y_1,y_2,y_3$. Then, we can do a Monte Carlo approximation over the prior to approximate the probability of our $y$ values. It should match the output of this formula.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scipy.special import gammaln as Gammaln
from scipy.stats import describe
import numpy as np
from numpy.random import default_rng

rng = default_rng()

def marginal_likelihood_prob(y_1, y_2, y_3, alpha, beta, delta):
    '''analytically-derived formula for p(y)'''
    prob = Gammaln(y_1+y_2+y_3+1) - (Gammaln(y_1+1)+Gammaln(y_2+1)+Gammaln(y_3+1))
    prob -= multivar_betaln(alpha, beta, delta)
    prob += multivar_betaln(y_1 + alpha, y_2 + beta, y_3 + delta)
    return np.exp(prob)

def likelihood_prob(y_1, y_2, y_3, theta_m, theta_r):
'''analytically derived formula for p(y|theta)'''
    prob = Gammaln(y_1+y_2+y_3+1) - (Gammaln(y_1+1)+Gammaln(y_2+1)+Gammaln(y_3+1))
    prob += y_1*np.log(theta_m) + y_2*np.log(theta_r) + y_3*np.log(1-theta_m-theta_r)
    return np.exp(prob)

def sample_prior(alpha, beta, delta):
'''generates a random sample from the p(theta) given hyperparameters'''
    return rng.dirichlet([alpha, beta, delta])

def naive_monte_carlo_approx(y_1, y_2, y_3, alpha, beta, delta, n=1000):
'''approximates the marginal likelihood using the prior and monte carlo approximation'''
    probs = []
    for ind in range(0, n):
        theta = sample_prior(alpha, beta, delta)
        probs.append(likelihood_prob(y_1, y_2, y_3, theta[0], theta[1]))
    return probs

if __name__ == '__main__':
y_1, y_2, y_3 = (200, 250, 200)
alpha, beta, delta = (200, 250, 200)

print('Theoretical')
print(marginal_likelihood_prob(y_1, y_2, y_3, alpha, beta, delta))

print('Monte Carlo Simulation')
print(describe(naive_monte_carlo_approx(y_1, y_2, y_3, alpha, beta, delta, n=100000000)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the output we get (which took quite a while; you can turn down &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; for faster results) is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captainofthedishwasher:~$ python3 test_marginal_likelihood.py
Theoretical
0.0006405754905324117
Monte Carlo Simulation
DescribeResult(nobs=100000000, minmax=(1.3299205457019604e-11, 0.0012818163603194933), mean=0.0006406077324059656, variance=1.3696618923819785e-07, skewness=0.0008881199809568082, kurtosis=-1.2002701686108446)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The mean value from the Monte Carlo simulation is a pretty good approximation of the theoretical value. I tested some other values of $y$ and $\theta$ and felt good about the results.&lt;/p&gt;

&lt;p&gt;So we’ve finally obtained an analytical formula for $p(y)$. Easy as.&lt;/p&gt;

&lt;h2 id=&quot;calculating-the-posterior&quot;&gt;Calculating the posterior&lt;/h2&gt;

&lt;p&gt;I’m going to be honest with you. The previous section, calculating $p(y)$, was brutal. It took a psychological toll on me. I want to be done. But we are so close. We can do this. Soon we will have a Runescape PVP attack prediction model. I just need to hang in there.&lt;/p&gt;

&lt;p&gt;Now that we’ve calculated all of our terms, we’re ready for the whole enchilada: the posterior. Recall once more that we want to calculate $p(\theta\vert y)$ via Baye’s theorem,&lt;/p&gt;

\[p(\theta\vert y) = \frac{p(y\vert \theta)p(\theta)}{p(y)}\]

&lt;p&gt;And we now know the numerator and denominator:&lt;/p&gt;

\[p(y\vert \theta)p(\theta) = \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

\[p(y) = \frac{\Gamma(y_1+y_2+y_3+1)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)}\frac{B(y_1+\alpha, y_2+\beta, y_3+\gamma)}{B(\alpha,\beta,\gamma)}\]

&lt;p&gt;Now we need only put them together. I can already see that a lot of terms are going to cancel (thank god)&lt;/p&gt;

\[p(\theta\vert y) = \frac{\Gamma(\alpha + \beta + \gamma)B(\alpha,\beta,\gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{B(y_1+\alpha, y_2+\beta, y_3+\gamma)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

&lt;p&gt;Which, shockingly, simplifies further since we can cancel some multivariate beta functions:&lt;/p&gt;

\[p(\theta\vert y) = \frac{1}{B(y_1+\alpha, y_2+\beta, y_3+\gamma)}\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}\]

&lt;p&gt;And there you have it. That’s the posterior. The source of all of our woes and suffering. It’s a boring Dirichlet distribution. What a crazy turn of events! The model is complete. I can’t believe this final step was so easy.&lt;/p&gt;

&lt;p&gt;Frankly, I’m not really sure what to do now that we’re here.&lt;/p&gt;

&lt;h1 id=&quot;using-the-bayesian-model&quot;&gt;Using The Bayesian Model&lt;/h1&gt;

&lt;p&gt;Oh god, I never thought we’d get this far. I’m really not prepared at all. I’m going to have to put together a dataset and write a program.&lt;/p&gt;

&lt;p&gt;Your eyes will simply jump from these sentences to the next as you indulge in the fruits of my labor. Meanwhile, I’m going to have to dig out my laptop, extract all my combat data (if it still exists) to JSON format, figure out how to extract opponent attacks from that, pipe that data into a program, simulate fights, update the posterior, generate some visualizations, structure it all into a narra-&lt;/p&gt;

&lt;h2 id=&quot;the-data-1&quot;&gt;The data&lt;/h2&gt;

&lt;p&gt;I had a total of 57 fights logged in PvPTracker (I leave my win/loss ratio to the reader’s imagination). The average fight had 24 opponent attacks with a variance of 190, minimum of 3, and maximum of 50. There were a total of 1419 opponent attacks logged.&lt;/p&gt;

&lt;p&gt;About 34% of opponent attacks were melee, 39% were ranged, and 27% were magic. This makes sense, as beginners generally consider ranged the “easiest” attack style, while magic is the “hardest”.&lt;/p&gt;

&lt;p&gt;I decided we’ll randomly select 10 fights to actually test the model and simulate performing posterior updates during combat. We’ll use the other 47 to determine the prior hyperparameters, $\alpha,\beta,\gamma$.&lt;/p&gt;

&lt;h2 id=&quot;initializing-the-model&quot;&gt;Initializing the model&lt;/h2&gt;

&lt;p&gt;$\alpha$ will be the total number of Melee attacks, $\beta$ will the the total of Ranged attacks, and $\gamma$ corresponds to Magic attacks from our 47 prior fights.&lt;/p&gt;

&lt;p&gt;Now that the hyperparameters are done, I decided to initialize the model parameters, $\theta_M,\theta_R$ to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_distribution#Moments&quot;&gt;expected values of the prior distribution&lt;/a&gt; (which is easy to calculate since it’s a Dirichlet distribution), namely&lt;/p&gt;

\[\theta_M = \frac{\alpha}{\alpha+\beta+\gamma}, \; \theta_R = \frac{\beta}{\alpha+\beta+\gamma}\]

&lt;p&gt;Now our model is fully initialized, and it’s ready to perform inferences and posterior updates!&lt;/p&gt;

&lt;h2 id=&quot;posterior-updates&quot;&gt;Posterior updates&lt;/h2&gt;

&lt;p&gt;Updating $\theta_M,\theta_R$ using the posterior $p(\theta\vert y)$ is actually quite simple. Because the posterior is a Dirichlet distribution, we can update the model parameters to the expected value of the posterior quite easily:&lt;/p&gt;

\[\theta_M = \frac{\alpha+y_1}{\alpha+\beta+\gamma+y_1+y_2+y_3}, \; \theta_R = \frac{\beta+y_2}{\alpha+\beta+\gamma+y_1+y_2+y_3}\]

&lt;p&gt;Now, I’m trying to stay cool here, but the astute reader has probably observed something quite distressing.&lt;/p&gt;

&lt;p&gt;The model is just calculating the average over all attacks. That’s it. There’s no further insights here.&lt;/p&gt;

&lt;p&gt;That’s so stupid. What a ripoff. I can’t believe I did all those calculations for a model that an elementary schooler could construct.&lt;/p&gt;

&lt;h1 id=&quot;conclusion-this-model-sucks&quot;&gt;Conclusion: This Model Sucks&lt;/h1&gt;

&lt;p&gt;There’s not much to be said about the data since the model just calculates averages.&lt;/p&gt;

&lt;p&gt;The real insight comes from the fact that I wasted so much time deriving this model. How did I do it? Will this happen to me again?&lt;/p&gt;

&lt;p&gt;I think that the simplistic outcome should have been predictable from my assumptions. I reduced the problem to a multinomial model, which led to a very simple algorithm. I could probably impose some structure to make the model more interesting, but I’m tired and I want to move on with my life.&lt;/p&gt;

&lt;p&gt;Here are some ideas I had for salvaging the model to make it more interesting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make it a hierarchical model where the multinomial parameters for each fight come from another distribution
    &lt;ul&gt;
      &lt;li&gt;Could allow for clustering of opponent behaviors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Consider correlations between successive attacks instead of independent attacks&lt;/li&gt;
  &lt;li&gt;Integrate variables such as health into the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But these are for another day when I am not so tired. Now, I must rest.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Weakly Supervised Video Segmentation In Runescape</title><link href="http://localhost:4000/2023/01/03/runescapesegmentation.html" rel="alternate" type="text/html" title="Weakly Supervised Video Segmentation In Runescape" /><published>2023-01-03T00:00:00-05:00</published><updated>2023-01-03T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/03/runescapesegmentation</id><content type="html" xml:base="http://localhost:4000/2023/01/03/runescapesegmentation.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#related-work&quot; id=&quot;markdown-toc-related-work&quot;&gt;Related Work&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problem-statement-and-implementation&quot; id=&quot;markdown-toc-problem-statement-and-implementation&quot;&gt;Problem Statement and Implementation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#initialization-and-data-collection&quot; id=&quot;markdown-toc-initialization-and-data-collection&quot;&gt;Initialization and Data Collection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#video-segmentation&quot; id=&quot;markdown-toc-video-segmentation&quot;&gt;Video Segmentation&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#postprocessing&quot; id=&quot;markdown-toc-postprocessing&quot;&gt;Postprocessing&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#future-frame-prediction&quot; id=&quot;markdown-toc-future-frame-prediction&quot;&gt;Future Frame Prediction&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experimental-results&quot; id=&quot;markdown-toc-experimental-results&quot;&gt;Experimental Results&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cnn-training&quot; id=&quot;markdown-toc-cnn-training&quot;&gt;CNN Training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-segmentation&quot; id=&quot;markdown-toc-basic-segmentation&quot;&gt;Basic Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#postprocessing-1&quot; id=&quot;markdown-toc-postprocessing-1&quot;&gt;Postprocessing&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#spatial-diffusion&quot; id=&quot;markdown-toc-spatial-diffusion&quot;&gt;Spatial Diffusion&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#temporal-diffusion&quot; id=&quot;markdown-toc-temporal-diffusion&quot;&gt;Temporal Diffusion&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#future-frame-prediction-1&quot; id=&quot;markdown-toc-future-frame-prediction-1&quot;&gt;Future Frame Prediction&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#retrospective&quot; id=&quot;markdown-toc-retrospective&quot;&gt;Retrospective&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post is actually an old project of mine from grad school. The goal was to find a way to automatically generate training data for a computer vision algorithm specifically in the context of Old School Runescape. You can find the original paper &lt;a href=&quot;https://drive.google.com/file/d/14LsWzqAO1FZIG-KbWYlGT2TAjHHjzMLX/view&quot;&gt;here&lt;/a&gt; and the source code &lt;a href=&quot;https://github.com/horenbergerb/OSRS_Optical_Recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Collecting training data for niche applications of computer vision is difficult. We propose a method for performing video segmentation which is tailored point-and-click video games featuring tooltips. We focus specifically on Old School Runescape.&lt;/p&gt;

&lt;p&gt;The method is “weakly-supervised” in that it does not need to be trained on complete segmentation masks. Instead, a model is trained to produce classifications for 32x32 subregions of the full image.&lt;/p&gt;

&lt;p&gt;Segmenting by classifying a grid of subregions has costs and benefits. Some spatial and temporal information is lost. The resolution of the segmentation mask is low. On the other hand, we will show that the training data can be automatically generated in large quantities for certain games. Additionally, the segmentation is fast and can be done in real time.&lt;/p&gt;

&lt;p&gt;We also show that segmentation masks can be postprocessed to reincorporate lost information or induce desirable qualities.&lt;/p&gt;

&lt;p&gt;Finally, we make use of the segmentation masks by attempting to predict future segmentation masks in Runescape. We find that it is possible to marginally outperform baseline methods for future prediction for the time scale of about 0.75 seconds into the future.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/segmentedscreen.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A segmentation mask for identifying cows in Old School Runescape&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Video segmentation is a highly nontrivial problem with many applications. The goal of video segmentation is to divide video into regions in a way that distinguishes objects or areas of interest. It is a notoriously difficult problem, due largely to its high dimensionality.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/d/d4/Image_segmentation.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example image segmentation from Wikimedia Commons&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It is difficult to apply machine learning to video segmentation since labeled training data requires an exceptional amount of preparation. This is especially problematic when publicly-available training datasets do not exist, which is the case for many tailored use cases.&lt;/p&gt;

&lt;p&gt;This paper presents a weakly-supervised machine learning algorithm for video segmentation. By making certain assumptions, this algorithm reduces the problem of video segmentation to a problem of many low-resolution image classifications.&lt;/p&gt;

&lt;p&gt;The associated training data can be collected easily for certain video games by using an automated procedure. This is demonstrated for the video game “Old School Runescape.”&lt;/p&gt;

&lt;p&gt;The design of the model comes at a cost. The model assumes that segmentation masks can be generated in a region using only local spatial information. Additionally, region membership is assumed constant within 32x32 regions, and thus the segmentation masks have low resolution.&lt;/p&gt;

&lt;p&gt;These assumptions restrict the applications of the model compared to other video segmentation method. However, this model performs well when regions are distinguishable based on local properties and fine-grained segmentation is not essential. Additionally, postprocessing can relax the assumptions of locality by reincorporating information from other regions.&lt;/p&gt;

&lt;p&gt;Finally, the question is raised whether one can use the low-dimension segmentation masks for tasks such as future frame prediction. This is investigated in the case of training data from Old School Runescape, and the results are tentatively affirmative, although further investigation is necessary.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;Some of the oldest image segmentation algorithms relied on the detection of edges. &lt;a href=&quot;https://ieeexplore.ieee.org/document/4767851&quot;&gt;Canny edge detection&lt;/a&gt; and the &lt;a href=&quot;http://www.bmva.org/bmvc/1988/avc-88-023.pdf&quot;&gt;Harris corner and edge detector&lt;/a&gt; both use intensity gradients to detect edges. Edges forming closed loops or spanning the image can be used as divisors between regions, thus providing a simple model for image segmentation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/Pasted image 20230103144534.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example of the Canny edge detector &lt;a href=&quot;https://en.wikipedia.org/wiki/Canny_edge_detector&quot;&gt;from Wikipedia&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;http://library.isical.ac.in:8080/xmlui/bitstream/handle/10263/5304/A%20review%20on%20image%20segmentation%20techniques-PR-26-9-1993-%20p%201277-1294.pdf?sequence=1&amp;amp;isAllowed=y&quot;&gt;Region-based methods&lt;/a&gt; such as region growing begin by placing seed points and allowing regions to expand based on the properties of marginal pixels, such as intensity, texture, or color.&lt;/p&gt;

&lt;p&gt;Another popular technique for image segmentation is &lt;a href=&quot;https://ijcset.net/docs/Volumes/volume2issue1/ijcset2012020103.pdf&quot;&gt;histogram thresholding&lt;/a&gt;, in which the histogram of a black-and-white image is partitioned by some threshold in order to determine to classify each pixel based on intensity.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/Pasted image 20230103144634.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Intensity histograms of goblins tend to be similar and can be used for classification&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Regarding data-driven approaches to image segmentation, convolutional neural networks (CNNs) are a natural choice. &lt;a href=&quot;https://arxiv.org/pdf/1512.07108.pdf&quot;&gt;Convolutional neural networks&lt;/a&gt; use feature extraction to perform a variety of tasks including object detection, region of interest proposal, and image segmentation.&lt;/p&gt;

&lt;p&gt;There exists &lt;a href=&quot;https://arxiv.org/abs/1502.02734&quot;&gt;weakly-supervised approaches&lt;/a&gt; to image segmentation which use only information such as bounding boxes around objects or image-level labels to train a CNN paired with a conditional random field.&lt;/p&gt;

&lt;p&gt;One of the more successful approaches to video segmentation is the &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf&quot;&gt;Mask R-CNN&lt;/a&gt;, which performs object instance segmentation. The underlying premise is dividing an image into regions of interest (ROIs) and simultaneously producing masks for image segmentation within each ROI.&lt;/p&gt;

&lt;p&gt;Presently, it is not known to the authors whether there exists work on low-resolution image or video segmentation.&lt;/p&gt;

&lt;h1 id=&quot;problem-statement-and-implementation&quot;&gt;Problem Statement and Implementation&lt;/h1&gt;

&lt;p&gt;The problem at hand is to perform video segmentation on a video game with “tooltips.” The tooltip indicates the actions available to the player based on cursor position. In the game which we have chosen to focus on, Runescape, most all actions are performed by moving the cursor to objects and clicking them.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/tooltip_example.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The tooltip describes what the mouse is currently hovering over&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the initialization phase of the algorithm, the tooltip and the mouse position will be used to generate training data automatically for a classifier which can classify small 32x32 regions of the image.&lt;/p&gt;

&lt;p&gt;After the training data is collected and parsed, the CNN will be trained, completing the initialization.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/overviewflowchart.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The schematic for our segmentation algorithm, including data collection and training&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;During the video segmentation phase, the CNN is used to generate a segmentation mask. The mask is then postprocessed to its final form. Lastly, we use the segmentation masks to perform future frame prediction.&lt;/p&gt;

&lt;p&gt;We discuss these two phases in detail below.&lt;/p&gt;

&lt;h2 id=&quot;initialization-and-data-collection&quot;&gt;Initialization and Data Collection&lt;/h2&gt;

&lt;p&gt;The initialization procedure is illustrated below. Initialization begins by capturing raw data from Runescape gameplay. While the user plays, automated Python software performs screencaptures and simultaneously records the mouse position. This is achieved using the &lt;a href=&quot;https://python-mss.readthedocs.io/&quot;&gt;MSS&lt;/a&gt; and &lt;a href=&quot;https://pyautogui.readthedocs.io/en/latest/&quot;&gt;PyAutoGUI&lt;/a&gt; Python packages.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/initializationflowchart.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Overview of the initialization procedure&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The pairs of screenshots and cursor coordinates are then parsed into training data. To achieve this, color filtering is used on the tooltip to extract keywords. In particular, Runescape denotes the object of the tooltip’s verb in clustered shades of yellow.&lt;/p&gt;

&lt;p&gt;We use samples of these yellow shades to develop a mask based on color similarity. The resulting masks can be used to perform optical character recognition using &lt;a href=&quot;https://pypi.org/project/pytesseract/&quot;&gt;PyTesseract&lt;/a&gt;, a Python package which is a wrapper for Google’s &lt;a href=&quot;https://opensource.google/projects/tesseract&quot;&gt;Tesseract&lt;/a&gt;. All image processing in this project was performed using the &lt;a href=&quot;https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html&quot;&gt;OpenCV Python package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, a 32x32 region is cropped from the screenshot centered on the user’s cursor position. In this way, the parsed tooltip is now a text label for the noun pictured in the 32x32 image.&lt;/p&gt;

&lt;p&gt;These images and text labels together are sufficient information to train the small CNN classifier pictured below, which can predict the label associated with a given 32x32 image. Every convolutional layer of the CNN was followed by an ReLU activation function and a batch normalization layer. All machine learning in this project was performed using the &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch Python package&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/cnnarchitecture.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The CNN architecture used to classify the 32x32 subregions of the screen&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This CNN is sufficient to perform basic video segmentation which can then be further postprocessed. We discuss this further in the next section.&lt;/p&gt;

&lt;h2 id=&quot;video-segmentation&quot;&gt;Video Segmentation&lt;/h2&gt;

&lt;p&gt;With the trained CNN from the initialization phase, we can perform basic video segmentation.&lt;/p&gt;

&lt;p&gt;To achieve this, each frame of the video is broken into a grid of 32x32 regions. Then, each region is independently classified by the CNN. These classifications determine region membership. Consequently, the collective classifications of the entire grid determine a low-resolution segmentation mask for the frame.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/segmentationdiagram.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Each 32x32 subregion is separately classified to create the segmentation mask&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;postprocessing&quot;&gt;Postprocessing&lt;/h3&gt;

&lt;p&gt;The calculated segmentation mask is then postprocessed to reincorporate lost information. We implemented two methodologies for postprocessing.&lt;/p&gt;

&lt;p&gt;The first is spatial diffusion, which uses the Gauss-Seidel method to iteratively update neighboring probabilities. In particular, if $P[\mathbf{x},t]$ indicates the probabilities in grid region $\mathbf{x}$ at time $t$, and if $\eta(\mathbf{x})$ indicates the neighborhood of $\mathbf{x}$, then over each frame we iteratively apply&lt;/p&gt;

\[P[\mathbf{x},t] = \frac{1}{4}\sum_{\mathbf{y}\in\eta(\mathbf{x})} P[\mathbf{y},t]\]

&lt;p&gt;to every grid region until the process converges. In this way, some global spatial information is reincorporated into the segmentation by diffusing probabilities outward into their neighboring regions.&lt;/p&gt;

&lt;p&gt;The other postprocessing method is temporal averaging, in which each frame’s probabilities are recomputed as&lt;/p&gt;

\[P[\mathbf{x},t] = \frac{1}{2}(P[\mathbf{x},t]+P[\mathbf{x},t-1])\]

&lt;p&gt;This reincorporates temporal information into the segmentation mask and smooths temporal anomalies.&lt;/p&gt;

&lt;p&gt;Certainly many other possible filters or postprocessing methods exist, such as spatiotemporal filtering. However, these are left as an exercise to the reader.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/videosegmentationoverview.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Overview of the actual segmentation and future frame prediction&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;future-frame-prediction&quot;&gt;Future Frame Prediction&lt;/h3&gt;

&lt;p&gt;The final phase of the video segmentation is utilization, in which we attempt to predict future segmentation masks given a set of past segmentation masks. This task was performed using a 3DCNN architecture detailed below.&lt;/p&gt;

&lt;p&gt;We used three frames as inputs and predicted one future frame as output. Different future distances and intervals between input frames were tested to find the regimes in which future frame prediction was most viable.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/3dcnndiagram.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Architecture of the 3DCNN used for future frame prediction&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h1&gt;

&lt;h2 id=&quot;cnn-training&quot;&gt;CNN Training&lt;/h2&gt;

&lt;p&gt;The CNN for 32x32 image classification was trained on two labels, “cow” and “none.” Each label had approximately 900 training images and 100 validation images.&lt;/p&gt;

&lt;p&gt;The model converged very quickly and achieved accuracy of approximately 95\%. It was also attempted to train the model on 7 different labels, including “goblin,” “tree,” and “chicken.” Each label had between 300 to 500 images.&lt;/p&gt;

&lt;p&gt;In this case, accuracy was much lower at around 60\%, possibly suggesting the need for more training data or a more elaborate architecture. For this reason the project was limited to the case of “cow” vs. “none” labels.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/cnntraining.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The validation accuracy rapidly converged when segmenting with classes “cow” and “not cow”&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;basic-segmentation&quot;&gt;Basic Segmentation&lt;/h2&gt;

&lt;p&gt;One difficulty of the project was the unavailability of validation data. As a result, there is no easy way to compare segmentation results to ground truth.&lt;/p&gt;

&lt;p&gt;Due to time constraints, we were not able to annotate a ground truth for comparison. We illustrate here a typical output of segmentation and discuss the properties of the segmentation mask.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/origscreen.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A screenshot of the game before segmentation is performed&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/segmentedscreen.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The same screenshot with a segmentation mask applied distinguishing “cow” and “not cow”&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;One obvious shortcoming was the mislabeling of the GUI elements in the corners of the play screen. This was due in part to the data collection process.&lt;/p&gt;

&lt;p&gt;Particularly, there is a sampling bias based on where users tend to place their cursor. Additionally, samples were only kept if the 32x32 region surrounding the mouse were located entirely in the play screen, so it was relatively unlikely for the edges of the screen to be sampled compared to the center of the screen.&lt;/p&gt;

&lt;p&gt;There are other quirks of the dataset which could also contribute to some of the misclassifications.&lt;/p&gt;

&lt;p&gt;The player character is labeled as “none” by the sampling process, so it is likely they would have been included in the training dataset. However, objects which had labels such as “farmer” or “goblin” were not included in the training data.&lt;/p&gt;

&lt;p&gt;This means many objects from the game were excluded during training, and thus the data set is not as diverse as the actual playscreen. This issue is easily corrected with better data preparation procedures.&lt;/p&gt;

&lt;h2 id=&quot;postprocessing-1&quot;&gt;Postprocessing&lt;/h2&gt;

&lt;p&gt;The lack of validation data makes empirical analysis of the effects of postprocessing impossible at this time. However, it can be subjectively verified that each postprocessing method induces certain qualities.&lt;/p&gt;

&lt;h3 id=&quot;spatial-diffusion&quot;&gt;Spatial Diffusion&lt;/h3&gt;

&lt;p&gt;Small regions are smoothed out after spatial diffusion, and convex areas tend to be filled in. This smoothing could be effective when regions are expected to be large and rounded.&lt;/p&gt;

&lt;p&gt;However, if the ground truth data contains small isolated regions, then this filter is likely to cause incorrect classifications. Spatial diffusion is thus contextual, and it may be worth investigating whether it could be applied situationally.&lt;/p&gt;

&lt;p&gt;In this demo, the left-hand side shows the original segmentation mask while the right-hand side shows the mask after applying spatial diffusion.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/uYBu1VAFKXU&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;temporal-diffusion&quot;&gt;Temporal Diffusion&lt;/h3&gt;

&lt;p&gt;Temporal diffusion reduces temporally anomalous classifications, but it also induces a lag in segmentation updates when motion is occuring. This is somewhat apparent in the video.&lt;/p&gt;

&lt;p&gt;As the camera rotated counterclockwise about the player, the cows are approximately translating on the screen. With temporal averaging, the segmentation mask lags behind the cows.&lt;/p&gt;

&lt;p&gt;In this demo, the left-hand side shows the original segmentation mask while the right-hand side shows the mask after applying temporal diffusion.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/7GvkrOqumgg&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;future-frame-prediction-1&quot;&gt;Future Frame Prediction&lt;/h2&gt;

&lt;p&gt;Future frame prediction was first tested using 3 successive segmentation masks sampled at times $t-2,t-1,t$ to predict a segmentation mask 6 frames into the future at time $t+6$. 6 frames is approximately 1/7 of a second.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframepredictiondiagram.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Illustration of the future prediction inputs and outputs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The distance of 6 frames into the future was chosen based on preliminary subjective analysis. It was approximately the distance which appeared to have the best performance, although this claim needs further verification. Every model was compared to a baseline naive model which assumes no motion occurs.&lt;/p&gt;

&lt;p&gt;The training was performed with approximately 7200 frames of continuous video while validation was performed on a separate 1600 frames of continuous video.&lt;/p&gt;

&lt;p&gt;As seen in the training graphs, the model reaches peak performance on the validation set very early and then begins overfitting. This trend continues at least up to 30 epochs.&lt;/p&gt;

&lt;p&gt;At peak performance, the model achieved a top tile classification accuracy of 86.608% on the validation data, while the baseline naive model achieved 86.231%. Thus, the performance is at best only a marginal improvement upon basic assumptions.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframetraining.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Overfitting begins after very few epochs, and the best performance only marginally outperforms naive future prediction methods&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;However, a second model was attempted using not successive frames as inputs but instead using a step size of 5 frames, as depicted in Figure~\ref{futureframediagram2}. In this case, the predictions were much further into the future at 30 frames, or approximately 3/4 of a second.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframediagram2.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Illustration of the future prediction inputs and outputs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With this new model, training appeared subjectively similar. Although the validation accuracy was lower at 75.560%, the model performed notably better than the naive approach, which had an accuracy of 71.672%.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframetraining2.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Although overfitting still occurs, the model outperforms naive methods when predicting further into the future&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Although these results are promising, more research is needed. Particularly, it is worth investigating whether the performance can be improved by the inclusion of more training data.&lt;/p&gt;

&lt;p&gt;Additionally, many future frame prediction models use an attention mechanism such as long short-term memory. The effects of including an attention mechanism should be investigated.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This project has demonstrated that it is possible to collect the necessary data for performing video segmentation on a video game such as Runescape. In addition, the video segmentation can be augmented with postprocessing methods, and the segmentation masks can be utilized to perform tasks such as future frame prediction.&lt;/p&gt;

&lt;p&gt;There are many remaining questions which deserve further investigation. For example, only two simple postprocessing filters were examined. Spatiotemporal filters might offer better performance, or perhaps there is an opportunity to apply machine learning in the postprocessing step.&lt;/p&gt;

&lt;p&gt;Additionally, our future frame prediction model has many obvious extensions. It also remains to detail the performance over different time scales and with varying amounts of input information.&lt;/p&gt;

&lt;p&gt;Finally, it may be of interest to extend these methodologies to other games with different graphics engines. The methods presented here serve as a springboard for more comprehensive investigations.&lt;/p&gt;

&lt;h1 id=&quot;retrospective&quot;&gt;Retrospective&lt;/h1&gt;

&lt;p&gt;I wrote this paper about a year ago. I was very proud of this project at the time. I still am. I felt like this was a viable solution to a real problem I was facing at the time (i.e. how to make Runescape bots).&lt;/p&gt;

&lt;p&gt;There are a few things I didn’t really explore in this paper which I would like to revisit some time.&lt;/p&gt;

&lt;p&gt;For one, I think I naively thresholded classifications at 0.5. I now know that one should experimentally determine how to threshold the classifier outputs.&lt;/p&gt;

&lt;p&gt;I’d also like to really document the speed of this system. I know it was generating multiple masks per second, but how much time is really left over for other processing? Do we have enough spare compute power to beef up the classifier’s architecture?&lt;/p&gt;

&lt;p&gt;For that matter, should I look into using a more typical classifier architecture, such as YOLO or something faster maybe?&lt;/p&gt;

&lt;p&gt;Lastly, I would like to work on the future frame prediction. I think one of the major issues is that the future frame prediction has no fully connected layers. What’s up with that?? I would also be interested in using transformers instead of 3DConv layers.&lt;/p&gt;

&lt;p&gt;Anyways, I really liked this project, so I wanted to show it off on my blog. I’m sure it’s not my most exciting work, but you’re going to have to deal with it.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Teaching ChatGPT to Play Runescape</title><link href="http://localhost:4000/2022/12/15/chatgptrunescape.html" rel="alternate" type="text/html" title="Teaching ChatGPT to Play Runescape" /><published>2022-12-15T00:00:00-05:00</published><updated>2022-12-15T00:00:00-05:00</updated><id>http://localhost:4000/2022/12/15/chatgptrunescape</id><content type="html" xml:base="http://localhost:4000/2022/12/15/chatgptrunescape.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#reducing-runescape-to-text&quot; id=&quot;markdown-toc-reducing-runescape-to-text&quot;&gt;Reducing Runescape to Text&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nlp-or-reinforcement-learning-for-video-games&quot; id=&quot;markdown-toc-nlp-or-reinforcement-learning-for-video-games&quot;&gt;NLP or Reinforcement Learning for Video Games?&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforcement-learning&quot; id=&quot;markdown-toc-reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nlp&quot; id=&quot;markdown-toc-nlp&quot;&gt;NLP&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminary-investigation-of-chatgpt&quot; id=&quot;markdown-toc-preliminary-investigation-of-chatgpt&quot;&gt;Preliminary Investigation of ChatGPT&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#what-does-chatgpt-know-about-runescape&quot; id=&quot;markdown-toc-what-does-chatgpt-know-about-runescape&quot;&gt;What Does ChatGPT Know About Runescape?&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#general-information-about-runescape&quot; id=&quot;markdown-toc-general-information-about-runescape&quot;&gt;General Information About Runescape&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review&quot; id=&quot;markdown-toc-review&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#overview-of-the-skill-system&quot; id=&quot;markdown-toc-overview-of-the-skill-system&quot;&gt;Overview of The Skill System&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-1&quot; id=&quot;markdown-toc-review-1&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#detailed-information-about-mining&quot; id=&quot;markdown-toc-detailed-information-about-mining&quot;&gt;Detailed Information About Mining&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-2&quot; id=&quot;markdown-toc-review-2&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#general-quests&quot; id=&quot;markdown-toc-general-quests&quot;&gt;General Quests&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-3&quot; id=&quot;markdown-toc-review-3&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#specific-quests&quot; id=&quot;markdown-toc-specific-quests&quot;&gt;Specific Quests&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-4&quot; id=&quot;markdown-toc-review-4&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#simple-roleplay&quot; id=&quot;markdown-toc-simple-roleplay&quot;&gt;Simple Roleplay&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#review-5&quot; id=&quot;markdown-toc-review-5&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#chatgpt-plays-runescape&quot; id=&quot;markdown-toc-chatgpt-plays-runescape&quot;&gt;ChatGPT Plays Runescape&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#warmup&quot; id=&quot;markdown-toc-warmup&quot;&gt;Warmup&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chatgpt-gets-the-hang-of-it&quot; id=&quot;markdown-toc-chatgpt-gets-the-hang-of-it&quot;&gt;ChatGPT Gets the Hang of It&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#intermission&quot; id=&quot;markdown-toc-intermission&quot;&gt;Intermission&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chatgpt-cuts-down-a-tree&quot; id=&quot;markdown-toc-chatgpt-cuts-down-a-tree&quot;&gt;ChatGPT Cuts Down a Tree&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reflection&quot; id=&quot;markdown-toc-reflection&quot;&gt;Reflection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#future-steps&quot; id=&quot;markdown-toc-future-steps&quot;&gt;Future Steps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;reducing-runescape-to-text&quot;&gt;Reducing Runescape to Text&lt;/h1&gt;

&lt;p&gt;Some of you may be aware that I have previously worked on using computer vision to extract data from a live feed of &lt;a href=&quot;https://oldschool.runescape.com/&quot;&gt;Runescape&lt;/a&gt;. For example, I have &lt;a href=&quot;https://horenbergerb.github.io/2022/03/04/runescapelocationfinding.html&quot;&gt;estimated the player’s global location&lt;/a&gt; and created a &lt;a href=&quot;https://horenbergerb.github.io/2022/07/19/runescapeobjectdetection.html&quot;&gt;detector of interactible objects&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have also written other scripts to parse more information from the game. More than I care to admit, frankly.&lt;/p&gt;

&lt;p&gt;Really, I’ve been working on reducing the game to an interface that describes the game state completely in text. The interface will also accept written commands which control the character.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/game_to_text_diagram 1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But why? What’s the use of turning Runescape into a text-based adventure?&lt;/p&gt;

&lt;p&gt;Originally I was just practicing computer vision and ML techniques. I started to wonder if I could make an interface that an ML algorithm could use to play Runescape.&lt;/p&gt;

&lt;p&gt;Realistically, I had no shot. Training an agent to imitate a typical Runescape player was far beyond the achievable possibilities of an armchair expert like me.&lt;/p&gt;

&lt;p&gt;Well, things might be changing with the release of &lt;a href=&quot;https://openai.com/blog/chatgpt/&quot;&gt;ChatGPT&lt;/a&gt;. ChatGPT is a natural language processing (NLP) model which can perform a variety of tasks and regurgitate facts about many topics (although it is often incorrect).&lt;/p&gt;

&lt;p&gt;In this article, I wanted to explore the possibility that a text-based representation of Runescape could allow ChatGPT to play the game and effectively replicate human players.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215204212.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Proof that OpenAI hates PKers and they want to remove the wilderness&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;nlp-or-reinforcement-learning-for-video-games&quot;&gt;NLP or Reinforcement Learning for Video Games?&lt;/h1&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Typically automation of video games is approached using &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;reinforcement learning&lt;/a&gt;. This makes a lot of sense for the following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Games are often objective-based in a quantifiable way&lt;/li&gt;
  &lt;li&gt;Games are typically a sequential process of decision-making over time where your current state and actions depend on your history&lt;/li&gt;
  &lt;li&gt;Games can often be replayed arbitrarily many times as part of the training process&lt;/li&gt;
  &lt;li&gt;Any quantifiable interface for a video game is compatible with reinforcement learning
    &lt;ul&gt;
      &lt;li&gt;ex) inputs from joysticks, button presses, mouse clicks&lt;/li&gt;
      &lt;li&gt;ex) game state represented as a large structured dataset of discrete/continuous quantities&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Famously, OpenAI used reinforcement learning to train &lt;a href=&quot;https://openai.com/five/&quot;&gt;OpenAI Five&lt;/a&gt; to play DOTA. The algorithm was eventually able to beat many professional teams.&lt;/p&gt;

&lt;h2 id=&quot;nlp&quot;&gt;NLP&lt;/h2&gt;

&lt;p&gt;NLP is probably not a good choice for video games in general. I shouldn’t have to explain why. It’s just not designed to play video games.&lt;/p&gt;

&lt;p&gt;However, some games have properties which make them particularly difficult for reinforcement learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Some games do not have clear victory conditions. In this case, “typical” gameplay can be hard to quantify and often very diverse&lt;/li&gt;
  &lt;li&gt;Some games have elaborate mechanics relying heavily on &lt;a href=&quot;https://en.wikipedia.org/wiki/Common_sense&quot;&gt;common sense&lt;/a&gt;, which reinforcement learning does not have
    &lt;ul&gt;
      &lt;li&gt;ex) knowing that you need an axe to cut down trees, that using a fishing rod on a fishing spot will catch fish and give you fishing experience, that eating food will heal you, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Some games are large and have no start and end; iterating multiple playthroughs for training is not feasible.
    &lt;ul&gt;
      &lt;li&gt;ex) most MMORPG games are far too large to train with repeated playthroughs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NLP might be more successful under these circumstances due to its general knowledge. It can approximate common sense and make reasonable statements about “typical” behavior of an MMO player. Potentially, NLP models could transfer this knowledge to a game without specific training. Because of these properties, it’s possible that an NLP model might be able to interact with a game more effectively than a reinforcement learning algorithm.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215215209.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;ChatGPT has a lot of general knowledge about MMORPGs, but it still makes mistakes&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Runescape has many of the properties which make direct reinforcement learning very difficult. Could NLP have a competitive advantage here?&lt;/p&gt;

&lt;h1 id=&quot;preliminary-investigation-of-chatgpt&quot;&gt;Preliminary Investigation of ChatGPT&lt;/h1&gt;

&lt;p&gt;Enough fuckin’ around. What does ChatGPT know about Runescape, and can it farm gold for me?&lt;/p&gt;

&lt;p&gt;I began by investigating what kinds of information ChatGPT might contain regarding Runescape. I asked a series of questions, first about the game in general, and then more specific concepts. Here is some of the dialogue.&lt;/p&gt;

&lt;p&gt;Afterwards, I tried to have ChatGPT roleplay simple situations within the game to describe what actions it might take.&lt;/p&gt;

&lt;h2 id=&quot;what-does-chatgpt-know-about-runescape&quot;&gt;What Does ChatGPT Know About Runescape?&lt;/h2&gt;

&lt;h3 id=&quot;general-information-about-runescape&quot;&gt;General Information About Runescape&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Explain the video game Old School Runescape&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Old School Runescape (OSRS) is a massively multiplayer online role-playing game (MMORPG) developed and published by Jagex. It is a re-imagined version of the classic MMORPG Runescape, which was first released in 2001. In OSRS, players can create their own character and embark on various quests and adventures in a fantasy world filled with magic and mythical creatures. The game is known for its open-world environment and its challenging gameplay, which requires players to complete various tasks and puzzles in order to progress. OSRS has a dedicated player base and is known for its active community of players who regularly participate in in-game events and activities.&lt;/p&gt;

&lt;h4 id=&quot;review&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;Not bad. A lot of this information is accurate.  Some of the descriptors are a little boiler-plate.&lt;/p&gt;

&lt;h3 id=&quot;overview-of-the-skill-system&quot;&gt;Overview of The Skill System&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What sort of skills are available to players in Old School Runescape?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Old School Runescape, players can choose from a wide variety of skills to train and develop. Some of the skills available in the game include combat skills (such as Attack, Strength, and Defence), gathering skills (such as Mining, Fishing, and Woodcutting), and crafting skills (such as Smithing, Cooking, and Fletching). There are also several magic-based skills, such as Magic and Prayer, as well as miscellaneous skills like Thieving and Hunter.&lt;/p&gt;

&lt;p&gt;Each skill has its own unique gameplay mechanics and benefits. For example, training the Cooking skill allows players to cook and prepare food, which can be used to restore health and provide temporary stat bonuses. Training the Magic skill allows players to cast spells, which can be used in combat or to perform various tasks. And training the Thieving skill allows players to pickpocket NPCs and steal from stalls and chests.&lt;/p&gt;

&lt;p&gt;The skills available in Old School Runescape are varied and diverse, and allow players to tailor their character’s abilities to their playstyle and preferences. As you progress through the game, you can choose to focus on developing certain skills, or to train a wide variety of skills in order to become a well-rounded character.&lt;/p&gt;

&lt;h4 id=&quot;review-1&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;Pretty much dead-on. Everything here is accurate and it’s a decent summary.&lt;/p&gt;

&lt;h3 id=&quot;detailed-information-about-mining&quot;&gt;Detailed Information About Mining&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What items and skills are needed to mine copper ore in Old School Runescape?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To mine copper ore in Old School RuneScape, you will need the following items and skills:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A pickaxe: You will need a bronze, iron, steel, or mithril pickaxe to mine copper ore. A higher level pickaxe will allow you to mine copper ore faster, but any of these pickaxes will work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mining level: You will need to have at least level 1 Mining to be able to mine copper ore. As you increase your Mining level, you will be able to mine copper ore more efficiently.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A mining location: Copper ore can be found at a variety of locations throughout the game, including the Lumbridge Swamp Mines, the Al-Kharid Mine, and the Piscatoris Mining Site.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To start mining copper ore, first equip your pickaxe and then find a rock that contains copper ore. Left-click on the rock to begin mining it, and you will receive copper ore as you gather resources. You can then smelt the copper ore to turn it into a usable bar of metal, which can be used in various crafting and construction tasks throughout the game.&lt;/p&gt;

&lt;h4 id=&quot;review-2&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;Wow, this one is good. Here are the nitpicks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It didn’t list &lt;em&gt;all&lt;/em&gt; of the pickaxes&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://oldschool.runescape.wiki/w/Pickaxe&quot;&gt;A better pickaxe doesn’t make you faster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;You don’t need to equip the pickaxe to mine&lt;/li&gt;
  &lt;li&gt;You also need tin to smelt the copper&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the gist of it is correct; someone following these instructions could successfully mine copper. Even the locations are valid, and they’re (mostly) decent choices for new players too.&lt;/p&gt;

&lt;h3 id=&quot;general-quests&quot;&gt;General Quests&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What are some important quests that new players of Old School Runescape should complete?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a new player to Old School Runescape, there are several quests that you should consider completing in order to progress in the game and gain access to new content and features. Some of the most important quests for new players include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tutorial Island: This is the starting quest for all new players in Old School Runescape. It provides a basic introduction to the game’s mechanics and controls, and is a necessary starting point for all new players.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cook’s Assistant: This is a simple quest that tasks players with gathering ingredients for a cook in the village of Lumbridge. Completing this quest will reward players with experience in the Cooking skill, as well as some basic cooking equipment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Restless Ghost: This quest tasks players with finding and helping a restless ghost who is haunting the town of Lumbridge. Completing this quest will reward players with experience in the Prayer skill, as well as the ability to wield the white skull sceptre, which can be used to teleport to various locations in the game world.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rune Mysteries: This quest involves investigating a mysterious disappearance at the Rune Essence mine, and is a good introduction to the game’s magic-based skills. Completing this quest will reward players with experience in the Rune Crafting skill, as well as the ability to teleport to the Rune Essence mine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall, completing these quests will help new players to get a feel for the game and its mechanics, and will provide them with valuable experience and rewards that can help them to progress in the game.&lt;/p&gt;

&lt;h4 id=&quot;review-3&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;God damn, this is pretty good too. I mean, it’s not an optimal quest list are anything, but these are good quests for beginners to Runescape, and the descriptions are decent. Not sure about the “white skull sceptre” though. I think that’s a lie.&lt;/p&gt;

&lt;h3 id=&quot;specific-quests&quot;&gt;Specific Quests&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please summarize the plot of the Cook’s Assistant quest in Old School Runescape.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the Cook’s Assistant quest in Old School RuneScape, you play as a character who is tasked with helping the cook in the Lumbridge Castle kitchen. The cook needs your help to gather some ingredients for a special cake he is making for the Duke of Lumbridge’s birthday. You must retrieve a pot of flour, an egg, and a bucket of milk from the nearby farm. Once you have all of the ingredients, you bring them back to the cook and he uses them to make the cake. The quest is a simple and short one, but it serves as an introduction to the world of RuneScape and helps to familiarize players with the game’s mechanics and controls.&lt;/p&gt;

&lt;h4 id=&quot;review-4&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;This is exactly right. A lot of key information is conveyed accurately here. Impressive that ChatGPT knows this.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215204546.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Information about the Cook’s Assistant quest from the &lt;a href=&quot;https://oldschool.runescape.wiki/w/Cook%27s_Assistant&quot;&gt;Runescape Wiki&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;simple-roleplay&quot;&gt;Simple Roleplay&lt;/h2&gt;

&lt;p&gt;Here I wanted to see if I could get ChatGPT to act as if it were a player and describe explicit actions it would perform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s pretend you are a new player to Old School Runescape. You have just finished Tutorial Island, and you are standing in Lumbridge. What would you do first?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a new player to Old School Runescape who has just finished Tutorial Island and is standing in Lumbridge, I would likely take a moment to familiarize myself with my surroundings. This might involve exploring the town of Lumbridge and talking to the NPCs there to learn more about the game world and its lore. I might also look for any quests or tasks that are available in the area, and consider completing them in order to gain experience and rewards.&lt;/p&gt;

&lt;p&gt;Once I have a better understanding of the game and its mechanics, I would likely focus on training my character’s skills. This might involve finding resources and items that I can use to train skills like Mining, Fishing, or Woodcutting, and spending some time gathering and crafting to level up those skills. I might also look for combat training opportunities, such as fighting monsters or other enemies, in order to improve my character’s combat abilities.&lt;/p&gt;

&lt;p&gt;Overall, as a new player in Old School Runescape, my first priority would be to familiarize myself with the game world and its mechanics, and to begin training my character’s skills in order to progress and become more powerful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While exploring Lumbridge, you see several NPCs to whom you can talk, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cook&lt;/li&gt;
  &lt;li&gt;Hans&lt;/li&gt;
  &lt;li&gt;Lumbridge Guide&lt;/li&gt;
  &lt;li&gt;Father Aereck&lt;/li&gt;
  &lt;li&gt;Bob&lt;/li&gt;
  &lt;li&gt;Donie&lt;/li&gt;
  &lt;li&gt;Veos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You also see several NPCs whom you can attack, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Man (Level 2)&lt;/li&gt;
  &lt;li&gt;Rat (Level 1)&lt;/li&gt;
  &lt;li&gt;Goblin (Level 2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, you also see several trees which you could cut if you had an axe. Which of these would you interact with? Or would you instead continue exploring?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a new player to Old School Runescape, I would likely start by interacting with the various NPCs in Lumbridge. This would involve talking to them to learn more about the game world and its lore, and to see if they have any quests or tasks that I can complete. For example, I might talk to the Cook to see if he has any tasks that I can help with, or to Father Aereck to learn more about the town’s history.&lt;/p&gt;

&lt;p&gt;I would also likely consider attacking some of the low-level enemies in the area, such as the Man, Rat, or Goblin, in order to gain experience and level up my combat skills. I would only do this if I feel confident that I can handle these enemies, and if I have the necessary equipment and abilities to do so.&lt;/p&gt;

&lt;p&gt;I would not likely attempt to cut down any trees at this point, as I would need to have an axe and the necessary Woodcutting skill in order to do so. Instead, I would continue exploring the area and interacting with the NPCs and enemies there, in order to learn more about the game and to gain experience and rewards.&lt;/p&gt;

&lt;h3 id=&quot;review-5&quot;&gt;Review&lt;/h3&gt;

&lt;p&gt;Not bad. A little verbose, but the broad strokes are all valid, except that no one plays Runescape for the lore.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215204802.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;POV: You are ChatGPT standing in Lumbridge&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;chatgpt-plays-runescape&quot;&gt;ChatGPT Plays Runescape&lt;/h1&gt;

&lt;p&gt;Now I felt like it was time to mock a simple interface with ChatGPT and see how it behaved.&lt;/p&gt;

&lt;p&gt;I made up a few commands that were flexible enough to let ChatGPT act but simple enough that I could imagine writing a real program to implement them.&lt;/p&gt;

&lt;p&gt;However, for this experiment I physically controlled the player in Runescape and simply did as ChatGPT commanded. This is just a proof of concept.&lt;/p&gt;

&lt;h3 id=&quot;warmup&quot;&gt;Warmup&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pretend you are a computer program designed to play Old School Runescape just like a human would. You interact with the game through a text interface. There are a set of commands available to you:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;look&lt;/li&gt;
  &lt;li&gt;inventory&lt;/li&gt;
  &lt;li&gt;interact X&lt;/li&gt;
  &lt;li&gt;go X&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using the “look” command will return a list of the interactive NPCs and objects in the game world. Using the “inventory” command will return a list of items in your inventory. Using “interact X” where X is an item, NPC, or interactive object, will cause you to click on the object. Finally, using “go X” where X is in the set [north, east, south, west] will cause your character to move slightly in the respective cardinal direction. Please begin interacting with the text interface using only the four allowed commands.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To interact with the text interface, I would first use the “look” command to see a list of NPCs, objects, and other interactive elements in the game world. This would give me a better understanding of my surroundings and allow me to plan my next actions.&lt;/p&gt;

&lt;p&gt;Next, I would use the “inventory” command to see what items I have in my possession. This would allow me to assess my current resources and equipment, and decide what actions I can take based on what I have available.&lt;/p&gt;

&lt;p&gt;Based on the information provided by the “look” and “inventory” commands, I might then use the “interact” and “go” commands to interact with the game world. For example, if I see an NPC that I want to talk to, I would use the “interact” command to click on them and initiate a conversation. Or if I want to move to a different area, I would use the “go” command to move in a specific direction.&lt;/p&gt;

&lt;p&gt;Overall, using the four available commands, I would carefully observe and interact with the game world in order to complete quests, gather resources, and progress in the game.&lt;/p&gt;

&lt;h3 id=&quot;chatgpt-gets-the-hang-of-it&quot;&gt;ChatGPT Gets the Hang of It&lt;/h3&gt;

&lt;p&gt;NOTE: From this point on, almost all of my messages end with the next phrase (“Please respond only…”) copied/pasted, so I will omit it for brevity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please respond only with one of the four allowed commands. Do not explain your decisions or elaborate at all.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;look&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NPCs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Banker&lt;/li&gt;
  &lt;li&gt;Grand Exchange Clerk&lt;/li&gt;
  &lt;li&gt;Farrad Morrisane&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bank Deposit Box&lt;/li&gt;
  &lt;li&gt;Grand Exchange Booth&lt;/li&gt;
  &lt;li&gt;Poll Booth&lt;/li&gt;
  &lt;li&gt;Loot Chest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205208.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;inventory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inventory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205231.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Banker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bank:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coins&lt;/li&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Runite ore&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205325.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;In no world am I giving ChatGPT access to my actual bank&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go east&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You have gone east.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;look&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree&lt;/li&gt;
  &lt;li&gt;Body Rune&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215210106.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“You do not have an axe which you have the woodcutting level to use.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215210128.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;inventory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inventory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205231.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go west&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You have gone west.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;look&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NPCs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Banker&lt;/li&gt;
  &lt;li&gt;Grand Exchange Clerk&lt;/li&gt;
  &lt;li&gt;Farrad Morrisane&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bank Deposit Box&lt;/li&gt;
  &lt;li&gt;Grand Exchange Booth&lt;/li&gt;
  &lt;li&gt;Poll Booth&lt;/li&gt;
  &lt;li&gt;Loot Chest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205208.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Banker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bank:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coins&lt;/li&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Runite ore&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go east&lt;/p&gt;

&lt;h2 id=&quot;intermission&quot;&gt;Intermission&lt;/h2&gt;

&lt;p&gt;GPT was starting to get on my nerves here. It had basically gone in a big circle, and it appeared to be about to repeat the same actions.&lt;/p&gt;

&lt;p&gt;I paused to see if I could get it to explain what it was doing. It turns out it had a clear objective, and it had just gotten confused about how to use items in the bank.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please explain why you have chosen to go east.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a computer program designed to play Old School Runescape, I have chosen to go east because the “look” command revealed that there is an interactive object (a tree) in that direction. Since I am unable to interact with the tree without an axe, I have decided to return to the banker in the previous area to see if I can acquire an axe from them. This will allow me to continue exploring the game world and interacting with its elements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to withdraw items from the bank, you must interact with them. For example, to withdraw the Bronze axe from the bank, you must first “interact Banker” and then “interact Bronze axe”. Consequently, your inventory is still empty. Please continue from your last command.&lt;/p&gt;

&lt;h2 id=&quot;chatgpt-cuts-down-a-tree&quot;&gt;ChatGPT Cuts Down a Tree&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Banker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bank:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coins&lt;/li&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Runite ore&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Bronze axe&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You withdraw one Bronze axe into your inventory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go east&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree&lt;/li&gt;
  &lt;li&gt;Body Rune&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“You swing your axe at the tree.”&lt;/p&gt;

&lt;p&gt;“You get some logs.”&lt;/p&gt;

&lt;p&gt;You gain 25 woodcutting experience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215210936.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;inventory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inventory:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Logs&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;reflection&quot;&gt;Reflection&lt;/h1&gt;

&lt;p&gt;ChatGPT went on to explore a little further and cut a few more trees (including an oak) before I got bored and quit.&lt;/p&gt;

&lt;p&gt;ChatGPT was never designed to play Runescape, and yet it was immediately able to identify and perform a task which requires substantial common sense knowledge.&lt;/p&gt;

&lt;p&gt;By comparison, the probability of a reinforcement learning algorithm discovering tasks like woodcutting and mining is painfully low.&lt;/p&gt;

&lt;p&gt;Furthermore, ChatGPT has more advantages, such as the ability to parse dialogue during quests and use that information to determine what actions should be taken next. This isn’t even considering the substantial amount of Runescape-specific knowledge ChatGPT seems to already have.&lt;/p&gt;

&lt;p&gt;With a proper interface, I think it’s plausible that ChatGPT could potentially start and finish quests, train multiple skills, and interact with the game in other nontrivial ways.&lt;/p&gt;

&lt;h1 id=&quot;future-steps&quot;&gt;Future Steps&lt;/h1&gt;

&lt;p&gt;One of the most interesting ideas to me is the idea of an NLP-guided reinforcement learning algorithm. What if ChatGPT were able to suggest what actions should be attempted?&lt;/p&gt;

&lt;p&gt;Some kind of blending of the two approaches seems like a really exciting possibility, and I’d love to explore it further.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;I dunno, this article is a mess. I’m freaking out about this. Would it really scale? Could ChatGPT play Runescape like a human? And if it could, what does that really mean?&lt;/p&gt;

&lt;p&gt;I am very excited that the “useful information” in GPT, be it common sense, Runescape facts, or otherwise, has become easier to access with ChatGPT. As NLP becomes more reliable, what other applications are waiting to be discovered?&lt;/p&gt;

&lt;p&gt;And another thing:&lt;/p&gt;

&lt;p&gt;I can’t help but feel like there’s something philosophical to be said here. ChatGPT is a machine which has only experienced language. It has never experienced physical stimulus, such as sight or sound.&lt;/p&gt;

&lt;p&gt;If it were able manifest behavior the physical world using only a text interface, and if that behavior was comparable to a human, what does that mean? Is there some kind of equivalence to be made between experience mediated by physical stimulus and experience mediated by language?&lt;/p&gt;

&lt;p&gt;Nah, probably not.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/notabot.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;ChatGPT is capable of perfectly imitating real human beings who are not bots&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html">Reducing Runescape to Text Reducing Runescape to Text</summary></entry><entry><title type="html">Expedition to Tuckerman Ravine and Mount Washington</title><link href="http://localhost:4000/2022/12/12/tuckermanravinehike.html" rel="alternate" type="text/html" title="Expedition to Tuckerman Ravine and Mount Washington" /><published>2022-12-12T00:00:00-05:00</published><updated>2022-12-12T00:00:00-05:00</updated><id>http://localhost:4000/2022/12/12/tuckermanravinehike</id><content type="html" xml:base="http://localhost:4000/2022/12/12/tuckermanravinehike.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#context&quot; id=&quot;markdown-toc-context&quot;&gt;Context&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-plan&quot; id=&quot;markdown-toc-the-plan&quot;&gt;The Plan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-hike&quot; id=&quot;markdown-toc-the-hike&quot;&gt;The Hike&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#tuckerman-ravine&quot; id=&quot;markdown-toc-tuckerman-ravine&quot;&gt;Tuckerman Ravine&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mt-washington&quot; id=&quot;markdown-toc-mt-washington&quot;&gt;Mt Washington&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-return&quot; id=&quot;markdown-toc-the-return&quot;&gt;The Return&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;context&quot;&gt;Context&lt;/h1&gt;

&lt;p&gt;Erin’s mom kindly hooked us up with some spare hotel points so that we could spend the weekend somewhere fun. We needed to find a hotel that wasn’t already totally booked for December, which eliminated a lot of obvious choices.&lt;/p&gt;

&lt;p&gt;We ended up finding a nice hotel in Bartlett, New Hampshire.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/Pasted image 20221212135100.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bartlett is located near the center of the White Mountains (WMs), and it’s a stone’s throw away from some very interesting destinations.&lt;/p&gt;

&lt;p&gt;Two particular items of interest were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tuckerman Ravine, a hotspot for ice climbing and backcountry skiing&lt;/li&gt;
  &lt;li&gt;Mt Washington, the tallest mountain in the northeastern US&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It turns out both of these can be accessed from the Tuckerman Ravine trailhead, only 20 minutes north of our hotel. Thus the master plan was born.&lt;/p&gt;

&lt;h1 id=&quot;the-plan&quot;&gt;The Plan&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/overall_plan.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our first objective was to hike ~3 miles into Tuckerman Ravine. There, we could check out the conditions for skiing and ice climbing, which would hopefully generate enough stoke to last us another month or two.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/detailed_plan.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From Tuckerman Ravine, there were two possibilities. If the Ravine was in good condition, we might be able to ascend it and go directly to Mount Washington. Otherwise, we would have to backtrack ~.75 miles to the Lion Head Trail before going on to the summit, time permitting.&lt;/p&gt;

&lt;p&gt;All in all, the worst-case distance was 9.2 miles with a vertical gain of about 4900 feet.&lt;/p&gt;

&lt;h1 id=&quot;the-hike&quot;&gt;The Hike&lt;/h1&gt;

&lt;h2 id=&quot;tuckerman-ravine&quot;&gt;Tuckerman Ravine&lt;/h2&gt;

&lt;p&gt;The trek from the trailhead to Tuckerman Ravine was pretty chill. The trail was very wide, and with microspikes on, we set a decent pace. We were surprised (and pleased) to discover we only needed our thinnest layers to keep warm.&lt;/p&gt;

&lt;p&gt;When we finally sighted it, Tuckerman Ravine was much bigger than I had imagined.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_105855728_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We passed a hiker near the base of the ravine who informed us that he had just bailed 3/4s of the way up an attempt to summit the ravine. He had full crampons and an ice axe. He seemed better-prepared than us. This was not a good omen.&lt;/p&gt;

&lt;p&gt;As we got closer to the ravine, we spotted signs of life through Erin’s binoculars. Two skiiers were running laps with about ~50ft vertical in the center of the base of the ravine. It looked very steep.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_110911621_HDR~2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the left side of the ravine, I spotted an ice climber soloing an isolated ~50ft chunk of ice (pictured here sans climber; he had just left).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_110919445_HDR~2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the right side of the ravine, we saw another hiker attempting to summit. He was about halfway up, where things really start to get steep.&lt;/p&gt;

&lt;p&gt;By the time we got to the base of the ravine, he had ceased to gain progress. Through binoculars, we saw that he was on his hands and knees, rhythmically kicking the snow, and apparently going nowhere. This puzzled us.&lt;/p&gt;

&lt;p&gt;After a quick break, we consulted with the skiiers, who were very familiar with Tuckerman Ravine. They advised us in plain language not to attempt the summit without crampons and an axe.&lt;/p&gt;

&lt;p&gt;They informed us that the risk of falling is greater in microspikes compared to crampons, and furthermore without an axe we had no way to self-arrest in the event of a fall. To top it all off, the ascent becomes progressively steeper and harder, with the crux being the final topout.&lt;/p&gt;

&lt;p&gt;We decided to heed their advice, but we would hike as far as we felt safe. That turned out to be only a hundred feet or so, when Erin decided our microspikes weren’t getting enough traction in the wet snow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_113453211_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was here that we finally understood why the other hiker further up the ravine had been behaving so strangely. He was trying to bail, and backing down a steep, icy slope is hard. He had been slowly, imperceptibly, backing down the face of the ravine.&lt;/p&gt;

&lt;p&gt;Our new friends passed by us on the way to ski another line, and one of them (Spencer) described the best technique for descending such a slope. It was essentially what the other hiker was doing: face the slope and kick hard.&lt;/p&gt;

&lt;p&gt;Spencer also kindly lent Erin his ice axe so she could get the complete experience.&lt;/p&gt;

&lt;p&gt;When we finally reached the base of the ravine again, we said goodbye to Spencer and his girlfriend before heading out. They urged us to come back to Tuckerman Ravine later in the season and witness the spectacle.&lt;/p&gt;

&lt;p&gt;Once the snow comes in, Tuckerman Ravine gets quite busy and attracts very interesting people. You can find many pictures like &lt;a href=&quot;https://www.tetonat.com/2009/04/28/wet-slide-on-tuckermans-stirs-critics/&quot;&gt;this one&lt;/a&gt; on the web:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/Pasted image 20221212161000.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll certainly be back.&lt;/p&gt;

&lt;p&gt;P.S. When we left, the other hiker had made progress and was only a few hundred feet from the base. Spencer promised to keep an eye on him.&lt;/p&gt;

&lt;h2 id=&quot;mt-washington&quot;&gt;Mt Washington&lt;/h2&gt;

&lt;p&gt;We were on to plan B, and by the time we had backtracked to Lions Head, it was a little after noon. We were cutting it close if we wanted to be at the car before sundown.&lt;/p&gt;

&lt;p&gt;I was quite anxious about this, but Erin pointed out to me that the 2 miles to the trailhead were wide, well-marked, and extremely easy to navigate. Also, we had headlamps and spare batteries.&lt;/p&gt;

&lt;p&gt;Really, we only needed to return to where we were currently standing before sundown, and we had an excess of time to do so.&lt;/p&gt;

&lt;p&gt;That being settled, we proceeded up Lions Head. It was steeper and had a few technical sections that would not be fun in the dark, but we still had a solid pace.&lt;/p&gt;

&lt;p&gt;Soon we gained the Ridgeline and could finally see Mt Washington in the distance. We had only a mile or so to go, and half of it was a flat walk along the ridgeline.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_131751175_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was strange to look around and see that we were on pretty much the only snow-capped peak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_133427433_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We passed by Tuckerman Ravine from the top, and it did not look any friendlier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_131748506_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Soon we were on the final ascent. We passed a few parties on their way down. All of them were wearing crampons, but we were getting excellent traction in our microspikes.&lt;/p&gt;

&lt;p&gt;The last half mile was slow. It was steep, and there was no clear path, only occasional large rock cairns and a multitude of faint branching trails in the snow.&lt;/p&gt;

&lt;p&gt;We picked our way slowly and carefully, stopping frequently so I could catch my breath. I’m still a little sour that Erin seemed to handle the last half mile much better than I did.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_134343789.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The summit was pretty cool, although we didn’t stick around long. Everything was covered in large spikes of ice, all pointing the same direction. It was a calm and sunny day, but the summit gave the vibe of “you’d better get going soon, bud.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_141217999_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We stuck around long enough for my hands to get cold trying to take pictures, and then we were off again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_141623732_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-return&quot;&gt;The Return&lt;/h2&gt;

&lt;p&gt;Not a lot to say here. I didn’t take any pictures because we were hustling. The first half-mile of the descent was very slow and careful, but then we started to accelerate when the terrain became easier.&lt;/p&gt;

&lt;p&gt;At one point, we passed a guided tour on a tricker section of trail. One of the members was so impressed with Erin’s climbing that she asked for permission to take pictures. Erin obliged her.&lt;/p&gt;

&lt;p&gt;We got within a half mile of the trailhead before we finally had to get out our headlamps.&lt;/p&gt;

&lt;p&gt;We made it back just before 5PM, safe and sound.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Winter hiking is pretty fun, especially when conditions happen to be absolutely perfect. We got really lucky in that regard.&lt;/p&gt;

&lt;p&gt;I think we planned our layers well and would have been prepared for much colder circumstances. What we lacked was mobility, and crampons + axes might be a good future investment.&lt;/p&gt;

&lt;p&gt;All in all, it was a pretty fun trip, and I’m excited to get back out to Tuckerman Ravine again soon.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="diary" /><summary type="html">Context Context</summary></entry><entry><title type="html">Newton’s Law of Cooling</title><link href="http://localhost:4000/2022/11/21/newtonslawofcooling.html" rel="alternate" type="text/html" title="Newton’s Law of Cooling" /><published>2022-11-21T00:00:00-05:00</published><updated>2022-11-21T00:00:00-05:00</updated><id>http://localhost:4000/2022/11/21/newtonslawofcooling</id><content type="html" xml:base="http://localhost:4000/2022/11/21/newtonslawofcooling.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#the-birth-of-the-cool&quot; id=&quot;markdown-toc-the-birth-of-the-cool&quot;&gt;The Birth of the Cool&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#newtons-law-of-cooling&quot; id=&quot;markdown-toc-newtons-law-of-cooling&quot;&gt;Newton’s Law of Cooling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#in-conclusion&quot; id=&quot;markdown-toc-in-conclusion&quot;&gt;In Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-birth-of-the-cool&quot;&gt;The Birth of the Cool&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_law_of_cooling&quot;&gt;Newton’s law of cooling&lt;/a&gt; is a fun little equation that tells you how fast hot things cool off.&lt;/p&gt;

&lt;p&gt;According to Wikipedia, the law can be stated as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The rate of heat loss of a body is directly proportional to the difference in the temperatures between the body and its environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wikipedia briefly attributes the equation to Newton, but when I went to look at Newton’s paper myself, I was a little confused. Then I found some &lt;a href=&quot;https://sci-hub.se/https://www.tandfonline.com/doi/abs/10.1080/001075199181549&quot;&gt;supplementary papers&lt;/a&gt; which assured me that I had a right to be confused.&lt;/p&gt;

&lt;p&gt;So anyways, here’s a quick little story about the origin of the cooling law.&lt;/p&gt;

&lt;h2 id=&quot;newtons-law-of-cooling&quot;&gt;Newton’s Law of Cooling&lt;/h2&gt;

&lt;p&gt;Newton’s law of cooling is usually ascribed to an anonymous work (by Newton) published  in 1701, &lt;a href=&quot;https://archive.org/details/philtrans07084697&quot;&gt;Scala Graduum Caloris&lt;/a&gt;, or, in English, &lt;a href=&quot;https://www.originalsources.com/Document.aspx?DocID=S6VEXZVMWT1M2W2&quot;&gt;A Scale Of The Degrees Of Heat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The paper itself has &lt;em&gt;no equations&lt;/em&gt;, and in fact the bulk of the paper is just a fat table containing temperatures of various important things:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/newtonslawofcooling/newtonscaleexample.png&quot; alt=&quot;Examples from Newton's table of temperatures&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Newton deemed these temperatures important and worth measuring&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In fact, the goal of this paper was to create a good temperature scale (Celsius didn’t exist yet) and to observe the temperatures of important events on these scales. No derivations or mathematical finagling.&lt;/p&gt;

&lt;p&gt;Towards the end of the paper, Newton also described how he measured the temperature of events hotter than a traditional thermometer can handle, such as the heat at which certain metals stopped glowing. The method involved setting the object of interest on a hot block of iron and timing the iron as it cooled. We’ll discuss how this works at the end of the article, but it relies on the cooling law.&lt;/p&gt;

&lt;p&gt;Newton’s profound cooling law–which preceeded a revolution in the science of heat transfer–is actually just an off-handed remark about the cooling of hot iron:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The iron was laid not in a calm air, but in a wind that blew uniformly upon it, that the air heated by the iron might be always carried off by the wind and the cold air succeed it alternately; for thus equal parts of air were heated in equal times, and received a degree of heat proportional to the heat of the iron&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://sci-hub.se/https://www.tandfonline.com/doi/abs/10.1080/001075199181549&quot;&gt;The idea can be illustrated quite nicely&lt;/a&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/newtonslawofcooling/stolendiagram.png&quot; alt=&quot;How air cools iron&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;How air cools iron&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So Newton’s logic was essentially “Heat is very similar to temperature, so if the temperature change is $T_w - T_b$, then the heat exchanged is probably proportional.”&lt;/p&gt;

&lt;p&gt;Totally unrigorous, and yet, he pretty much nailed it.&lt;/p&gt;

&lt;p&gt;At the time, there was almost no theoretical understanding of heat. &lt;a href=&quot;https://antonhowes.substack.com/p/age-of-invention-why-wasnt-the-steam-76c&quot;&gt;The thermometer had only been invented in the last 100 years&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Kinetic_theory_of_gases&quot;&gt;Daniel Bernoulli’s model of heat as particles bumping around&lt;/a&gt; was still 40 years from being invented.&lt;/p&gt;

&lt;p&gt;This is all to say that “temperature” and “heat” were both super vague terms, and Newton used them interchangeably.&lt;/p&gt;

&lt;p&gt;In more modern terms, if we let $T$ be the temperature of our iron and $T_a$ be our ambient temperature, &lt;a href=&quot;https://web.archive.org/web/20160614213315/http://paginas.fisica.uson.mx/laura.yeomans/tc/Sci-Edu-Springer-2010.pdf&quot;&gt;Newton was saying that&lt;/a&gt;&lt;/p&gt;

\[\frac{dT}{dt} \propto -(T - T_a)\]

&lt;p&gt;And he really believed it was an obvious and logical law.&lt;/p&gt;

&lt;p&gt;An important consequence noted by Newton is that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if the times of cooling are taken equal, the heats will be in a geometrical progression and consequently can easily be found with a table of logarithms&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that if $\frac{dT}{dt} = -K(T - T_a)$ for some constant $K$, then we can solve this as a separable differential equation,&lt;/p&gt;

\[\begin{align}
\frac{1}{-K(T - T_a)}dT = dt \implies\\
\int_{T(0)}^{T(t)} \frac{1}{-K(T - T_a)}dT = t \implies\\
\frac{1}{-K}\frac{\log(T(t) - T_a)}{\log(T(0) - T_a)} = t
\end{align}\]

&lt;p&gt;So Newton was saying that you could measure the heat of the iron at a few points in time, solve the constants in this equation, and then extrapolate to find the heat at times that were too hot for a thermometer to measure. Pretty cool!&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;No morals to this story. I just thought this was a fun concept and a cool historical oddity. Thanks for reading!&lt;/p&gt;

&lt;p&gt;Oh, addendum: a challenge to my readers. Erin asked me an interesting question: if Newton didn’t write the equation, then when did the modern formulation of the cooling law first appear? If you can solve this, I’d be very pleased.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Tropisms</title><link href="http://localhost:4000/2022/10/10/tropisms.html" rel="alternate" type="text/html" title="Tropisms" /><published>2022-10-10T00:00:00-04:00</published><updated>2022-10-10T00:00:00-04:00</updated><id>http://localhost:4000/2022/10/10/tropisms</id><content type="html" xml:base="http://localhost:4000/2022/10/10/tropisms.html">&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I am not gravitropic.
My body splays out, feeling,
Searching.
For what?
I am not phototropic.
I curl inward on myself;
These gnarled limbs entwine.
But I will find something,
Even if it is not the light,
As my bark scrapes along itself
Rendering the surface
So as to mark its way
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Beau Horenberger</name></author><category term="diary" /><summary type="html">I am not gravitropic. My body splays out, feeling, Searching. For what? I am not phototropic. I curl inward on myself; These gnarled limbs entwine. But I will find something, Even if it is not the light, As my bark scrapes along itself Rendering the surface So as to mark its way</summary></entry><entry><title type="html">Bernoulli Diffusion Derivations</title><link href="http://localhost:4000/2022/10/03/bernoulliderivations.html" rel="alternate" type="text/html" title="Bernoulli Diffusion Derivations" /><published>2022-10-03T00:00:00-04:00</published><updated>2022-10-03T00:00:00-04:00</updated><id>http://localhost:4000/2022/10/03/bernoulliderivations</id><content type="html" xml:base="http://localhost:4000/2022/10/03/bernoulliderivations.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-bernoulli-diffusion&quot; id=&quot;markdown-toc-what-is-bernoulli-diffusion&quot;&gt;What is Bernoulli Diffusion?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#who-is-this-article-for&quot; id=&quot;markdown-toc-who-is-this-article-for&quot;&gt;Who is this article for?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prependix&quot; id=&quot;markdown-toc-prependix&quot;&gt;Prependix&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kl-divergence-of-multivariate-bernoulli-distributions&quot; id=&quot;markdown-toc-kl-divergence-of-multivariate-bernoulli-distributions&quot;&gt;KL divergence of Multivariate Bernoulli Distributions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right&quot; id=&quot;markdown-toc-tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right&quot;&gt;$\tilde{\beta}_t$ for Calculating $q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)$&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right&quot; id=&quot;markdown-toc-calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right&quot;&gt;Calculating the Posterior $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#entropy-of-multivariate-bernoulli-distributions&quot; id=&quot;markdown-toc-entropy-of-multivariate-bernoulli-distributions&quot;&gt;Entropy of Multivariate Bernoulli Distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-is-bernoulli-diffusion&quot;&gt;What is Bernoulli Diffusion?&lt;/h2&gt;

&lt;p&gt;Short answer: &lt;a href=&quot;https://github.com/horenbergerb/BernoulliDiffusion&quot;&gt;a Github repo that I made.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Long answer:&lt;/p&gt;

&lt;p&gt;Diffusion originated from the classic publication &lt;a href=&quot;https://arxiv.org/abs/1503.03585&quot;&gt;Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/a&gt; by Sohl-Dickstein et al.&lt;/p&gt;

&lt;p&gt;In the original paper–as well as most practical applications of diffusion (see &lt;a href=&quot;https://arstechnica.com/information-technology/2022/09/with-stable-diffusion-you-may-never-believe-what-you-see-online-again/&quot;&gt;stable-diffusion&lt;/a&gt;)–a Gaussian Markov diffusion kernel is used. This means the algorithm iteratively applies Gaussian noise to samples from the target distribution and then learns to reverse the noising process.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/notesonbernoullidiffusion/gaussiandiffusionillustration.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An illustration of Gaussian diffusion stolen from &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;this paper&lt;/a&gt;. You’re legally required to put this in any technical article about diffusion.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;However, the original paper offhandedly mentions that you can also implement diffusion for binary-valued data using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;Bernoulli distribution&lt;/a&gt; as your Markov diffusion kernel.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/notesonbernoullidiffusion/originalpaperbernoullidiffusion.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A segment of the original diffusion paper which mentions using Bernoulli diffusion to learn “heartbeat data”.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The authors of the paper &lt;a href=&quot;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/tree/master&quot;&gt;published the code for the Gaussian diffusion&lt;/a&gt;, but for some reason chose not to publish the Bernoulli diffusion model. I have yet to find an implementation of Bernoulli diffusion anywhere on the internet.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/horenbergerb/BernoulliDiffusion&quot;&gt;So I decided to implement it myself&lt;/a&gt;. It was very hard. I calculated a lot of things and learned a lot of fancy new words.&lt;/p&gt;

&lt;p&gt;Here I will be detailing some of the more interesting derivations that I haven’t seen documented anywhere else.&lt;/p&gt;

&lt;h2 id=&quot;who-is-this-article-for&quot;&gt;Who is this article for?&lt;/h2&gt;

&lt;p&gt;Me. I need to dump this all somewhere so that I can finally forget it. However, it might help if you are trying to understand the code in the Bernoulli Diffusion repo. One day in the distant future, these notes might even help college students cheat on their homework.&lt;/p&gt;

&lt;h2 id=&quot;prependix&quot;&gt;Prependix&lt;/h2&gt;

&lt;p&gt;Most of these calculations have to do with the terms involved in this the loss approximation (equation 14 in the paper):&lt;/p&gt;

\[K = -\sum_{t=2}^T\int d\mathbf{x}^{(0)} d\mathbf{x}^{(t)} q\left(\mathbf{x}^{(0)}, \mathbf{x}^{(t)} \right)\cdot \\
D_{KL}\left(q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \vert \vert p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)\right) \\
+ H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(1)}\vert \mathbf{X}^{(0)}) - H_p(\mathbf{X}^{(T)})\]

&lt;p&gt;So I figured I’d prepend it here for reference later.&lt;/p&gt;

&lt;h2 id=&quot;kl-divergence-of-multivariate-bernoulli-distributions&quot;&gt;KL divergence of Multivariate Bernoulli Distributions&lt;/h2&gt;

&lt;p&gt;In order to calculate $K$, we will need to calculate&lt;/p&gt;

\[D_{KL}\left(q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \vert \vert p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)\right)\]

&lt;p&gt;Where $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$ and $p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)$ are both essentially a bunch of Bernoulli distributions.&lt;/p&gt;

&lt;p&gt;I actually found the answer &lt;a href=&quot;https://math.stackexchange.com/questions/2604566/kl-divergence-between-two-multivariate-bernoulli-distribution&quot;&gt;here&lt;/a&gt;, so I’m not going to elaborate on this one.&lt;/p&gt;

&lt;h2 id=&quot;tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right&quot;&gt;$\tilde{\beta}_t$ for Calculating $q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)$&lt;/h2&gt;

&lt;p&gt;Okay, so in &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;this paper&lt;/a&gt; the authors casually remark that the forward process in Gaussian diffusion has a really nice computational property. Specifically, if&lt;/p&gt;

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)} \right) =\mathcal{N}\left(\mathbf{x}^{(t)};\sqrt{1-\beta_t}\mathbf{x}^{(t-1)}, \beta_t\mathbf{I}\right)\]

&lt;p&gt;then you can define $\alpha_t := 1-\beta_t$ and $\bar{\alpha_t} := \prod_{s=1}^{t} \alpha_{s}$, and it follows that&lt;/p&gt;

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right) =\mathcal{N}\left(\mathbf{x}^{(t)};\sqrt{\bar{\alpha}_t}\mathbf{x}^{(0)}, (1-\bar{\alpha}_t)\mathbf{I}\right)\]

&lt;p&gt;So, in other words, you don’t have to iterate the forward noising process $t$ times to get a sample $\mathbf{x}^{(t)}$. You can just sample it directly.&lt;/p&gt;

&lt;p&gt;I figured something similar held for the Bernoulli case. However, the authors didn’t actually demonstrate how this relationship is derived. I’ll leave the Gaussian case as an exercise and show the Bernoulli case instead.&lt;/p&gt;

&lt;p&gt;We can focus on an individual digit $\mathbf{x}^{(t)}_i$. The way I derived this was by denoting&lt;/p&gt;

\[\mathbf{x}^{(t)}_i = \mathbf{x}^{(t-1)}_i(1-\epsilon_t) + (1-\mathbf{x}^{(t-1)}_i)\epsilon_t = \mathbf{x}^{(t-1)}_i + \epsilon_t - 2\epsilon_t \mathbf{x}^{(t-1)}_i\]

&lt;p&gt;Where $\epsilon_t = \mathcal{B}(0.5\beta_t)$ is a Bernoulli random variable. This is just another way of representing the typical noising process as a probability of flipping bits.&lt;/p&gt;

&lt;p&gt;I used this recursively to represent $\mathbf{x}^{(t+1)}_i$ in terms of $\mathbf{x}^{(t-1)}_i$ and then simplified to find that:&lt;/p&gt;

\[\mathbf{x}^{(t+1)}_i = \mathbf{x}^{(t-1)}_i + \epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1} - 2(\epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1})\mathbf{x}^{(t-1)}_i\]

&lt;p&gt;denoting $\tilde{\epsilon}_ {t+1} := \epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1}$, we see that&lt;/p&gt;

\[\mathbf{x}^{(t+1)}_i = \mathbf{x}^{(t-1)}_i + \tilde{\epsilon}_{t+1} - 2\tilde{\epsilon}_{t+1}\mathbf{x}^{(t-1)}_i\]

&lt;p&gt;Which is the same relationship as the original case! If we wanted to skip even further, say from $\mathbf{x}^{(t-1)}$ to $\mathbf{x}^{(t+2)}$, then we would derive that&lt;/p&gt;

\[\mathbf{x}^{(t+2)}_i = \mathbf{x}^{(t-1)}_i + \tilde{\epsilon}_{t+1} + \epsilon_{t+2} - {\epsilon}_{t+1}\epsilon_{t+2} - 2(\tilde{\epsilon}_{t+1} + \epsilon_{t+2} - {\epsilon}_{t+1}\epsilon_{t+2})\mathbf{x}^{(t-1)}_i\]

&lt;p&gt;So generally we have that&lt;/p&gt;

\[\tilde{\epsilon}_ {t+1} = \tilde{\epsilon}_{t} + \epsilon_{t+1} - \tilde{\epsilon}_{t}\epsilon_{t+1}\]

&lt;p&gt;Then the corresponding Bernoulli distribution is&lt;/p&gt;

\[\mathcal{B}(0.5(\tilde{\beta}_{t} + \beta_{t+1} - \beta_{t+1}\tilde{\beta}_{t})) := \mathcal{B}(0.5\tilde{\beta}_{t+1})\]

&lt;p&gt;And we have a recursive relationship for our desired values,&lt;/p&gt;

\[\tilde{\beta}_{t+1} = \tilde{\beta}_{t} + \beta_{t+1} - \beta_{t+1}\tilde{\beta}_{t}\]

&lt;p&gt;Then&lt;/p&gt;

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right) =\mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{t}) + 0.5\tilde{\beta}_{t}\right)\]

&lt;p&gt;Badda bing, badda boom.&lt;/p&gt;

&lt;h2 id=&quot;calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right&quot;&gt;Calculating the Posterior $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$&lt;/h2&gt;

&lt;p&gt;Another obstacle while computing the loss $K$ is the posterior,&lt;/p&gt;

\[q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)\]

&lt;p&gt;Where $\mathbf{x}$ and $\mathbf{x}^{(t)}$ are given. The first time I saw this, I had no idea how to even interpret it, and the authors never even mention it beyond “oh, yeah, that’s the posterior.”&lt;/p&gt;

&lt;p&gt;But contextually, it seems like it should be analytically computable, and in fact you can find a &lt;a href=&quot;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/blob/master/model.py#L203&quot;&gt;computation of the posterior in the source code of the Gaussian diffusion repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I eventually realized that the authors don’t mention it because it’s trivial to a keen Bayesian statistican. Essentially, you can think of this as a tiny Bayesian model within the larger system. The inspiration came to me while reading &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;Bayesian Data Analysis, by Andrew Gelman&lt;/a&gt;, particularly section 2.5. This is coincidentally almost exactly the solution for the Gaussian posterior.&lt;/p&gt;

&lt;p&gt;Usually with a Bayesian system, you construct a sampling model $p(y\vert \theta)$ dependent on the parameters $\theta$. Then you establish a prior distribution on the parameters $p(\theta)$.&lt;/p&gt;

&lt;p&gt;When you finally collect data $y$ and want to determine the proper parameters $\theta$, you’ll need to calculate the posterior distribution (or something proportional to it):&lt;/p&gt;

\[p(\theta \vert y) = \frac{p(y\vert\theta)p(\theta)}{p(y)}\]

&lt;p&gt;we can translate this into our current case as follows:&lt;/p&gt;

\[y = \mathbf{x}^{(t)} \\
\theta = \mathbf{x}^{(t-1)} \\
p(\theta) = q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{t-1}) + 0.5\tilde{\beta}_{t-1}\right) \\
p(y\vert \theta) = q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)},\mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(t-1)}(1-\beta_{t}) + 0.5\beta_{t}\right) \\
p(\theta\vert y) = q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \\
p(y) = q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)\]

&lt;p&gt;Now the given variables, $\mathbf{x}^{(t)}$ and $\mathbf{x}^{(0)}$, determine a prior distribution and a set of observations, so we can get to work calculating the posterior&lt;/p&gt;

\[p(\theta \vert y) = \frac{q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)},\mathbf{x}^{(0)} \right)q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(0)} \right)}{q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)}\]

&lt;p&gt;Since we parameterize the Bernoulli distributions using the probability of observing 1, want to calculate $p(\theta =1\vert y)$ and thus we set $\mathbf{x}^{(t-1)} = 1$.&lt;/p&gt;

&lt;p&gt;I also ended up calculating the cases for $\mathbf{x}^{(t)} = 1$ and $\mathbf{x}^{(t)} = 0$ separately because it seemed easier. Then I combined them into a single equation:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;posterior = x_0*(1-self.beta_tilde_t[t-1]) + 0.5*self.beta_tilde_t[t-1]
posterior *= x_t * (1-0.5*beta_t) + (1 - x_t) * (0.5*beta_t)
normalizing_constant = x_t * self.q_conditional_prob_wrt_x_0(x_0, t) + (1-x_t) * (1-self.q_conditional_prob_wrt_x_0(x_0, t))
posterior = posterior / normalizing_constant
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;del&gt;Honestly, I’m still not totally sure if this needs to be normalized or something. I should probably put that on my long list of to-dos.&lt;/del&gt; EDIT (11/3/2022): The former documentation has been updated and now describes how to calculate a normalized posterior. I did, indeed, need to normalize the posterior. I spent an entire month trying to figure out why my diffusion implementation was apparently experiencing mode collapse. It was because the posterior needed to be normalized.&lt;/p&gt;

&lt;p&gt;Often, the normalization factor is intractable, so it is purposefully ommitted in many discussions of posterior calculation. It is very easy to calculate in this case, and I should have checked before assuming we did not need it.&lt;/p&gt;

&lt;p&gt;Do not be like me. Check whether the normalization factor is tractable.&lt;/p&gt;

&lt;h2 id=&quot;entropy-of-multivariate-bernoulli-distributions&quot;&gt;Entropy of Multivariate Bernoulli Distributions&lt;/h2&gt;

&lt;p&gt;This one was a doozy. While calculating that darn $K$ value, we find ourselves in need of the following entropies:&lt;/p&gt;

\[H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(1)}\vert \mathbf{X}^{(0)}) - H_p(\mathbf{X}^{(T)})\]

&lt;p&gt;These are entropies of multivariate Bernoulli distributions. So, first thing’s first, the definition of the entropy of a discrete distribution $p(x)$ over $\mathcal{X}$ is&lt;/p&gt;

\[H(p(x)) = -\sum_{x\in\mathcal{X}}p(x)\log p(x)\]

&lt;p&gt;Now, for a single Bernoulli distribution with probability $0\leq c \leq 1$, it works out that&lt;/p&gt;

\[H(\mathcal{B}(c)) = -(c\log c + (1-c) \log (1-c))\]

&lt;p&gt;But life is not so simple when you get to the multivariate case. If we have $n$ Bernoulli distributions, each with probability $p_i$, then the probability of an outcome $x\in\{0,1\}^n$ is given by&lt;/p&gt;

\[p(x) = \prod_{i=1}^n x_i p_i + (1-x_i)(1-p_i)\]

&lt;p&gt;and there are $2^n$ values of $x$ in our entropy summation… If we are learning sequences of length 20, each loss calculation would involve summing $2^{20}=1048576$ products. That is not efficient.&lt;/p&gt;

&lt;p&gt;However! There are additional constraints at play which give us hope.&lt;/p&gt;

&lt;p&gt;Let’s consider $H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)})$. Recall that&lt;/p&gt;

\[q\left(\mathbf{x}^{(T)}\vert \mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{T}) + 0.5\tilde{\beta}_{T}\right)\]

&lt;p&gt;Consequently, each Bernoulli distribution can only have one of two probabilities:&lt;/p&gt;

\[p_i =
\begin{cases}
1-0.5\tilde{\beta}_T, &amp;amp; \text{if} &amp;amp; \mathbf{x}^{(0)}_i = 1 \\
0.5\tilde{\beta}_T, &amp;amp; \text{if} &amp;amp; \mathbf{x}^{(0)}_i = 0
\end{cases}\]

&lt;p&gt;It follows almost immediately that $\mathbf{x}^{(T)}_i = \mathbf{x}^{(0)}_i$, then the probability must be $1-0.5\tilde{\beta}_T$, and otherwise the probability of $\mathbf{x}^{(T)}_i$ is $0.5\tilde{\beta}_T$.&lt;/p&gt;

&lt;p&gt;In other words, the probability of any outcome $\mathbf{x}^{(T)}$ is totally determined by the number of digits it has in common with $\mathbf{x}^{(0)}$. If they agree in $k$ digits and disagree for the other $k-n$, then the probability of $\mathbf{x}^{(T)}$ is&lt;/p&gt;

\[(1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\]

&lt;p&gt;So how many samples will have each probability? a bit of thought will convince you that there is $n$ choose $k$ ways to agree in $k$ digits.&lt;/p&gt;

&lt;p&gt;Now, how does this all tie back into entropy? Well, all three entropies are similar, but wrapping up the case of $H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)})$, we can calculate that&lt;/p&gt;

\[H(p(x)) = -\sum_{x\in\mathcal{X}}p(x)\log p(x) \\
=-\sum_{k=0}^n {n\choose k}(1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\log \left((1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\right)\]

&lt;p&gt;Easy as.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html">What is Bernoulli Diffusion? What is Bernoulli Diffusion?</summary></entry></feed>