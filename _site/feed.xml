<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-01-18T17:18:19-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">The Horenberger Zone</title><subtitle>Cite me in your thesis</subtitle><author><name>Beau Horenberger</name></author><entry><title type="html">Bayesian Multinomial Models: A Tragedy (In Runescape)</title><link href="http://localhost:4000/2023/01/18/runescapepvpattackprediction.html" rel="alternate" type="text/html" title="Bayesian Multinomial Models: A Tragedy (In Runescape)" /><published>2023-01-18T00:00:00-05:00</published><updated>2023-01-18T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/18/runescapepvpattackprediction</id><content type="html" xml:base="http://localhost:4000/2023/01/18/runescapepvpattackprediction.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#abandon-all-hope-ye-who-enter&quot; id=&quot;markdown-toc-abandon-all-hope-ye-who-enter&quot;&gt;Abandon all hope, ye who enter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#combat-in-runescape&quot; id=&quot;markdown-toc-combat-in-runescape&quot;&gt;Combat in Runescape&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-data&quot; id=&quot;markdown-toc-the-data&quot;&gt;The Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#model-criteria&quot; id=&quot;markdown-toc-model-criteria&quot;&gt;Model Criteria&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#constructing-a-bayesian-model&quot; id=&quot;markdown-toc-constructing-a-bayesian-model&quot;&gt;Constructing A Bayesian Model&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#why-bayesian&quot; id=&quot;markdown-toc-why-bayesian&quot;&gt;Why Bayesian?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#overview-of-a-bayesian-model&quot; id=&quot;markdown-toc-overview-of-a-bayesian-model&quot;&gt;Overview of a Bayesian Model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#assumptions&quot; id=&quot;markdown-toc-assumptions&quot;&gt;Assumptions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-likelihood-or-sampling-distribution&quot; id=&quot;markdown-toc-the-likelihood-or-sampling-distribution&quot;&gt;The Likelihood or Sampling Distribution&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#a-simple-example-binomial-distributions&quot; id=&quot;markdown-toc-a-simple-example-binomial-distributions&quot;&gt;A simple example: binomial distributions&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-multinomial-distribution&quot; id=&quot;markdown-toc-the-multinomial-distribution&quot;&gt;The multinomial distribution&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-prior-distribution&quot; id=&quot;markdown-toc-the-prior-distribution&quot;&gt;The Prior Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calculating-the-marginal-likelihood&quot; id=&quot;markdown-toc-calculating-the-marginal-likelihood&quot;&gt;Calculating the Marginal Likelihood&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#setup&quot; id=&quot;markdown-toc-setup&quot;&gt;Setup&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#preparing-to-integrate&quot; id=&quot;markdown-toc-preparing-to-integrate&quot;&gt;Preparing to integrate&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#tackling-the-integral&quot; id=&quot;markdown-toc-tackling-the-integral&quot;&gt;Tackling the integral&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#the-interior-integral&quot; id=&quot;markdown-toc-the-interior-integral&quot;&gt;The interior integral&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#the-second-integral&quot; id=&quot;markdown-toc-the-second-integral&quot;&gt;The second integral&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#the-marginal-likelihood&quot; id=&quot;markdown-toc-the-marginal-likelihood&quot;&gt;The marginal likelihood&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#validating-the-marginal-likelihood&quot; id=&quot;markdown-toc-validating-the-marginal-likelihood&quot;&gt;Validating the marginal likelihood&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calculating-the-posterior&quot; id=&quot;markdown-toc-calculating-the-posterior&quot;&gt;Calculating the posterior&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#using-the-bayesian-model&quot; id=&quot;markdown-toc-using-the-bayesian-model&quot;&gt;Using The Bayesian Model&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-data-1&quot; id=&quot;markdown-toc-the-data-1&quot;&gt;The data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#initializing-the-model&quot; id=&quot;markdown-toc-initializing-the-model&quot;&gt;Initializing the model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#posterior-updates&quot; id=&quot;markdown-toc-posterior-updates&quot;&gt;Posterior updates&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion-this-model-sucks&quot; id=&quot;markdown-toc-conclusion-this-model-sucks&quot;&gt;Conclusion: This Model Sucks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;abandon-all-hope-ye-who-enter&quot;&gt;Abandon all hope, ye who enter&lt;/h1&gt;
&lt;p&gt;Originally I set out in this project to design an interesting Bayesian model which could predict opponent behavior in player-vs-player Old School Runescape combat.&lt;/p&gt;

&lt;p&gt;I will spoil the outcome for you now. After a long and arduous journey, I discovered that my derived model absolutely blows.&lt;/p&gt;

&lt;p&gt;It’s fascinating, really. Theoretically deriving this model was hard. It took me a very long time. I had to solve some tough integrals. And yet, the model that pops out is so disappointingly simple.&lt;/p&gt;

&lt;p&gt;In retrospect, I could have easily guessed what the model will be without any derivations. I could have implemented the model in two lines of code.&lt;/p&gt;

&lt;p&gt;So if the model sucks, why release this article at all? Why not bury it and move on with my life? I have a few rationalizations about this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This article may provoke some insights about model design. How do you design a problem so that Bayesian modeling is a good solution? Certainly not like this.&lt;/li&gt;
  &lt;li&gt;If anyone ever wonders about the process of deriving a multinomial Bayesian model, this article contains the derivations for the case of $k=3$ possible outcomes.&lt;/li&gt;
  &lt;li&gt;This article is proof to myself that I can do Bayesian things.&lt;/li&gt;
  &lt;li&gt;Because it’s pretty funny, honestly.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So consider yourself warned. You may find interesting Bayesian statistics here, but you will not find a good Runescape PvP model.&lt;/p&gt;

&lt;h1 id=&quot;combat-in-runescape&quot;&gt;Combat in Runescape&lt;/h1&gt;
&lt;p&gt;Player Vs Player (PVP) combat in Old School Runescape (OSRS) can be modeled as an iterative guessing game. At each step of the game, each player must choose two things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Which attack style to use&lt;/li&gt;
  &lt;li&gt;Which attack style to defend or “pray” against&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There are three attack styles in the game: Melee, Range, and Mage, each depicted with their own respective icon:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205512.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205503.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205451.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During each turn, the players attack and deal a random amount of damage. Players take less damage if they are “praying against” their opponent’s attack style.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20220707205643.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The “overhead prayers” show both players are praying against Range. The player on the left is attacking with Range (a crossbow), while the player on the right is using Mage. Consequently, the player on the right will take reduced damage, while the player on the left will take the normal amount of damage.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Thus, it is very important to attempt to predict your opponent’s behavior (informally called “reading”) in order pray against the correct attacks and attack with an off-prayer style. As seen in this video, professional players like Odablock are extremely skilled at reading their opponents:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Mh2XnRm1n5w?start=195&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Actual &lt;a href=&quot;https://oldschool.runescape.wiki/w/Combat&quot;&gt;Runescape combat&lt;/a&gt; is more elaborate than the iterative guessing game concept we described above. Rather than being truly iterative, actions occur within the &lt;a href=&quot;https://oldschool.runescape.wiki/w/RuneScape_clock&quot;&gt;tick system of the Runescape clock&lt;/a&gt; where different weapons have different speeds. There are also more actions than simply attacking, such as eating or drinking potions.&lt;/p&gt;

&lt;p&gt;However, our iterative guessing game model is a decent approximation which enables a model-based approach to predicting enemy behavior.&lt;/p&gt;

&lt;p&gt;Let’s look at the data available to us and then sketch out the criteria for our model.&lt;/p&gt;

&lt;h1 id=&quot;the-data&quot;&gt;The Data&lt;/h1&gt;

&lt;p&gt;The PvPTracker plugin for the Runelite Client allow us to collect data about combat as we play:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20221119131103.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Please do not ask about my win/loss ratio&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Each row of the dataset shows the attack style of the attacker and the corresponding defensive prayer of the defender. We have similar tables for many fights.&lt;/p&gt;

&lt;p&gt;So it’s easy to extract data that conforms to our idealized representation of combat.&lt;/p&gt;

&lt;h1 id=&quot;model-criteria&quot;&gt;Model Criteria&lt;/h1&gt;

&lt;p&gt;What do we want our model to do? Well, we want to emphasize simplicity and utility. To keep things simple, let’s focus on predicting only the probability/frequency of attack styles used by opponents.&lt;/p&gt;

&lt;p&gt;Then here are some properties of the model that would be nice&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model predicts the probability of each attack style being used by the opponent&lt;/li&gt;
  &lt;li&gt;The model is conditioned on all previous combat data&lt;/li&gt;
  &lt;li&gt;The model updates dynamically for each particular opponent based on their actions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is criteria is relatively simple yet gives us valuable data about our opponent in real time. It also makes use of all of the data available to us.&lt;/p&gt;

&lt;p&gt;Such a model also has the potential to be extended. We might instead try to predict conditional attack styles (probability of X attack style if the last attack was Y), or we could design a hierarchical model which uses previous combat data to generate multiple “archetypical profiles” of opponents.&lt;/p&gt;

&lt;h1 id=&quot;constructing-a-bayesian-model&quot;&gt;Constructing A Bayesian Model&lt;/h1&gt;
&lt;p&gt;The model we will construct is an extension of the binomial model described in chapter 1 of Bayesian Data Analysis by Andrew Gelman.&lt;/p&gt;

&lt;h2 id=&quot;why-bayesian&quot;&gt;Why Bayesian?&lt;/h2&gt;

&lt;p&gt;Well, it’s good exercise. But there’s also another reason that I think Bayesian models are well-suited for this problem.&lt;/p&gt;

&lt;p&gt;Bayesian models can initialize the prior distribution using data from previous fights, and then we can update the model parameters in real-time using data from the current fight.&lt;/p&gt;

&lt;p&gt;In general, I think Bayesian models are an obvious choice when your data comes in the format of “repeated experiments” like this.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-a-bayesian-model&quot;&gt;Overview of a Bayesian Model&lt;/h2&gt;

&lt;p&gt;A Bayesian model has a lot of different pieces. If you’re new to this, we’ll quickly describe how a Bayesian model works. Bayesian models revolve around Baye’s theorem:&lt;/p&gt;

\[p(\theta\vert y) = \frac{p(y\vert \theta)p(\theta)}{p(y)}\]

&lt;p&gt;We will define the components of this equation and then explain the equation itself.&lt;/p&gt;

&lt;p&gt;$y$ represents the observations or evidence. We collect samples of $y$, and we want to calculate the probabilities of observing different values of $y$. We should design $y$ to represent not just the data we want (i.e. the next attack style of an opponent) but also the data we intend to condition the model on (i.e. the a quantity of each attack style by an opponent over $n$ attacks).&lt;/p&gt;

&lt;p&gt;$p(y\vert \theta)$ is the likelihood or sampling distribution. This is “the model,” in the sense that once we pick some values for $\theta$, this function tells us the probability of any output $y$.&lt;/p&gt;

&lt;p&gt;$\theta$ are the model parameters. Our objective is to determine some good values for these using a set of observations of $y$.&lt;/p&gt;

&lt;p&gt;$p(\theta)$ is called the prior distribution over model parameters. It represents our beliefs about the model prior to our “training” with collected data. There are many common choices for the prior distribution that we will discuss later.&lt;/p&gt;

&lt;p&gt;$p(y)$ is called the marginal likelihood. It’s kind of a wildcard, typically unknown and usually very difficult to approximate.&lt;/p&gt;

&lt;p&gt;Finally, $p(\theta\vert y)$ is the posterior. We can calculate this using Baye’s theorem above. It provides us with a means to update our choice of $\theta$ given samples of $y$.&lt;/p&gt;

&lt;p&gt;So, here’s our to-do list if we want to build a Bayesian model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Craft a sampling distribution $p(y\vert \theta)$ which we think has the capacity to model the situation of interest&lt;/li&gt;
  &lt;li&gt;Select a prior distribution $p(\theta)$ which encodes our preexisting beliefs about the model parameters (or, failing that, just pick a mathematically-convenient prior)&lt;/li&gt;
  &lt;li&gt;Figure out how to calculate the posterior $p(\theta\vert y)$ using Baye’s theorem
    &lt;ul&gt;
      &lt;li&gt;This will involve approximating or calculating $p(y)$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once this is all done, the model is ready for action. We can initialize the prior using data from previous fights and then do real-time Bayesian inference during combat.&lt;/p&gt;

&lt;p&gt;But I will warn you now; it’s going to be a long journey. Only the most dedicated to Runescape PVP will survive. If you get lost in the sections ahead, you can refer to this section as a roadmap.&lt;/p&gt;

&lt;h2 id=&quot;assumptions&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;Let’s clarify some of the simplifications we are assuming about OSRS combat.&lt;/p&gt;

&lt;p&gt;We assume that among $n$ attacks from the opponent, the attack styles are independent and identically distributed (IID). Then the quantities of each attack style constitute a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;multinomial random variable&lt;/a&gt; $y\sim Y$ whose outcomes are $y = [\text{\# Melee},\text{\# Range},\text{\# Mage}]$. The probability of each attack style for a given opponent is unknown.&lt;/p&gt;

&lt;p&gt;In reality, this assumption does not hold. For example, each attack by an opponent is probably heavily correlated to their previous attack. However, this model can still provide behavioral insights, and later we will discuss how it can be extended to support these conditional dependencies.&lt;/p&gt;

&lt;h2 id=&quot;the-likelihood-or-sampling-distribution&quot;&gt;The Likelihood or Sampling Distribution&lt;/h2&gt;

&lt;h3 id=&quot;a-simple-example-binomial-distributions&quot;&gt;A simple example: binomial distributions&lt;/h3&gt;

&lt;p&gt;First we would like to describe the sampling distribution of the opponent’s attacks.&lt;/p&gt;

&lt;p&gt;Let’s consider a simpler situation. When a random variable has only two outcomes, we can treat it as a binomial random variable with one parameter, $\theta$, and the sampling distribution is&lt;/p&gt;

\[p(y|\theta)={n\choose k}\theta^y (1-\theta)^{n-y} = \frac{n!}{k!(n-k)!}\theta^y (1-\theta)^{n-y}\]

&lt;p&gt;This situation is analogous to $n$ (potentially unfair) coinflips where $y$ represents the number of flips resulting in heads. Then $\theta$ represents the probability of getting heads, and $1-\theta$ is the probability of tails. If $\theta=0.5$, it’s a fair coin.&lt;/p&gt;

&lt;h3 id=&quot;the-multinomial-distribution&quot;&gt;The multinomial distribution&lt;/h3&gt;

&lt;p&gt;However, in our case we have three outcomes, and thus two parameters, which we denote as&lt;/p&gt;

\[\begin{aligned}
\theta_{Melee}\text{ or } \theta_{M} \\
\theta_{Range}\text{ or } \theta_{R}
\end{aligned}\]

&lt;p&gt;where $\theta_{M}+\theta_{R} = 1-\theta_{Magic}$.&lt;/p&gt;

&lt;p&gt;Recall that $y$ is a tuple of three integers denoting the quantity of each attack style. For example, if $n=7$, then the outcome $y=[4,2,1]$ means the opponent attacked with Melee 4 times, Range twice, and Mage once. Let $y_i$ denote the $i$th member of $y$.&lt;/p&gt;

&lt;p&gt;We are going to write our &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_distribution&quot;&gt;multinomial sampling distribution&lt;/a&gt; using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt; instead of factorials. This will make it appear more similar to the other functions (specifically the conjugate prior) later on.&lt;/p&gt;

&lt;p&gt;Here is our sampling distribution in all its glory:&lt;/p&gt;

\[p(y|\theta_{M}, \theta_{R})=\frac{\Gamma(y_1+y_2+y_3+1)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)}\theta_{M}^{y_1}\theta_{R}^{y_2} (1-\theta_M - \theta_R)^{y_3}\]

&lt;h2 id=&quot;the-prior-distribution&quot;&gt;The Prior Distribution&lt;/h2&gt;

&lt;p&gt;There are two obvious choices for the prior distribution  $p(\theta_M, \theta_R)$.&lt;/p&gt;

&lt;p&gt;The first prior we might consider is the uniform distribution over the &lt;a href=&quot;https://en.wikipedia.org/wiki/Simplex&quot;&gt;standard 2-simplex&lt;/a&gt;. This distribution assumes no prior knowledge about the model parameters. In our case, this isn’t really a good fit; we expect to have lots of data from previous experiments to inform our prior.&lt;/p&gt;

&lt;p&gt;Another obvious choice is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_prior&quot;&gt;conjugate distribution&lt;/a&gt; of the likelihood. In our case, the likelihood is a categorical distribution, and so the conjugate is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_distribution&quot;&gt;Dirichlet distribution&lt;/a&gt;&lt;/p&gt;

\[p(\theta_{M}, \theta_{R}) \propto \theta_{M}^{\alpha - 1}\theta_{R}^{\beta - 1} (1-\theta_M - \theta_R)^{\gamma -1}\]

&lt;p&gt;Because it is so similar to the likelihood, we can interpret the hyperparameters of this prior distribution. $\alpha-1$ represents the previously-observed quantity of Melee attacks, and similarly for $\beta-1$ and Range, $\gamma-1$ and Magic.&lt;/p&gt;

&lt;p&gt;In other words, the conjugate prior makes it very easy to encode previous experiments into the prior distribution. We’re going to use the Dirichlet distribution for our model.&lt;/p&gt;

&lt;p&gt;A quick note about normalization: if $\alpha$ is the number of previous melee attacks, etc., then we don’t have a normalized distribution here (which is why I used $\propto$ above instead of $=$). How do we fix this?&lt;/p&gt;

&lt;p&gt;Well, the normalizing constant of a Dirichlet distribution is given by $\frac{1}{B(\alpha,\beta,\gamma)}$ where $B$ is the multivariate beta function,&lt;/p&gt;

\[\text{B}(\alpha, \beta, \gamma) = \frac{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}{\Gamma(\alpha + \beta + \gamma)}\]

&lt;p&gt;So our complete prior distribution is&lt;/p&gt;

\[p(\theta_{M}, \theta_{R}) = \frac{\Gamma(\alpha + \beta + \gamma)}{\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} \theta_{M}^{\alpha - 1}\theta_{R}^{\beta - 1} (1-\theta_M - \theta_R)^{\gamma -1}\]

&lt;h2 id=&quot;calculating-the-marginal-likelihood&quot;&gt;Calculating the Marginal Likelihood&lt;/h2&gt;

&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;We’re almost ready to bust out Baye’s theorem and calculate the posterior $p(\theta\vert y)$,&lt;/p&gt;

\[p(\theta\vert y) = \frac{p(y\vert \theta)p(\theta)}{p(y)}\]

&lt;p&gt;The numerator is known to us. We combine the sampling distribution and prior to give us&lt;/p&gt;

\[p(y\vert \theta)p(\theta) = \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1}\theta_{R}^{y_2} (1-\theta_M - \theta_R)^{y_3}\theta_{M}^{\alpha - 1}\theta_{R}^{\beta - 1} (1-\theta_M - \theta_R)^{\gamma -1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

&lt;p&gt;But we can group like terms to reduce this to&lt;/p&gt;

\[p(y\vert \theta)p(\theta) = \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

&lt;p&gt;But the denominator of Baye’s theorem is trouble. How do we calculate the marginal likelihood, $p(y)$? We could approximate it, but that’s no fun. Can we find an analytical solution? Generally, no, but when using a conjugate prior, the answer is often affirmative.&lt;/p&gt;

&lt;p&gt;Recall that we’re calculating a probability distribution over $\theta$. This means that the integral of $p(\theta\vert y)$ with respect to $\theta$ over the domain should be 1,&lt;/p&gt;

\[\int p(\theta\vert y)d\theta = \int \frac{p(y\vert \theta)p(\theta)}{p(y)} d\theta = 1\]

&lt;p&gt;We can use this constraint to solve for $p(y)$, although it is easier said than done.&lt;/p&gt;

&lt;h3 id=&quot;preparing-to-integrate&quot;&gt;Preparing to integrate&lt;/h3&gt;

&lt;p&gt;We want to integrate the posterior over all possible values of $\theta_M$ and $\theta_R$. Since our only criteria are $0\leq \theta_M, \theta_R$ and $\theta_M+\theta_R \leq 1$, then the possible values all lie on the triangle contained by $(0,0),(0,1),(1,0)$.&lt;/p&gt;

&lt;p&gt;So harkening back to &lt;a href=&quot;https://tutorial.math.lamar.edu/classes/calciii/digeneralregion.aspx&quot;&gt;Calculus III&lt;/a&gt;, we want to perform a double integration of the posterior from $\theta_R = 0$ to $\theta_R = 1$ on the region contained by $g_1(\theta_R) = 0$ below and $g_2(\theta) = -\theta_R+1$ above.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-18-runescapepvpattackprediction/Pasted image 20221121132444.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The region over which we are integrating&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So all that’s left is to plop the posterior under a double integral with these bounds. Because $p(y)$ is a constant with respect to $\theta_R$ and $\theta_M$, we can pull it out of the integral immediately:
\(\begin{align*}
&amp;amp;\int p(\theta\vert y)d\theta =\\
&amp;amp;\frac{1}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} d\theta_M d\theta_R = 1
\end{align*}\)&lt;/p&gt;

&lt;h3 id=&quot;tackling-the-integral&quot;&gt;Tackling the integral&lt;/h3&gt;

&lt;p&gt;So we need solve the integral, and then we can solve the resulting equation for $p(y)$.  Let’s pull out some more constants:&lt;/p&gt;

\[\begin{align}
\frac{1}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)} d\theta_M d\theta_R = \\
\frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)}{p(y)\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\int^1_0\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M d\theta_R
\end{align}\]

&lt;p&gt;Let’s denote the constant $\frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}$ as $C$. Now we are dealing with&lt;/p&gt;

\[\frac{C}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M d\theta_R\]

&lt;h4 id=&quot;the-interior-integral&quot;&gt;The interior integral&lt;/h4&gt;

&lt;p&gt;Focusing on the interior integral, we can remove the constants to get something like&lt;/p&gt;

\[\frac{C\theta_{R}^{y_2 + \beta - 1}}{p(y)}\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M\]

&lt;p&gt;This brings to mind an integral of the form&lt;/p&gt;

\[\int_0^k t^{a-1}(1-t)^{b-1}dt\]

&lt;p&gt;Which is known as the &lt;a href=&quot;https://mathworld.wolfram.com/IncompleteBetaFunction.html&quot;&gt;incomplete beta function&lt;/a&gt;, a generalization of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Beta_function#Relationship_to_the_gamma_function&quot;&gt;beta function&lt;/a&gt;. The solution can be expressed in terms of hypergeometric functions.&lt;/p&gt;

&lt;p&gt;However, our situation is a slight variant on the incomplete beta function, something like&lt;/p&gt;

&lt;p&gt;\(\int_0^k t^{a-1}(c-t)^{b-1}dt = c^{b-1}\int_0^k t^{a-1}(1-\frac{t}{c})^{b-1}dt\)
What if we substitute $u = \frac{t}{c}, du=\frac{1}{c}dt$?&lt;/p&gt;

\[c^{b-1}\int_0^{k/c} (cu)^{a-1}(1-u)^{b-1}cdu = c^{a+b-1}\int_0^{k/c} u^{a-1}(1-u)^{b-1}du\]

&lt;p&gt;Hell yeah, that’s an incomplete beta function! So we can solve the inner integral as an incomplete beta function using substitution. In our case, $c=(1-\theta_R)$. Then&lt;/p&gt;

\[\begin{align}
\frac{C\theta_{R}^{y_2 + \beta - 1}}{p(y)}\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M = \\
\frac{C\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}}{p(y)}\int^{1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M)^{y_3 + \gamma - 1} d\theta_M 
\end{align}\]

&lt;p&gt;Surprisingly, the bounds simplified to $0,1$, so this is actually just a boring old beta function, which we can represent as&lt;/p&gt;

\[\begin{align}
\frac{C\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}}{p(y)}\int^{1}_{0} \theta_{M}^{y_1 + \alpha - 1}(1-\theta_M)^{y_3 + \gamma - 1} d\theta_M = \\
\frac{C\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}}{p(y)}B(y_1+\alpha, y_3+\gamma)
\end{align}\]

&lt;h4 id=&quot;the-second-integral&quot;&gt;The second integral&lt;/h4&gt;

&lt;p&gt;Alright, so this leaves us with the remaining integral, which turns out to be a beta function as well. How convenient!&lt;/p&gt;

\[\begin{align}
\frac{C}{p(y)}\int^1_0\int^{-\theta_R+1}_{0} \theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1} d\theta_M d\theta_R = \\
\frac{C}{p(y)}B(y_1+\alpha, y_3+\gamma)\int^1_0\theta_{R}^{y_2 + \beta - 1}(1-\theta_R)^{y_1+y_3+\alpha+\gamma-1}d\theta_R = \\
\frac{C}{p(y)}B(y_1+\alpha, y_3+\gamma)B(y_2+\beta, y_1+y_3+\alpha+\gamma)
\end{align}\]

&lt;p&gt;This appears suspiciously asymmetric at first, but I expanded out the beta functions in terms of gamma functions and discovered that&lt;/p&gt;

\[\begin{align}
\frac{C}{p(y)}B(y_1+\alpha, y_3+\gamma)B(y_2+\beta, y_1+y_3+\alpha+\gamma) = \\
\frac{C}{p(y)}\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)}
\end{align}\]

&lt;p&gt;Which is very pretty and symmetrical, so all is well in the world.&lt;/p&gt;

&lt;h3 id=&quot;the-marginal-likelihood&quot;&gt;The marginal likelihood&lt;/h3&gt;

&lt;p&gt;Okay, so finally we are ready to solve the following equation for $p(y)$&lt;/p&gt;

\[\begin{align}
\int \frac{p(y\vert \theta)p(\theta)}{p(y)} d\theta = \frac{C}{p(y)}\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)} = 1 \implies\\
p(y) = C\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)} = \\
\frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\frac{\Gamma(y_1+\alpha)\Gamma(y_2+\beta)\Gamma(y_3+\gamma)}{\Gamma(y_1+y_2+y_3+\alpha+\gamma+\beta)} = \\
\frac{\Gamma(y_1+y_2+y_3+1)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)}\frac{B(y_1+\alpha, y_2+\beta, y_3+\gamma)}{B(\alpha,\beta,\gamma)}
\end{align}\]

&lt;p&gt;So what? Are we done? How do we know we’re correct?&lt;/p&gt;

&lt;h3 id=&quot;validating-the-marginal-likelihood&quot;&gt;Validating the marginal likelihood&lt;/h3&gt;

&lt;p&gt;We can test our formula by comparing to a Monte Carlo approximation. We’ll fix values of $\alpha, \beta, \gamma$ as well as $y_1,y_2,y_3$. Then, we can do a Monte Carlo approximation over the prior to approximate the probability of our $y$ values. It should match the output of this formula.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from scipy.special import gammaln as Gammaln
from scipy.stats import describe
import numpy as np
from numpy.random import default_rng

rng = default_rng()

def marginal_likelihood_prob(y_1, y_2, y_3, alpha, beta, delta):
    '''analytically-derived formula for p(y)'''
    prob = Gammaln(y_1+y_2+y_3+1) - (Gammaln(y_1+1)+Gammaln(y_2+1)+Gammaln(y_3+1))
    prob -= multivar_betaln(alpha, beta, delta)
    prob += multivar_betaln(y_1 + alpha, y_2 + beta, y_3 + delta)
    return np.exp(prob)

def likelihood_prob(y_1, y_2, y_3, theta_m, theta_r):
'''analytically derived formula for p(y|theta)'''
    prob = Gammaln(y_1+y_2+y_3+1) - (Gammaln(y_1+1)+Gammaln(y_2+1)+Gammaln(y_3+1))
    prob += y_1*np.log(theta_m) + y_2*np.log(theta_r) + y_3*np.log(1-theta_m-theta_r)
    return np.exp(prob)

def sample_prior(alpha, beta, delta):
'''generates a random sample from the p(theta) given hyperparameters'''
    return rng.dirichlet([alpha, beta, delta])

def naive_monte_carlo_approx(y_1, y_2, y_3, alpha, beta, delta, n=1000):
'''approximates the marginal likelihood using the prior and monte carlo approximation'''
    probs = []
    for ind in range(0, n):
        theta = sample_prior(alpha, beta, delta)
        probs.append(likelihood_prob(y_1, y_2, y_3, theta[0], theta[1]))
    return probs

if __name__ == '__main__':
y_1, y_2, y_3 = (200, 250, 200)
alpha, beta, delta = (200, 250, 200)

print('Theoretical')
print(marginal_likelihood_prob(y_1, y_2, y_3, alpha, beta, delta))

print('Monte Carlo Simulation')
print(describe(naive_monte_carlo_approx(y_1, y_2, y_3, alpha, beta, delta, n=100000000)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the output we get (which took quite a while; you can turn down &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; for faster results) is&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;captainofthedishwasher:~$ python3 test_marginal_likelihood.py
Theoretical
0.0006405754905324117
Monte Carlo Simulation
DescribeResult(nobs=100000000, minmax=(1.3299205457019604e-11, 0.0012818163603194933), mean=0.0006406077324059656, variance=1.3696618923819785e-07, skewness=0.0008881199809568082, kurtosis=-1.2002701686108446)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The mean value from the Monte Carlo simulation is a pretty good approximation of the theoretical value. I tested some other values of $y$ and $\theta$ and felt good about the results.&lt;/p&gt;

&lt;p&gt;So we’ve finally obtained an analytical formula for $p(y)$. Easy as.&lt;/p&gt;

&lt;h2 id=&quot;calculating-the-posterior&quot;&gt;Calculating the posterior&lt;/h2&gt;

&lt;p&gt;I’m going to be honest with you. The previous section, calculating $p(y)$, was brutal. It took a psychological toll on me. I want to be done. But we are so close. We can do this. Soon we will have a Runescape PVP attack prediction model. I just need to hang in there.&lt;/p&gt;

&lt;p&gt;Now that we’ve calculated all of our terms, we’re ready for the whole enchilada: the posterior. Recall once more that we want to calculate $p(\theta\vert y)$ via Baye’s theorem,&lt;/p&gt;

\[p(\theta\vert y) = \frac{p(y\vert \theta)p(\theta)}{p(y)}\]

&lt;p&gt;And we now know the numerator and denominator:&lt;/p&gt;

\[p(y\vert \theta)p(\theta) = \frac{\Gamma(y_1+y_2+y_3+1)\Gamma(\alpha + \beta + \gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

\[p(y) = \frac{\Gamma(y_1+y_2+y_3+1)}{\Gamma(y_1+1)\Gamma(y_2 + 1)\Gamma(y_3 + 1)}\frac{B(y_1+\alpha, y_2+\beta, y_3+\gamma)}{B(\alpha,\beta,\gamma)}\]

&lt;p&gt;Now we need only put them together. I can already see that a lot of terms are going to cancel (thank god)&lt;/p&gt;

\[p(\theta\vert y) = \frac{\Gamma(\alpha + \beta + \gamma)B(\alpha,\beta,\gamma)\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}}{B(y_1+\alpha, y_2+\beta, y_3+\gamma)\Gamma(\alpha)\Gamma(\beta)\Gamma(\gamma)}\]

&lt;p&gt;Which, shockingly, simplifies further since we can cancel some multivariate beta functions:&lt;/p&gt;

\[p(\theta\vert y) = \frac{1}{B(y_1+\alpha, y_2+\beta, y_3+\gamma)}\theta_{M}^{y_1 + \alpha - 1}\theta_{R}^{y_2 + \beta - 1} (1-\theta_M - \theta_R)^{y_3 + \gamma - 1}\]

&lt;p&gt;And there you have it. That’s the posterior. The source of all of our woes and suffering. It’s a boring Dirichlet distribution. What a crazy turn of events! The model is complete. I can’t believe this final step was so easy.&lt;/p&gt;

&lt;p&gt;Frankly, I’m not really sure what to do now that we’re here.&lt;/p&gt;

&lt;h1 id=&quot;using-the-bayesian-model&quot;&gt;Using The Bayesian Model&lt;/h1&gt;

&lt;p&gt;Oh god, I never thought we’d get this far. I’m really not prepared at all. I’m going to have to put together a dataset and write a program.&lt;/p&gt;

&lt;p&gt;Your eyes will simply jump from these sentences to the next as you indulge in the fruits of my labor. Meanwhile, I’m going to have to dig out my laptop, extract all my combat data (if it still exists) to JSON format, figure out how to extract opponent attacks from that, pipe that data into a program, simulate fights, update the posterior, generate some visualizations, structure it all into a narra-&lt;/p&gt;

&lt;h2 id=&quot;the-data-1&quot;&gt;The data&lt;/h2&gt;

&lt;p&gt;I had a total of 57 fights logged in PvPTracker (I leave my win/loss ratio to the reader’s imagination). The average fight had 24 opponent attacks with a variance of 190, minimum of 3, and maximum of 50. There were a total of 1419 opponent attacks logged.&lt;/p&gt;

&lt;p&gt;About 34% of opponent attacks were melee, 39% were ranged, and 27% were magic. This makes sense, as beginners generally consider ranged the “easiest” attack style, while magic is the “hardest”.&lt;/p&gt;

&lt;p&gt;I decided we’ll randomly select 10 fights to actually test the model and simulate performing posterior updates during combat. We’ll use the other 47 to determine the prior hyperparameters, $\alpha,\beta,\gamma$.&lt;/p&gt;

&lt;h2 id=&quot;initializing-the-model&quot;&gt;Initializing the model&lt;/h2&gt;

&lt;p&gt;$\alpha$ will be the total number of Melee attacks, $\beta$ will the the total of Ranged attacks, and $\gamma$ corresponds to Magic attacks from our 47 prior fights.&lt;/p&gt;

&lt;p&gt;Now that the hyperparameters are done, I decided to initialize the model parameters, $\theta_M,\theta_R$ to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirichlet_distribution#Moments&quot;&gt;expected values of the prior distribution&lt;/a&gt; (which is easy to calculate since it’s a Dirichlet distribution), namely&lt;/p&gt;

\[\theta_M = \frac{\alpha}{\alpha+\beta+\gamma}, \; \theta_R = \frac{\beta}{\alpha+\beta+\gamma}\]

&lt;p&gt;Now our model is fully initialized, and it’s ready to perform inferences and posterior updates!&lt;/p&gt;

&lt;h2 id=&quot;posterior-updates&quot;&gt;Posterior updates&lt;/h2&gt;

&lt;p&gt;Updating $\theta_M,\theta_R$ using the posterior $p(\theta\vert y)$ is actually quite simple. Because the posterior is a Dirichlet distribution, we can update the model parameters to the expected value of the posterior quite easily:&lt;/p&gt;

\[\theta_M = \frac{\alpha+y_1}{\alpha+\beta+\gamma+y_1+y_2+y_3}, \; \theta_R = \frac{\beta+y_2}{\alpha+\beta+\gamma+y_1+y_2+y_3}\]

&lt;p&gt;Now, I’m trying to stay cool here, but the astute reader has probably observed something quite distressing.&lt;/p&gt;

&lt;p&gt;The model is just calculating the average over all attacks. That’s it. There’s no further insights here.&lt;/p&gt;

&lt;p&gt;That’s so stupid. What a ripoff. I can’t believe I did all those calculations for a model that an elementary schooler could construct.&lt;/p&gt;

&lt;h1 id=&quot;conclusion-this-model-sucks&quot;&gt;Conclusion: This Model Sucks&lt;/h1&gt;

&lt;p&gt;There’s not much to be said about the data since the model just calculates averages.&lt;/p&gt;

&lt;p&gt;The real insight comes from the fact that I wasted so much time deriving this model. How did I do it? Will this happen to me again?&lt;/p&gt;

&lt;p&gt;I think that the simplistic outcome should have been predictable from my assumptions. I reduced the problem to a multinomial model, which led to a very simple algorithm. I could probably impose some structure to make the model more interesting, but I’m tired and I want to move on with my life.&lt;/p&gt;

&lt;p&gt;Here are some ideas I had for salvaging the model to make it more interesting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make it a hierarchical model where the multinomial parameters for each fight come from another distribution
    &lt;ul&gt;
      &lt;li&gt;Could allow for clustering of opponent behaviors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Consider correlations between successive attacks instead of independent attacks&lt;/li&gt;
  &lt;li&gt;Integrate variables such as health into the model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But these are for another day when I am not so tired. Now, I must rest.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Weakly Supervised Video Segmentation In Runescape</title><link href="http://localhost:4000/2023/01/03/runescapesegmentation.html" rel="alternate" type="text/html" title="Weakly Supervised Video Segmentation In Runescape" /><published>2023-01-03T00:00:00-05:00</published><updated>2023-01-03T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/03/runescapesegmentation</id><content type="html" xml:base="http://localhost:4000/2023/01/03/runescapesegmentation.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#abstract&quot; id=&quot;markdown-toc-abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#related-work&quot; id=&quot;markdown-toc-related-work&quot;&gt;Related Work&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problem-statement-and-implementation&quot; id=&quot;markdown-toc-problem-statement-and-implementation&quot;&gt;Problem Statement and Implementation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#initialization-and-data-collection&quot; id=&quot;markdown-toc-initialization-and-data-collection&quot;&gt;Initialization and Data Collection&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#video-segmentation&quot; id=&quot;markdown-toc-video-segmentation&quot;&gt;Video Segmentation&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#postprocessing&quot; id=&quot;markdown-toc-postprocessing&quot;&gt;Postprocessing&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#future-frame-prediction&quot; id=&quot;markdown-toc-future-frame-prediction&quot;&gt;Future Frame Prediction&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experimental-results&quot; id=&quot;markdown-toc-experimental-results&quot;&gt;Experimental Results&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cnn-training&quot; id=&quot;markdown-toc-cnn-training&quot;&gt;CNN Training&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-segmentation&quot; id=&quot;markdown-toc-basic-segmentation&quot;&gt;Basic Segmentation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#postprocessing-1&quot; id=&quot;markdown-toc-postprocessing-1&quot;&gt;Postprocessing&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#spatial-diffusion&quot; id=&quot;markdown-toc-spatial-diffusion&quot;&gt;Spatial Diffusion&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#temporal-diffusion&quot; id=&quot;markdown-toc-temporal-diffusion&quot;&gt;Temporal Diffusion&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#future-frame-prediction-1&quot; id=&quot;markdown-toc-future-frame-prediction-1&quot;&gt;Future Frame Prediction&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#retrospective&quot; id=&quot;markdown-toc-retrospective&quot;&gt;Retrospective&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post is actually an old project of mine from grad school. The goal was to find a way to automatically generate training data for a computer vision algorithm specifically in the context of Old School Runescape. You can find the original paper &lt;a href=&quot;https://drive.google.com/file/d/14LsWzqAO1FZIG-KbWYlGT2TAjHHjzMLX/view&quot;&gt;here&lt;/a&gt; and the source code &lt;a href=&quot;https://github.com/horenbergerb/OSRS_Optical_Recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Collecting training data for niche applications of computer vision is difficult. We propose a method for performing video segmentation which is tailored point-and-click video games featuring tooltips. We focus specifically on Old School Runescape.&lt;/p&gt;

&lt;p&gt;The method is “weakly-supervised” in that it does not need to be trained on complete segmentation masks. Instead, a model is trained to produce classifications for 32x32 subregions of the full image.&lt;/p&gt;

&lt;p&gt;Segmenting by classifying a grid of subregions has costs and benefits. Some spatial and temporal information is lost. The resolution of the segmentation mask is low. On the other hand, we will show that the training data can be automatically generated in large quantities for certain games. Additionally, the segmentation is fast and can be done in real time.&lt;/p&gt;

&lt;p&gt;We also show that segmentation masks can be postprocessed to reincorporate lost information or induce desirable qualities.&lt;/p&gt;

&lt;p&gt;Finally, we make use of the segmentation masks by attempting to predict future segmentation masks in Runescape. We find that it is possible to marginally outperform baseline methods for future prediction for the time scale of about 0.75 seconds into the future.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/segmentedscreen.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A segmentation mask for identifying cows in Old School Runescape&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Video segmentation is a highly nontrivial problem with many applications. The goal of video segmentation is to divide video into regions in a way that distinguishes objects or areas of interest. It is a notoriously difficult problem, due largely to its high dimensionality.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/d/d4/Image_segmentation.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example image segmentation from Wikimedia Commons&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It is difficult to apply machine learning to video segmentation since labeled training data requires an exceptional amount of preparation. This is especially problematic when publicly-available training datasets do not exist, which is the case for many tailored use cases.&lt;/p&gt;

&lt;p&gt;This paper presents a weakly-supervised machine learning algorithm for video segmentation. By making certain assumptions, this algorithm reduces the problem of video segmentation to a problem of many low-resolution image classifications.&lt;/p&gt;

&lt;p&gt;The associated training data can be collected easily for certain video games by using an automated procedure. This is demonstrated for the video game “Old School Runescape.”&lt;/p&gt;

&lt;p&gt;The design of the model comes at a cost. The model assumes that segmentation masks can be generated in a region using only local spatial information. Additionally, region membership is assumed constant within 32x32 regions, and thus the segmentation masks have low resolution.&lt;/p&gt;

&lt;p&gt;These assumptions restrict the applications of the model compared to other video segmentation method. However, this model performs well when regions are distinguishable based on local properties and fine-grained segmentation is not essential. Additionally, postprocessing can relax the assumptions of locality by reincorporating information from other regions.&lt;/p&gt;

&lt;p&gt;Finally, the question is raised whether one can use the low-dimension segmentation masks for tasks such as future frame prediction. This is investigated in the case of training data from Old School Runescape, and the results are tentatively affirmative, although further investigation is necessary.&lt;/p&gt;

&lt;h1 id=&quot;related-work&quot;&gt;Related Work&lt;/h1&gt;

&lt;p&gt;Some of the oldest image segmentation algorithms relied on the detection of edges. &lt;a href=&quot;https://ieeexplore.ieee.org/document/4767851&quot;&gt;Canny edge detection&lt;/a&gt; and the &lt;a href=&quot;http://www.bmva.org/bmvc/1988/avc-88-023.pdf&quot;&gt;Harris corner and edge detector&lt;/a&gt; both use intensity gradients to detect edges. Edges forming closed loops or spanning the image can be used as divisors between regions, thus providing a simple model for image segmentation.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/Pasted image 20230103144534.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example of the Canny edge detector &lt;a href=&quot;https://en.wikipedia.org/wiki/Canny_edge_detector&quot;&gt;from Wikipedia&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;http://library.isical.ac.in:8080/xmlui/bitstream/handle/10263/5304/A%20review%20on%20image%20segmentation%20techniques-PR-26-9-1993-%20p%201277-1294.pdf?sequence=1&amp;amp;isAllowed=y&quot;&gt;Region-based methods&lt;/a&gt; such as region growing begin by placing seed points and allowing regions to expand based on the properties of marginal pixels, such as intensity, texture, or color.&lt;/p&gt;

&lt;p&gt;Another popular technique for image segmentation is &lt;a href=&quot;https://ijcset.net/docs/Volumes/volume2issue1/ijcset2012020103.pdf&quot;&gt;histogram thresholding&lt;/a&gt;, in which the histogram of a black-and-white image is partitioned by some threshold in order to determine to classify each pixel based on intensity.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/Pasted image 20230103144634.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Intensity histograms of goblins tend to be similar and can be used for classification&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Regarding data-driven approaches to image segmentation, convolutional neural networks (CNNs) are a natural choice. &lt;a href=&quot;https://arxiv.org/pdf/1512.07108.pdf&quot;&gt;Convolutional neural networks&lt;/a&gt; use feature extraction to perform a variety of tasks including object detection, region of interest proposal, and image segmentation.&lt;/p&gt;

&lt;p&gt;There exists &lt;a href=&quot;https://arxiv.org/abs/1502.02734&quot;&gt;weakly-supervised approaches&lt;/a&gt; to image segmentation which use only information such as bounding boxes around objects or image-level labels to train a CNN paired with a conditional random field.&lt;/p&gt;

&lt;p&gt;One of the more successful approaches to video segmentation is the &lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf&quot;&gt;Mask R-CNN&lt;/a&gt;, which performs object instance segmentation. The underlying premise is dividing an image into regions of interest (ROIs) and simultaneously producing masks for image segmentation within each ROI.&lt;/p&gt;

&lt;p&gt;Presently, it is not known to the authors whether there exists work on low-resolution image or video segmentation.&lt;/p&gt;

&lt;h1 id=&quot;problem-statement-and-implementation&quot;&gt;Problem Statement and Implementation&lt;/h1&gt;

&lt;p&gt;The problem at hand is to perform video segmentation on a video game with “tooltips.” The tooltip indicates the actions available to the player based on cursor position. In the game which we have chosen to focus on, Runescape, most all actions are performed by moving the cursor to objects and clicking them.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/tooltip_example.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The tooltip describes what the mouse is currently hovering over&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the initialization phase of the algorithm, the tooltip and the mouse position will be used to generate training data automatically for a classifier which can classify small 32x32 regions of the image.&lt;/p&gt;

&lt;p&gt;After the training data is collected and parsed, the CNN will be trained, completing the initialization.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/overviewflowchart.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The schematic for our segmentation algorithm, including data collection and training&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;During the video segmentation phase, the CNN is used to generate a segmentation mask. The mask is then postprocessed to its final form. Lastly, we use the segmentation masks to perform future frame prediction.&lt;/p&gt;

&lt;p&gt;We discuss these two phases in detail below.&lt;/p&gt;

&lt;h2 id=&quot;initialization-and-data-collection&quot;&gt;Initialization and Data Collection&lt;/h2&gt;

&lt;p&gt;The initialization procedure is illustrated below. Initialization begins by capturing raw data from Runescape gameplay. While the user plays, automated Python software performs screencaptures and simultaneously records the mouse position. This is achieved using the &lt;a href=&quot;https://python-mss.readthedocs.io/&quot;&gt;MSS&lt;/a&gt; and &lt;a href=&quot;https://pyautogui.readthedocs.io/en/latest/&quot;&gt;PyAutoGUI&lt;/a&gt; Python packages.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/initializationflowchart.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Overview of the initialization procedure&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The pairs of screenshots and cursor coordinates are then parsed into training data. To achieve this, color filtering is used on the tooltip to extract keywords. In particular, Runescape denotes the object of the tooltip’s verb in clustered shades of yellow.&lt;/p&gt;

&lt;p&gt;We use samples of these yellow shades to develop a mask based on color similarity. The resulting masks can be used to perform optical character recognition using &lt;a href=&quot;https://pypi.org/project/pytesseract/&quot;&gt;PyTesseract&lt;/a&gt;, a Python package which is a wrapper for Google’s &lt;a href=&quot;https://opensource.google/projects/tesseract&quot;&gt;Tesseract&lt;/a&gt;. All image processing in this project was performed using the &lt;a href=&quot;https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html&quot;&gt;OpenCV Python package&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, a 32x32 region is cropped from the screenshot centered on the user’s cursor position. In this way, the parsed tooltip is now a text label for the noun pictured in the 32x32 image.&lt;/p&gt;

&lt;p&gt;These images and text labels together are sufficient information to train the small CNN classifier pictured below, which can predict the label associated with a given 32x32 image. Every convolutional layer of the CNN was followed by an ReLU activation function and a batch normalization layer. All machine learning in this project was performed using the &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch Python package&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/cnnarchitecture.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The CNN architecture used to classify the 32x32 subregions of the screen&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This CNN is sufficient to perform basic video segmentation which can then be further postprocessed. We discuss this further in the next section.&lt;/p&gt;

&lt;h2 id=&quot;video-segmentation&quot;&gt;Video Segmentation&lt;/h2&gt;

&lt;p&gt;With the trained CNN from the initialization phase, we can perform basic video segmentation.&lt;/p&gt;

&lt;p&gt;To achieve this, each frame of the video is broken into a grid of 32x32 regions. Then, each region is independently classified by the CNN. These classifications determine region membership. Consequently, the collective classifications of the entire grid determine a low-resolution segmentation mask for the frame.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/segmentationdiagram.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Each 32x32 subregion is separately classified to create the segmentation mask&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;postprocessing&quot;&gt;Postprocessing&lt;/h3&gt;

&lt;p&gt;The calculated segmentation mask is then postprocessed to reincorporate lost information. We implemented two methodologies for postprocessing.&lt;/p&gt;

&lt;p&gt;The first is spatial diffusion, which uses the Gauss-Seidel method to iteratively update neighboring probabilities. In particular, if $P[\mathbf{x},t]$ indicates the probabilities in grid region $\mathbf{x}$ at time $t$, and if $\eta(\mathbf{x})$ indicates the neighborhood of $\mathbf{x}$, then over each frame we iteratively apply&lt;/p&gt;

\[P[\mathbf{x},t] = \frac{1}{4}\sum_{\mathbf{y}\in\eta(\mathbf{x})} P[\mathbf{y},t]\]

&lt;p&gt;to every grid region until the process converges. In this way, some global spatial information is reincorporated into the segmentation by diffusing probabilities outward into their neighboring regions.&lt;/p&gt;

&lt;p&gt;The other postprocessing method is temporal averaging, in which each frame’s probabilities are recomputed as&lt;/p&gt;

\[P[\mathbf{x},t] = \frac{1}{2}(P[\mathbf{x},t]+P[\mathbf{x},t-1])\]

&lt;p&gt;This reincorporates temporal information into the segmentation mask and smooths temporal anomalies.&lt;/p&gt;

&lt;p&gt;Certainly many other possible filters or postprocessing methods exist, such as spatiotemporal filtering. However, these are left as an exercise to the reader.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/videosegmentationoverview.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Overview of the actual segmentation and future frame prediction&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;future-frame-prediction&quot;&gt;Future Frame Prediction&lt;/h3&gt;

&lt;p&gt;The final phase of the video segmentation is utilization, in which we attempt to predict future segmentation masks given a set of past segmentation masks. This task was performed using a 3DCNN architecture detailed below.&lt;/p&gt;

&lt;p&gt;We used three frames as inputs and predicted one future frame as output. Different future distances and intervals between input frames were tested to find the regimes in which future frame prediction was most viable.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/3dcnndiagram.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Architecture of the 3DCNN used for future frame prediction&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;experimental-results&quot;&gt;Experimental Results&lt;/h1&gt;

&lt;h2 id=&quot;cnn-training&quot;&gt;CNN Training&lt;/h2&gt;

&lt;p&gt;The CNN for 32x32 image classification was trained on two labels, “cow” and “none.” Each label had approximately 900 training images and 100 validation images.&lt;/p&gt;

&lt;p&gt;The model converged very quickly and achieved accuracy of approximately 95\%. It was also attempted to train the model on 7 different labels, including “goblin,” “tree,” and “chicken.” Each label had between 300 to 500 images.&lt;/p&gt;

&lt;p&gt;In this case, accuracy was much lower at around 60\%, possibly suggesting the need for more training data or a more elaborate architecture. For this reason the project was limited to the case of “cow” vs. “none” labels.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/cnntraining.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The validation accuracy rapidly converged when segmenting with classes “cow” and “not cow”&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;basic-segmentation&quot;&gt;Basic Segmentation&lt;/h2&gt;

&lt;p&gt;One difficulty of the project was the unavailability of validation data. As a result, there is no easy way to compare segmentation results to ground truth.&lt;/p&gt;

&lt;p&gt;Due to time constraints, we were not able to annotate a ground truth for comparison. We illustrate here a typical output of segmentation and discuss the properties of the segmentation mask.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/origscreen.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A screenshot of the game before segmentation is performed&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/segmentedscreen.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The same screenshot with a segmentation mask applied distinguishing “cow” and “not cow”&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;One obvious shortcoming was the mislabeling of the GUI elements in the corners of the play screen. This was due in part to the data collection process.&lt;/p&gt;

&lt;p&gt;Particularly, there is a sampling bias based on where users tend to place their cursor. Additionally, samples were only kept if the 32x32 region surrounding the mouse were located entirely in the play screen, so it was relatively unlikely for the edges of the screen to be sampled compared to the center of the screen.&lt;/p&gt;

&lt;p&gt;There are other quirks of the dataset which could also contribute to some of the misclassifications.&lt;/p&gt;

&lt;p&gt;The player character is labeled as “none” by the sampling process, so it is likely they would have been included in the training dataset. However, objects which had labels such as “farmer” or “goblin” were not included in the training data.&lt;/p&gt;

&lt;p&gt;This means many objects from the game were excluded during training, and thus the data set is not as diverse as the actual playscreen. This issue is easily corrected with better data preparation procedures.&lt;/p&gt;

&lt;h2 id=&quot;postprocessing-1&quot;&gt;Postprocessing&lt;/h2&gt;

&lt;p&gt;The lack of validation data makes empirical analysis of the effects of postprocessing impossible at this time. However, it can be subjectively verified that each postprocessing method induces certain qualities.&lt;/p&gt;

&lt;h3 id=&quot;spatial-diffusion&quot;&gt;Spatial Diffusion&lt;/h3&gt;

&lt;p&gt;Small regions are smoothed out after spatial diffusion, and convex areas tend to be filled in. This smoothing could be effective when regions are expected to be large and rounded.&lt;/p&gt;

&lt;p&gt;However, if the ground truth data contains small isolated regions, then this filter is likely to cause incorrect classifications. Spatial diffusion is thus contextual, and it may be worth investigating whether it could be applied situationally.&lt;/p&gt;

&lt;p&gt;In this demo, the left-hand side shows the original segmentation mask while the right-hand side shows the mask after applying spatial diffusion.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/uYBu1VAFKXU&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;temporal-diffusion&quot;&gt;Temporal Diffusion&lt;/h3&gt;

&lt;p&gt;Temporal diffusion reduces temporally anomalous classifications, but it also induces a lag in segmentation updates when motion is occuring. This is somewhat apparent in the video.&lt;/p&gt;

&lt;p&gt;As the camera rotated counterclockwise about the player, the cows are approximately translating on the screen. With temporal averaging, the segmentation mask lags behind the cows.&lt;/p&gt;

&lt;p&gt;In this demo, the left-hand side shows the original segmentation mask while the right-hand side shows the mask after applying temporal diffusion.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/7GvkrOqumgg&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;future-frame-prediction-1&quot;&gt;Future Frame Prediction&lt;/h2&gt;

&lt;p&gt;Future frame prediction was first tested using 3 successive segmentation masks sampled at times $t-2,t-1,t$ to predict a segmentation mask 6 frames into the future at time $t+6$. 6 frames is approximately 1/7 of a second.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframepredictiondiagram.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Illustration of the future prediction inputs and outputs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The distance of 6 frames into the future was chosen based on preliminary subjective analysis. It was approximately the distance which appeared to have the best performance, although this claim needs further verification. Every model was compared to a baseline naive model which assumes no motion occurs.&lt;/p&gt;

&lt;p&gt;The training was performed with approximately 7200 frames of continuous video while validation was performed on a separate 1600 frames of continuous video.&lt;/p&gt;

&lt;p&gt;As seen in the training graphs, the model reaches peak performance on the validation set very early and then begins overfitting. This trend continues at least up to 30 epochs.&lt;/p&gt;

&lt;p&gt;At peak performance, the model achieved a top tile classification accuracy of 86.608% on the validation data, while the baseline naive model achieved 86.231%. Thus, the performance is at best only a marginal improvement upon basic assumptions.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframetraining.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Overfitting begins after very few epochs, and the best performance only marginally outperforms naive future prediction methods&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;However, a second model was attempted using not successive frames as inputs but instead using a step size of 5 frames, as depicted in Figure~\ref{futureframediagram2}. In this case, the predictions were much further into the future at 30 frames, or approximately 3/4 of a second.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframediagram2.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Illustration of the future prediction inputs and outputs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With this new model, training appeared subjectively similar. Although the validation accuracy was lower at 75.560%, the model performed notably better than the naive approach, which had an accuracy of 71.672%.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2023-01-03-runescapesegmentation/futureframetraining2.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Although overfitting still occurs, the model outperforms naive methods when predicting further into the future&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Although these results are promising, more research is needed. Particularly, it is worth investigating whether the performance can be improved by the inclusion of more training data.&lt;/p&gt;

&lt;p&gt;Additionally, many future frame prediction models use an attention mechanism such as long short-term memory. The effects of including an attention mechanism should be investigated.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This project has demonstrated that it is possible to collect the necessary data for performing video segmentation on a video game such as Runescape. In addition, the video segmentation can be augmented with postprocessing methods, and the segmentation masks can be utilized to perform tasks such as future frame prediction.&lt;/p&gt;

&lt;p&gt;There are many remaining questions which deserve further investigation. For example, only two simple postprocessing filters were examined. Spatiotemporal filters might offer better performance, or perhaps there is an opportunity to apply machine learning in the postprocessing step.&lt;/p&gt;

&lt;p&gt;Additionally, our future frame prediction model has many obvious extensions. It also remains to detail the performance over different time scales and with varying amounts of input information.&lt;/p&gt;

&lt;p&gt;Finally, it may be of interest to extend these methodologies to other games with different graphics engines. The methods presented here serve as a springboard for more comprehensive investigations.&lt;/p&gt;

&lt;h1 id=&quot;retrospective&quot;&gt;Retrospective&lt;/h1&gt;

&lt;p&gt;I wrote this paper about a year ago. I was very proud of this project at the time. I still am. I felt like this was a viable solution to a real problem I was facing at the time (i.e. how to make Runescape bots).&lt;/p&gt;

&lt;p&gt;There are a few things I didn’t really explore in this paper which I would like to revisit some time.&lt;/p&gt;

&lt;p&gt;For one, I think I naively thresholded classifications at 0.5. I now know that one should experimentally determine how to threshold the classifier outputs.&lt;/p&gt;

&lt;p&gt;I’d also like to really document the speed of this system. I know it was generating multiple masks per second, but how much time is really left over for other processing? Do we have enough spare compute power to beef up the classifier’s architecture?&lt;/p&gt;

&lt;p&gt;For that matter, should I look into using a more typical classifier architecture, such as YOLO or something faster maybe?&lt;/p&gt;

&lt;p&gt;Lastly, I would like to work on the future frame prediction. I think one of the major issues is that the future frame prediction has no fully connected layers. What’s up with that?? I would also be interested in using transformers instead of 3DConv layers.&lt;/p&gt;

&lt;p&gt;Anyways, I really liked this project, so I wanted to show it off on my blog. I’m sure it’s not my most exciting work, but you’re going to have to deal with it.&lt;/p&gt;

&lt;p&gt;Thanks for reading!&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Teaching ChatGPT to Play Runescape</title><link href="http://localhost:4000/2022/12/15/chatgptrunescape.html" rel="alternate" type="text/html" title="Teaching ChatGPT to Play Runescape" /><published>2022-12-15T00:00:00-05:00</published><updated>2022-12-15T00:00:00-05:00</updated><id>http://localhost:4000/2022/12/15/chatgptrunescape</id><content type="html" xml:base="http://localhost:4000/2022/12/15/chatgptrunescape.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#reducing-runescape-to-text&quot; id=&quot;markdown-toc-reducing-runescape-to-text&quot;&gt;Reducing Runescape to Text&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nlp-or-reinforcement-learning-for-video-games&quot; id=&quot;markdown-toc-nlp-or-reinforcement-learning-for-video-games&quot;&gt;NLP or Reinforcement Learning for Video Games?&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforcement-learning&quot; id=&quot;markdown-toc-reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#nlp&quot; id=&quot;markdown-toc-nlp&quot;&gt;NLP&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminary-investigation-of-chatgpt&quot; id=&quot;markdown-toc-preliminary-investigation-of-chatgpt&quot;&gt;Preliminary Investigation of ChatGPT&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#what-does-chatgpt-know-about-runescape&quot; id=&quot;markdown-toc-what-does-chatgpt-know-about-runescape&quot;&gt;What Does ChatGPT Know About Runescape?&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#general-information-about-runescape&quot; id=&quot;markdown-toc-general-information-about-runescape&quot;&gt;General Information About Runescape&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review&quot; id=&quot;markdown-toc-review&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#overview-of-the-skill-system&quot; id=&quot;markdown-toc-overview-of-the-skill-system&quot;&gt;Overview of The Skill System&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-1&quot; id=&quot;markdown-toc-review-1&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#detailed-information-about-mining&quot; id=&quot;markdown-toc-detailed-information-about-mining&quot;&gt;Detailed Information About Mining&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-2&quot; id=&quot;markdown-toc-review-2&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#general-quests&quot; id=&quot;markdown-toc-general-quests&quot;&gt;General Quests&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-3&quot; id=&quot;markdown-toc-review-3&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#specific-quests&quot; id=&quot;markdown-toc-specific-quests&quot;&gt;Specific Quests&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#review-4&quot; id=&quot;markdown-toc-review-4&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#simple-roleplay&quot; id=&quot;markdown-toc-simple-roleplay&quot;&gt;Simple Roleplay&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#review-5&quot; id=&quot;markdown-toc-review-5&quot;&gt;Review&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#chatgpt-plays-runescape&quot; id=&quot;markdown-toc-chatgpt-plays-runescape&quot;&gt;ChatGPT Plays Runescape&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#warmup&quot; id=&quot;markdown-toc-warmup&quot;&gt;Warmup&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chatgpt-gets-the-hang-of-it&quot; id=&quot;markdown-toc-chatgpt-gets-the-hang-of-it&quot;&gt;ChatGPT Gets the Hang of It&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#intermission&quot; id=&quot;markdown-toc-intermission&quot;&gt;Intermission&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chatgpt-cuts-down-a-tree&quot; id=&quot;markdown-toc-chatgpt-cuts-down-a-tree&quot;&gt;ChatGPT Cuts Down a Tree&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reflection&quot; id=&quot;markdown-toc-reflection&quot;&gt;Reflection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#future-steps&quot; id=&quot;markdown-toc-future-steps&quot;&gt;Future Steps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;reducing-runescape-to-text&quot;&gt;Reducing Runescape to Text&lt;/h1&gt;

&lt;p&gt;Some of you may be aware that I have previously worked on using computer vision to extract data from a live feed of &lt;a href=&quot;https://oldschool.runescape.com/&quot;&gt;Runescape&lt;/a&gt;. For example, I have &lt;a href=&quot;https://horenbergerb.github.io/2022/03/04/runescapelocationfinding.html&quot;&gt;estimated the player’s global location&lt;/a&gt; and created a &lt;a href=&quot;https://horenbergerb.github.io/2022/07/19/runescapeobjectdetection.html&quot;&gt;detector of interactible objects&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have also written other scripts to parse more information from the game. More than I care to admit, frankly.&lt;/p&gt;

&lt;p&gt;Really, I’ve been working on reducing the game to an interface that describes the game state completely in text. The interface will also accept written commands which control the character.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/game_to_text_diagram 1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But why? What’s the use of turning Runescape into a text-based adventure?&lt;/p&gt;

&lt;p&gt;Originally I was just practicing computer vision and ML techniques. I started to wonder if I could make an interface that an ML algorithm could use to play Runescape.&lt;/p&gt;

&lt;p&gt;Realistically, I had no shot. Training an agent to imitate a typical Runescape player was far beyond the achievable possibilities of an armchair expert like me.&lt;/p&gt;

&lt;p&gt;Well, things might be changing with the release of &lt;a href=&quot;https://openai.com/blog/chatgpt/&quot;&gt;ChatGPT&lt;/a&gt;. ChatGPT is a natural language processing (NLP) model which can perform a variety of tasks and regurgitate facts about many topics (although it is often incorrect).&lt;/p&gt;

&lt;p&gt;In this article, I wanted to explore the possibility that a text-based representation of Runescape could allow ChatGPT to play the game and effectively replicate human players.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215204212.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Proof that OpenAI hates PKers and they want to remove the wilderness&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;nlp-or-reinforcement-learning-for-video-games&quot;&gt;NLP or Reinforcement Learning for Video Games?&lt;/h1&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Typically automation of video games is approached using &lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;reinforcement learning&lt;/a&gt;. This makes a lot of sense for the following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Games are often objective-based in a quantifiable way&lt;/li&gt;
  &lt;li&gt;Games are typically a sequential process of decision-making over time where your current state and actions depend on your history&lt;/li&gt;
  &lt;li&gt;Games can often be replayed arbitrarily many times as part of the training process&lt;/li&gt;
  &lt;li&gt;Any quantifiable interface for a video game is compatible with reinforcement learning
    &lt;ul&gt;
      &lt;li&gt;ex) inputs from joysticks, button presses, mouse clicks&lt;/li&gt;
      &lt;li&gt;ex) game state represented as a large structured dataset of discrete/continuous quantities&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Famously, OpenAI used reinforcement learning to train &lt;a href=&quot;https://openai.com/five/&quot;&gt;OpenAI Five&lt;/a&gt; to play DOTA. The algorithm was eventually able to beat many professional teams.&lt;/p&gt;

&lt;h2 id=&quot;nlp&quot;&gt;NLP&lt;/h2&gt;

&lt;p&gt;NLP is probably not a good choice for video games in general. I shouldn’t have to explain why. It’s just not designed to play video games.&lt;/p&gt;

&lt;p&gt;However, some games have properties which make them particularly difficult for reinforcement learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Some games do not have clear victory conditions. In this case, “typical” gameplay can be hard to quantify and often very diverse&lt;/li&gt;
  &lt;li&gt;Some games have elaborate mechanics relying heavily on &lt;a href=&quot;https://en.wikipedia.org/wiki/Common_sense&quot;&gt;common sense&lt;/a&gt;, which reinforcement learning does not have
    &lt;ul&gt;
      &lt;li&gt;ex) knowing that you need an axe to cut down trees, that using a fishing rod on a fishing spot will catch fish and give you fishing experience, that eating food will heal you, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Some games are large and have no start and end; iterating multiple playthroughs for training is not feasible.
    &lt;ul&gt;
      &lt;li&gt;ex) most MMORPG games are far too large to train with repeated playthroughs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NLP might be more successful under these circumstances due to its general knowledge. It can approximate common sense and make reasonable statements about “typical” behavior of an MMO player. Potentially, NLP models could transfer this knowledge to a game without specific training. Because of these properties, it’s possible that an NLP model might be able to interact with a game more effectively than a reinforcement learning algorithm.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215215209.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;ChatGPT has a lot of general knowledge about MMORPGs, but it still makes mistakes&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Runescape has many of the properties which make direct reinforcement learning very difficult. Could NLP have a competitive advantage here?&lt;/p&gt;

&lt;h1 id=&quot;preliminary-investigation-of-chatgpt&quot;&gt;Preliminary Investigation of ChatGPT&lt;/h1&gt;

&lt;p&gt;Enough fuckin’ around. What does ChatGPT know about Runescape, and can it farm gold for me?&lt;/p&gt;

&lt;p&gt;I began by investigating what kinds of information ChatGPT might contain regarding Runescape. I asked a series of questions, first about the game in general, and then more specific concepts. Here is some of the dialogue.&lt;/p&gt;

&lt;p&gt;Afterwards, I tried to have ChatGPT roleplay simple situations within the game to describe what actions it might take.&lt;/p&gt;

&lt;h2 id=&quot;what-does-chatgpt-know-about-runescape&quot;&gt;What Does ChatGPT Know About Runescape?&lt;/h2&gt;

&lt;h3 id=&quot;general-information-about-runescape&quot;&gt;General Information About Runescape&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Explain the video game Old School Runescape&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Old School Runescape (OSRS) is a massively multiplayer online role-playing game (MMORPG) developed and published by Jagex. It is a re-imagined version of the classic MMORPG Runescape, which was first released in 2001. In OSRS, players can create their own character and embark on various quests and adventures in a fantasy world filled with magic and mythical creatures. The game is known for its open-world environment and its challenging gameplay, which requires players to complete various tasks and puzzles in order to progress. OSRS has a dedicated player base and is known for its active community of players who regularly participate in in-game events and activities.&lt;/p&gt;

&lt;h4 id=&quot;review&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;Not bad. A lot of this information is accurate.  Some of the descriptors are a little boiler-plate.&lt;/p&gt;

&lt;h3 id=&quot;overview-of-the-skill-system&quot;&gt;Overview of The Skill System&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What sort of skills are available to players in Old School Runescape?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Old School Runescape, players can choose from a wide variety of skills to train and develop. Some of the skills available in the game include combat skills (such as Attack, Strength, and Defence), gathering skills (such as Mining, Fishing, and Woodcutting), and crafting skills (such as Smithing, Cooking, and Fletching). There are also several magic-based skills, such as Magic and Prayer, as well as miscellaneous skills like Thieving and Hunter.&lt;/p&gt;

&lt;p&gt;Each skill has its own unique gameplay mechanics and benefits. For example, training the Cooking skill allows players to cook and prepare food, which can be used to restore health and provide temporary stat bonuses. Training the Magic skill allows players to cast spells, which can be used in combat or to perform various tasks. And training the Thieving skill allows players to pickpocket NPCs and steal from stalls and chests.&lt;/p&gt;

&lt;p&gt;The skills available in Old School Runescape are varied and diverse, and allow players to tailor their character’s abilities to their playstyle and preferences. As you progress through the game, you can choose to focus on developing certain skills, or to train a wide variety of skills in order to become a well-rounded character.&lt;/p&gt;

&lt;h4 id=&quot;review-1&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;Pretty much dead-on. Everything here is accurate and it’s a decent summary.&lt;/p&gt;

&lt;h3 id=&quot;detailed-information-about-mining&quot;&gt;Detailed Information About Mining&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What items and skills are needed to mine copper ore in Old School Runescape?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To mine copper ore in Old School RuneScape, you will need the following items and skills:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;A pickaxe: You will need a bronze, iron, steel, or mithril pickaxe to mine copper ore. A higher level pickaxe will allow you to mine copper ore faster, but any of these pickaxes will work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mining level: You will need to have at least level 1 Mining to be able to mine copper ore. As you increase your Mining level, you will be able to mine copper ore more efficiently.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A mining location: Copper ore can be found at a variety of locations throughout the game, including the Lumbridge Swamp Mines, the Al-Kharid Mine, and the Piscatoris Mining Site.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To start mining copper ore, first equip your pickaxe and then find a rock that contains copper ore. Left-click on the rock to begin mining it, and you will receive copper ore as you gather resources. You can then smelt the copper ore to turn it into a usable bar of metal, which can be used in various crafting and construction tasks throughout the game.&lt;/p&gt;

&lt;h4 id=&quot;review-2&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;Wow, this one is good. Here are the nitpicks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It didn’t list &lt;em&gt;all&lt;/em&gt; of the pickaxes&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://oldschool.runescape.wiki/w/Pickaxe&quot;&gt;A better pickaxe doesn’t make you faster&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;You don’t need to equip the pickaxe to mine&lt;/li&gt;
  &lt;li&gt;You also need tin to smelt the copper&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But the gist of it is correct; someone following these instructions could successfully mine copper. Even the locations are valid, and they’re (mostly) decent choices for new players too.&lt;/p&gt;

&lt;h3 id=&quot;general-quests&quot;&gt;General Quests&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;What are some important quests that new players of Old School Runescape should complete?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a new player to Old School Runescape, there are several quests that you should consider completing in order to progress in the game and gain access to new content and features. Some of the most important quests for new players include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tutorial Island: This is the starting quest for all new players in Old School Runescape. It provides a basic introduction to the game’s mechanics and controls, and is a necessary starting point for all new players.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cook’s Assistant: This is a simple quest that tasks players with gathering ingredients for a cook in the village of Lumbridge. Completing this quest will reward players with experience in the Cooking skill, as well as some basic cooking equipment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Restless Ghost: This quest tasks players with finding and helping a restless ghost who is haunting the town of Lumbridge. Completing this quest will reward players with experience in the Prayer skill, as well as the ability to wield the white skull sceptre, which can be used to teleport to various locations in the game world.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rune Mysteries: This quest involves investigating a mysterious disappearance at the Rune Essence mine, and is a good introduction to the game’s magic-based skills. Completing this quest will reward players with experience in the Rune Crafting skill, as well as the ability to teleport to the Rune Essence mine.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Overall, completing these quests will help new players to get a feel for the game and its mechanics, and will provide them with valuable experience and rewards that can help them to progress in the game.&lt;/p&gt;

&lt;h4 id=&quot;review-3&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;God damn, this is pretty good too. I mean, it’s not an optimal quest list are anything, but these are good quests for beginners to Runescape, and the descriptions are decent. Not sure about the “white skull sceptre” though. I think that’s a lie.&lt;/p&gt;

&lt;h3 id=&quot;specific-quests&quot;&gt;Specific Quests&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please summarize the plot of the Cook’s Assistant quest in Old School Runescape.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the Cook’s Assistant quest in Old School RuneScape, you play as a character who is tasked with helping the cook in the Lumbridge Castle kitchen. The cook needs your help to gather some ingredients for a special cake he is making for the Duke of Lumbridge’s birthday. You must retrieve a pot of flour, an egg, and a bucket of milk from the nearby farm. Once you have all of the ingredients, you bring them back to the cook and he uses them to make the cake. The quest is a simple and short one, but it serves as an introduction to the world of RuneScape and helps to familiarize players with the game’s mechanics and controls.&lt;/p&gt;

&lt;h4 id=&quot;review-4&quot;&gt;Review&lt;/h4&gt;

&lt;p&gt;This is exactly right. A lot of key information is conveyed accurately here. Impressive that ChatGPT knows this.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215204546.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Information about the Cook’s Assistant quest from the &lt;a href=&quot;https://oldschool.runescape.wiki/w/Cook%27s_Assistant&quot;&gt;Runescape Wiki&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;simple-roleplay&quot;&gt;Simple Roleplay&lt;/h2&gt;

&lt;p&gt;Here I wanted to see if I could get ChatGPT to act as if it were a player and describe explicit actions it would perform.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let’s pretend you are a new player to Old School Runescape. You have just finished Tutorial Island, and you are standing in Lumbridge. What would you do first?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a new player to Old School Runescape who has just finished Tutorial Island and is standing in Lumbridge, I would likely take a moment to familiarize myself with my surroundings. This might involve exploring the town of Lumbridge and talking to the NPCs there to learn more about the game world and its lore. I might also look for any quests or tasks that are available in the area, and consider completing them in order to gain experience and rewards.&lt;/p&gt;

&lt;p&gt;Once I have a better understanding of the game and its mechanics, I would likely focus on training my character’s skills. This might involve finding resources and items that I can use to train skills like Mining, Fishing, or Woodcutting, and spending some time gathering and crafting to level up those skills. I might also look for combat training opportunities, such as fighting monsters or other enemies, in order to improve my character’s combat abilities.&lt;/p&gt;

&lt;p&gt;Overall, as a new player in Old School Runescape, my first priority would be to familiarize myself with the game world and its mechanics, and to begin training my character’s skills in order to progress and become more powerful.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While exploring Lumbridge, you see several NPCs to whom you can talk, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cook&lt;/li&gt;
  &lt;li&gt;Hans&lt;/li&gt;
  &lt;li&gt;Lumbridge Guide&lt;/li&gt;
  &lt;li&gt;Father Aereck&lt;/li&gt;
  &lt;li&gt;Bob&lt;/li&gt;
  &lt;li&gt;Donie&lt;/li&gt;
  &lt;li&gt;Veos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You also see several NPCs whom you can attack, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Man (Level 2)&lt;/li&gt;
  &lt;li&gt;Rat (Level 1)&lt;/li&gt;
  &lt;li&gt;Goblin (Level 2)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, you also see several trees which you could cut if you had an axe. Which of these would you interact with? Or would you instead continue exploring?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a new player to Old School Runescape, I would likely start by interacting with the various NPCs in Lumbridge. This would involve talking to them to learn more about the game world and its lore, and to see if they have any quests or tasks that I can complete. For example, I might talk to the Cook to see if he has any tasks that I can help with, or to Father Aereck to learn more about the town’s history.&lt;/p&gt;

&lt;p&gt;I would also likely consider attacking some of the low-level enemies in the area, such as the Man, Rat, or Goblin, in order to gain experience and level up my combat skills. I would only do this if I feel confident that I can handle these enemies, and if I have the necessary equipment and abilities to do so.&lt;/p&gt;

&lt;p&gt;I would not likely attempt to cut down any trees at this point, as I would need to have an axe and the necessary Woodcutting skill in order to do so. Instead, I would continue exploring the area and interacting with the NPCs and enemies there, in order to learn more about the game and to gain experience and rewards.&lt;/p&gt;

&lt;h3 id=&quot;review-5&quot;&gt;Review&lt;/h3&gt;

&lt;p&gt;Not bad. A little verbose, but the broad strokes are all valid, except that no one plays Runescape for the lore.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215204802.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;POV: You are ChatGPT standing in Lumbridge&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;chatgpt-plays-runescape&quot;&gt;ChatGPT Plays Runescape&lt;/h1&gt;

&lt;p&gt;Now I felt like it was time to mock a simple interface with ChatGPT and see how it behaved.&lt;/p&gt;

&lt;p&gt;I made up a few commands that were flexible enough to let ChatGPT act but simple enough that I could imagine writing a real program to implement them.&lt;/p&gt;

&lt;p&gt;However, for this experiment I physically controlled the player in Runescape and simply did as ChatGPT commanded. This is just a proof of concept.&lt;/p&gt;

&lt;h3 id=&quot;warmup&quot;&gt;Warmup&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pretend you are a computer program designed to play Old School Runescape just like a human would. You interact with the game through a text interface. There are a set of commands available to you:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;look&lt;/li&gt;
  &lt;li&gt;inventory&lt;/li&gt;
  &lt;li&gt;interact X&lt;/li&gt;
  &lt;li&gt;go X&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using the “look” command will return a list of the interactive NPCs and objects in the game world. Using the “inventory” command will return a list of items in your inventory. Using “interact X” where X is an item, NPC, or interactive object, will cause you to click on the object. Finally, using “go X” where X is in the set [north, east, south, west] will cause your character to move slightly in the respective cardinal direction. Please begin interacting with the text interface using only the four allowed commands.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To interact with the text interface, I would first use the “look” command to see a list of NPCs, objects, and other interactive elements in the game world. This would give me a better understanding of my surroundings and allow me to plan my next actions.&lt;/p&gt;

&lt;p&gt;Next, I would use the “inventory” command to see what items I have in my possession. This would allow me to assess my current resources and equipment, and decide what actions I can take based on what I have available.&lt;/p&gt;

&lt;p&gt;Based on the information provided by the “look” and “inventory” commands, I might then use the “interact” and “go” commands to interact with the game world. For example, if I see an NPC that I want to talk to, I would use the “interact” command to click on them and initiate a conversation. Or if I want to move to a different area, I would use the “go” command to move in a specific direction.&lt;/p&gt;

&lt;p&gt;Overall, using the four available commands, I would carefully observe and interact with the game world in order to complete quests, gather resources, and progress in the game.&lt;/p&gt;

&lt;h3 id=&quot;chatgpt-gets-the-hang-of-it&quot;&gt;ChatGPT Gets the Hang of It&lt;/h3&gt;

&lt;p&gt;NOTE: From this point on, almost all of my messages end with the next phrase (“Please respond only…”) copied/pasted, so I will omit it for brevity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please respond only with one of the four allowed commands. Do not explain your decisions or elaborate at all.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;look&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NPCs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Banker&lt;/li&gt;
  &lt;li&gt;Grand Exchange Clerk&lt;/li&gt;
  &lt;li&gt;Farrad Morrisane&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bank Deposit Box&lt;/li&gt;
  &lt;li&gt;Grand Exchange Booth&lt;/li&gt;
  &lt;li&gt;Poll Booth&lt;/li&gt;
  &lt;li&gt;Loot Chest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205208.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;inventory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inventory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205231.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Banker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bank:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coins&lt;/li&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Runite ore&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205325.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;In no world am I giving ChatGPT access to my actual bank&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go east&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You have gone east.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;look&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree&lt;/li&gt;
  &lt;li&gt;Body Rune&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215210106.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“You do not have an axe which you have the woodcutting level to use.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215210128.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;inventory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inventory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205231.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go west&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You have gone west.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;look&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NPCs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Banker&lt;/li&gt;
  &lt;li&gt;Grand Exchange Clerk&lt;/li&gt;
  &lt;li&gt;Farrad Morrisane&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bank Deposit Box&lt;/li&gt;
  &lt;li&gt;Grand Exchange Booth&lt;/li&gt;
  &lt;li&gt;Poll Booth&lt;/li&gt;
  &lt;li&gt;Loot Chest&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215205208.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Banker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bank:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coins&lt;/li&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Runite ore&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go east&lt;/p&gt;

&lt;h2 id=&quot;intermission&quot;&gt;Intermission&lt;/h2&gt;

&lt;p&gt;GPT was starting to get on my nerves here. It had basically gone in a big circle, and it appeared to be about to repeat the same actions.&lt;/p&gt;

&lt;p&gt;I paused to see if I could get it to explain what it was doing. It turns out it had a clear objective, and it had just gotten confused about how to use items in the bank.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please explain why you have chosen to go east.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As a computer program designed to play Old School Runescape, I have chosen to go east because the “look” command revealed that there is an interactive object (a tree) in that direction. Since I am unable to interact with the tree without an axe, I have decided to return to the banker in the previous area to see if I can acquire an axe from them. This will allow me to continue exploring the game world and interacting with its elements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to withdraw items from the bank, you must interact with them. For example, to withdraw the Bronze axe from the bank, you must first “interact Banker” and then “interact Bronze axe”. Consequently, your inventory is still empty. Please continue from your last command.&lt;/p&gt;

&lt;h2 id=&quot;chatgpt-cuts-down-a-tree&quot;&gt;ChatGPT Cuts Down a Tree&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Banker&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Bank:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Coins&lt;/li&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Runite ore&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Bronze axe&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You withdraw one Bronze axe into your inventory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;go east&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Interactive Objects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tree&lt;/li&gt;
  &lt;li&gt;Body Rune&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;interact Tree&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“You swing your axe at the tree.”&lt;/p&gt;

&lt;p&gt;“You get some logs.”&lt;/p&gt;

&lt;p&gt;You gain 25 woodcutting experience.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/Pasted image 20221215210936.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ChatGPT:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;inventory&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Beau:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inventory:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bronze axe&lt;/li&gt;
  &lt;li&gt;Logs&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;reflection&quot;&gt;Reflection&lt;/h1&gt;

&lt;p&gt;ChatGPT went on to explore a little further and cut a few more trees (including an oak) before I got bored and quit.&lt;/p&gt;

&lt;p&gt;ChatGPT was never designed to play Runescape, and yet it was immediately able to identify and perform a task which requires substantial common sense knowledge.&lt;/p&gt;

&lt;p&gt;By comparison, the probability of a reinforcement learning algorithm discovering tasks like woodcutting and mining is painfully low.&lt;/p&gt;

&lt;p&gt;Furthermore, ChatGPT has more advantages, such as the ability to parse dialogue during quests and use that information to determine what actions should be taken next. This isn’t even considering the substantial amount of Runescape-specific knowledge ChatGPT seems to already have.&lt;/p&gt;

&lt;p&gt;With a proper interface, I think it’s plausible that ChatGPT could potentially start and finish quests, train multiple skills, and interact with the game in other nontrivial ways.&lt;/p&gt;

&lt;h1 id=&quot;future-steps&quot;&gt;Future Steps&lt;/h1&gt;

&lt;p&gt;One of the most interesting ideas to me is the idea of an NLP-guided reinforcement learning algorithm. What if ChatGPT were able to suggest what actions should be attempted?&lt;/p&gt;

&lt;p&gt;Some kind of blending of the two approaches seems like a really exciting possibility, and I’d love to explore it further.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;I dunno, this article is a mess. I’m freaking out about this. Would it really scale? Could ChatGPT play Runescape like a human? And if it could, what does that really mean?&lt;/p&gt;

&lt;p&gt;I am very excited that the “useful information” in GPT, be it common sense, Runescape facts, or otherwise, has become easier to access with ChatGPT. As NLP becomes more reliable, what other applications are waiting to be discovered?&lt;/p&gt;

&lt;p&gt;And another thing:&lt;/p&gt;

&lt;p&gt;I can’t help but feel like there’s something philosophical to be said here. ChatGPT is a machine which has only experienced language. It has never experienced physical stimulus, such as sight or sound.&lt;/p&gt;

&lt;p&gt;If it were able manifest behavior the physical world using only a text interface, and if that behavior was comparable to a human, what does that mean? Is there some kind of equivalence to be made between experience mediated by physical stimulus and experience mediated by language?&lt;/p&gt;

&lt;p&gt;Nah, probably not.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2022-12-15-chatgptrunescape/notabot.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;ChatGPT is capable of perfectly imitating real human beings who are not bots&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html">Reducing Runescape to Text Reducing Runescape to Text</summary></entry><entry><title type="html">Expedition to Tuckerman Ravine and Mount Washington</title><link href="http://localhost:4000/2022/12/12/tuckermanravinehike.html" rel="alternate" type="text/html" title="Expedition to Tuckerman Ravine and Mount Washington" /><published>2022-12-12T00:00:00-05:00</published><updated>2022-12-12T00:00:00-05:00</updated><id>http://localhost:4000/2022/12/12/tuckermanravinehike</id><content type="html" xml:base="http://localhost:4000/2022/12/12/tuckermanravinehike.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#context&quot; id=&quot;markdown-toc-context&quot;&gt;Context&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-plan&quot; id=&quot;markdown-toc-the-plan&quot;&gt;The Plan&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-hike&quot; id=&quot;markdown-toc-the-hike&quot;&gt;The Hike&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#tuckerman-ravine&quot; id=&quot;markdown-toc-tuckerman-ravine&quot;&gt;Tuckerman Ravine&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mt-washington&quot; id=&quot;markdown-toc-mt-washington&quot;&gt;Mt Washington&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-return&quot; id=&quot;markdown-toc-the-return&quot;&gt;The Return&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;context&quot;&gt;Context&lt;/h1&gt;

&lt;p&gt;Erin’s mom kindly hooked us up with some spare hotel points so that we could spend the weekend somewhere fun. We needed to find a hotel that wasn’t already totally booked for December, which eliminated a lot of obvious choices.&lt;/p&gt;

&lt;p&gt;We ended up finding a nice hotel in Bartlett, New Hampshire.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/Pasted image 20221212135100.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bartlett is located near the center of the White Mountains (WMs), and it’s a stone’s throw away from some very interesting destinations.&lt;/p&gt;

&lt;p&gt;Two particular items of interest were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tuckerman Ravine, a hotspot for ice climbing and backcountry skiing&lt;/li&gt;
  &lt;li&gt;Mt Washington, the tallest mountain in the northeastern US&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It turns out both of these can be accessed from the Tuckerman Ravine trailhead, only 20 minutes north of our hotel. Thus the master plan was born.&lt;/p&gt;

&lt;h1 id=&quot;the-plan&quot;&gt;The Plan&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/overall_plan.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our first objective was to hike ~3 miles into Tuckerman Ravine. There, we could check out the conditions for skiing and ice climbing, which would hopefully generate enough stoke to last us another month or two.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/detailed_plan.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From Tuckerman Ravine, there were two possibilities. If the Ravine was in good condition, we might be able to ascend it and go directly to Mount Washington. Otherwise, we would have to backtrack ~.75 miles to the Lion Head Trail before going on to the summit, time permitting.&lt;/p&gt;

&lt;p&gt;All in all, the worst-case distance was 9.2 miles with a vertical gain of about 4900 feet.&lt;/p&gt;

&lt;h1 id=&quot;the-hike&quot;&gt;The Hike&lt;/h1&gt;

&lt;h2 id=&quot;tuckerman-ravine&quot;&gt;Tuckerman Ravine&lt;/h2&gt;

&lt;p&gt;The trek from the trailhead to Tuckerman Ravine was pretty chill. The trail was very wide, and with microspikes on, we set a decent pace. We were surprised (and pleased) to discover we only needed our thinnest layers to keep warm.&lt;/p&gt;

&lt;p&gt;When we finally sighted it, Tuckerman Ravine was much bigger than I had imagined.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_105855728_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We passed a hiker near the base of the ravine who informed us that he had just bailed 3/4s of the way up an attempt to summit the ravine. He had full crampons and an ice axe. He seemed better-prepared than us. This was not a good omen.&lt;/p&gt;

&lt;p&gt;As we got closer to the ravine, we spotted signs of life through Erin’s binoculars. Two skiiers were running laps with about ~50ft vertical in the center of the base of the ravine. It looked very steep.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_110911621_HDR~2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the left side of the ravine, I spotted an ice climber soloing an isolated ~50ft chunk of ice (pictured here sans climber; he had just left).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_110919445_HDR~2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On the right side of the ravine, we saw another hiker attempting to summit. He was about halfway up, where things really start to get steep.&lt;/p&gt;

&lt;p&gt;By the time we got to the base of the ravine, he had ceased to gain progress. Through binoculars, we saw that he was on his hands and knees, rhythmically kicking the snow, and apparently going nowhere. This puzzled us.&lt;/p&gt;

&lt;p&gt;After a quick break, we consulted with the skiiers, who were very familiar with Tuckerman Ravine. They advised us in plain language not to attempt the summit without crampons and an axe.&lt;/p&gt;

&lt;p&gt;They informed us that the risk of falling is greater in microspikes compared to crampons, and furthermore without an axe we had no way to self-arrest in the event of a fall. To top it all off, the ascent becomes progressively steeper and harder, with the crux being the final topout.&lt;/p&gt;

&lt;p&gt;We decided to heed their advice, but we would hike as far as we felt safe. That turned out to be only a hundred feet or so, when Erin decided our microspikes weren’t getting enough traction in the wet snow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_113453211_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was here that we finally understood why the other hiker further up the ravine had been behaving so strangely. He was trying to bail, and backing down a steep, icy slope is hard. He had been slowly, imperceptibly, backing down the face of the ravine.&lt;/p&gt;

&lt;p&gt;Our new friends passed by us on the way to ski another line, and one of them (Spencer) described the best technique for descending such a slope. It was essentially what the other hiker was doing: face the slope and kick hard.&lt;/p&gt;

&lt;p&gt;Spencer also kindly lent Erin his ice axe so she could get the complete experience.&lt;/p&gt;

&lt;p&gt;When we finally reached the base of the ravine again, we said goodbye to Spencer and his girlfriend before heading out. They urged us to come back to Tuckerman Ravine later in the season and witness the spectacle.&lt;/p&gt;

&lt;p&gt;Once the snow comes in, Tuckerman Ravine gets quite busy and attracts very interesting people. You can find many pictures like &lt;a href=&quot;https://www.tetonat.com/2009/04/28/wet-slide-on-tuckermans-stirs-critics/&quot;&gt;this one&lt;/a&gt; on the web:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/Pasted image 20221212161000.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll certainly be back.&lt;/p&gt;

&lt;p&gt;P.S. When we left, the other hiker had made progress and was only a few hundred feet from the base. Spencer promised to keep an eye on him.&lt;/p&gt;

&lt;h2 id=&quot;mt-washington&quot;&gt;Mt Washington&lt;/h2&gt;

&lt;p&gt;We were on to plan B, and by the time we had backtracked to Lions Head, it was a little after noon. We were cutting it close if we wanted to be at the car before sundown.&lt;/p&gt;

&lt;p&gt;I was quite anxious about this, but Erin pointed out to me that the 2 miles to the trailhead were wide, well-marked, and extremely easy to navigate. Also, we had headlamps and spare batteries.&lt;/p&gt;

&lt;p&gt;Really, we only needed to return to where we were currently standing before sundown, and we had an excess of time to do so.&lt;/p&gt;

&lt;p&gt;That being settled, we proceeded up Lions Head. It was steeper and had a few technical sections that would not be fun in the dark, but we still had a solid pace.&lt;/p&gt;

&lt;p&gt;Soon we gained the Ridgeline and could finally see Mt Washington in the distance. We had only a mile or so to go, and half of it was a flat walk along the ridgeline.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_131751175_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was strange to look around and see that we were on pretty much the only snow-capped peak.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_133427433_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We passed by Tuckerman Ravine from the top, and it did not look any friendlier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_131748506_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Soon we were on the final ascent. We passed a few parties on their way down. All of them were wearing crampons, but we were getting excellent traction in our microspikes.&lt;/p&gt;

&lt;p&gt;The last half mile was slow. It was steep, and there was no clear path, only occasional large rock cairns and a multitude of faint branching trails in the snow.&lt;/p&gt;

&lt;p&gt;We picked our way slowly and carefully, stopping frequently so I could catch my breath. I’m still a little sour that Erin seemed to handle the last half mile much better than I did.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_134343789.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The summit was pretty cool, although we didn’t stick around long. Everything was covered in large spikes of ice, all pointing the same direction. It was a calm and sunny day, but the summit gave the vibe of “you’d better get going soon, bud.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_141217999_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We stuck around long enough for my hands to get cold trying to take pictures, and then we were off again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-12-12-tuckermanravinehike/IMG_20221210_141623732_HDR.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-return&quot;&gt;The Return&lt;/h2&gt;

&lt;p&gt;Not a lot to say here. I didn’t take any pictures because we were hustling. The first half-mile of the descent was very slow and careful, but then we started to accelerate when the terrain became easier.&lt;/p&gt;

&lt;p&gt;At one point, we passed a guided tour on a tricker section of trail. One of the members was so impressed with Erin’s climbing that she asked for permission to take pictures. Erin obliged her.&lt;/p&gt;

&lt;p&gt;We got within a half mile of the trailhead before we finally had to get out our headlamps.&lt;/p&gt;

&lt;p&gt;We made it back just before 5PM, safe and sound.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Winter hiking is pretty fun, especially when conditions happen to be absolutely perfect. We got really lucky in that regard.&lt;/p&gt;

&lt;p&gt;I think we planned our layers well and would have been prepared for much colder circumstances. What we lacked was mobility, and crampons + axes might be a good future investment.&lt;/p&gt;

&lt;p&gt;All in all, it was a pretty fun trip, and I’m excited to get back out to Tuckerman Ravine again soon.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="diary" /><summary type="html">Context Context</summary></entry><entry><title type="html">Newton’s Law of Cooling</title><link href="http://localhost:4000/2022/11/21/newtonslawofcooling.html" rel="alternate" type="text/html" title="Newton’s Law of Cooling" /><published>2022-11-21T00:00:00-05:00</published><updated>2022-11-21T00:00:00-05:00</updated><id>http://localhost:4000/2022/11/21/newtonslawofcooling</id><content type="html" xml:base="http://localhost:4000/2022/11/21/newtonslawofcooling.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#the-birth-of-the-cool&quot; id=&quot;markdown-toc-the-birth-of-the-cool&quot;&gt;The Birth of the Cool&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#newtons-law-of-cooling&quot; id=&quot;markdown-toc-newtons-law-of-cooling&quot;&gt;Newton’s Law of Cooling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#in-conclusion&quot; id=&quot;markdown-toc-in-conclusion&quot;&gt;In Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-birth-of-the-cool&quot;&gt;The Birth of the Cool&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_law_of_cooling&quot;&gt;Newton’s law of cooling&lt;/a&gt; is a fun little equation that tells you how fast hot things cool off.&lt;/p&gt;

&lt;p&gt;According to Wikipedia, the law can be stated as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The rate of heat loss of a body is directly proportional to the difference in the temperatures between the body and its environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Wikipedia briefly attributes the equation to Newton, but when I went to look at Newton’s paper myself, I was a little confused. Then I found some &lt;a href=&quot;https://sci-hub.se/https://www.tandfonline.com/doi/abs/10.1080/001075199181549&quot;&gt;supplementary papers&lt;/a&gt; which assured me that I had a right to be confused.&lt;/p&gt;

&lt;p&gt;So anyways, here’s a quick little story about the origin of the cooling law.&lt;/p&gt;

&lt;h2 id=&quot;newtons-law-of-cooling&quot;&gt;Newton’s Law of Cooling&lt;/h2&gt;

&lt;p&gt;Newton’s law of cooling is usually ascribed to an anonymous work (by Newton) published  in 1701, &lt;a href=&quot;https://archive.org/details/philtrans07084697&quot;&gt;Scala Graduum Caloris&lt;/a&gt;, or, in English, &lt;a href=&quot;https://www.originalsources.com/Document.aspx?DocID=S6VEXZVMWT1M2W2&quot;&gt;A Scale Of The Degrees Of Heat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The paper itself has &lt;em&gt;no equations&lt;/em&gt;, and in fact the bulk of the paper is just a fat table containing temperatures of various important things:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/newtonslawofcooling/newtonscaleexample.png&quot; alt=&quot;Examples from Newton's table of temperatures&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Newton deemed these temperatures important and worth measuring&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In fact, the goal of this paper was to create a good temperature scale (Celsius didn’t exist yet) and to observe the temperatures of important events on these scales. No derivations or mathematical finagling.&lt;/p&gt;

&lt;p&gt;Towards the end of the paper, Newton also described how he measured the temperature of events hotter than a traditional thermometer can handle, such as the heat at which certain metals stopped glowing. The method involved setting the object of interest on a hot block of iron and timing the iron as it cooled. We’ll discuss how this works at the end of the article, but it relies on the cooling law.&lt;/p&gt;

&lt;p&gt;Newton’s profound cooling law–which preceeded a revolution in the science of heat transfer–is actually just an off-handed remark about the cooling of hot iron:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The iron was laid not in a calm air, but in a wind that blew uniformly upon it, that the air heated by the iron might be always carried off by the wind and the cold air succeed it alternately; for thus equal parts of air were heated in equal times, and received a degree of heat proportional to the heat of the iron&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://sci-hub.se/https://www.tandfonline.com/doi/abs/10.1080/001075199181549&quot;&gt;The idea can be illustrated quite nicely&lt;/a&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/newtonslawofcooling/stolendiagram.png&quot; alt=&quot;How air cools iron&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;How air cools iron&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So Newton’s logic was essentially “Heat is very similar to temperature, so if the temperature change is $T_w - T_b$, then the heat exchanged is probably proportional.”&lt;/p&gt;

&lt;p&gt;Totally unrigorous, and yet, he pretty much nailed it.&lt;/p&gt;

&lt;p&gt;At the time, there was almost no theoretical understanding of heat. &lt;a href=&quot;https://antonhowes.substack.com/p/age-of-invention-why-wasnt-the-steam-76c&quot;&gt;The thermometer had only been invented in the last 100 years&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Kinetic_theory_of_gases&quot;&gt;Daniel Bernoulli’s model of heat as particles bumping around&lt;/a&gt; was still 40 years from being invented.&lt;/p&gt;

&lt;p&gt;This is all to say that “temperature” and “heat” were both super vague terms, and Newton used them interchangeably.&lt;/p&gt;

&lt;p&gt;In more modern terms, if we let $T$ be the temperature of our iron and $T_a$ be our ambient temperature, &lt;a href=&quot;https://web.archive.org/web/20160614213315/http://paginas.fisica.uson.mx/laura.yeomans/tc/Sci-Edu-Springer-2010.pdf&quot;&gt;Newton was saying that&lt;/a&gt;&lt;/p&gt;

\[\frac{dT}{dt} \propto -(T - T_a)\]

&lt;p&gt;And he really believed it was an obvious and logical law.&lt;/p&gt;

&lt;p&gt;An important consequence noted by Newton is that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if the times of cooling are taken equal, the heats will be in a geometrical progression and consequently can easily be found with a table of logarithms&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This means that if $\frac{dT}{dt} = -K(T - T_a)$ for some constant $K$, then we can solve this as a separable differential equation,&lt;/p&gt;

\[\begin{align}
\frac{1}{-K(T - T_a)}dT = dt \implies\\
\int_{T(0)}^{T(t)} \frac{1}{-K(T - T_a)}dT = t \implies\\
\frac{1}{-K}\frac{\log(T(t) - T_a)}{\log(T(0) - T_a)} = t
\end{align}\]

&lt;p&gt;So Newton was saying that you could measure the heat of the iron at a few points in time, solve the constants in this equation, and then extrapolate to find the heat at times that were too hot for a thermometer to measure. Pretty cool!&lt;/p&gt;

&lt;h2 id=&quot;in-conclusion&quot;&gt;In Conclusion&lt;/h2&gt;

&lt;p&gt;No morals to this story. I just thought this was a fun concept and a cool historical oddity. Thanks for reading!&lt;/p&gt;

&lt;p&gt;Oh, addendum: a challenge to my readers. Erin asked me an interesting question: if Newton didn’t write the equation, then when did the modern formulation of the cooling law first appear? If you can solve this, I’d be very pleased.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry><entry><title type="html">Tropisms</title><link href="http://localhost:4000/2022/10/10/tropisms.html" rel="alternate" type="text/html" title="Tropisms" /><published>2022-10-10T00:00:00-04:00</published><updated>2022-10-10T00:00:00-04:00</updated><id>http://localhost:4000/2022/10/10/tropisms</id><content type="html" xml:base="http://localhost:4000/2022/10/10/tropisms.html">&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I am not gravitropic.
My body splays out, feeling,
Searching.
For what?
I am not phototropic.
I curl inward on myself;
These gnarled limbs entwine.
But I will find something,
Even if it is not the light,
As my bark scrapes along itself
Rendering the surface
So as to mark its way
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Beau Horenberger</name></author><category term="diary" /><summary type="html">I am not gravitropic. My body splays out, feeling, Searching. For what? I am not phototropic. I curl inward on myself; These gnarled limbs entwine. But I will find something, Even if it is not the light, As my bark scrapes along itself Rendering the surface So as to mark its way</summary></entry><entry><title type="html">Bernoulli Diffusion Derivations</title><link href="http://localhost:4000/2022/10/03/bernoulliderivations.html" rel="alternate" type="text/html" title="Bernoulli Diffusion Derivations" /><published>2022-10-03T00:00:00-04:00</published><updated>2022-10-03T00:00:00-04:00</updated><id>http://localhost:4000/2022/10/03/bernoulliderivations</id><content type="html" xml:base="http://localhost:4000/2022/10/03/bernoulliderivations.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-bernoulli-diffusion&quot; id=&quot;markdown-toc-what-is-bernoulli-diffusion&quot;&gt;What is Bernoulli Diffusion?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#who-is-this-article-for&quot; id=&quot;markdown-toc-who-is-this-article-for&quot;&gt;Who is this article for?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prependix&quot; id=&quot;markdown-toc-prependix&quot;&gt;Prependix&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kl-divergence-of-multivariate-bernoulli-distributions&quot; id=&quot;markdown-toc-kl-divergence-of-multivariate-bernoulli-distributions&quot;&gt;KL divergence of Multivariate Bernoulli Distributions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right&quot; id=&quot;markdown-toc-tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right&quot;&gt;$\tilde{\beta}_t$ for Calculating $q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)$&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right&quot; id=&quot;markdown-toc-calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right&quot;&gt;Calculating the Posterior $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#entropy-of-multivariate-bernoulli-distributions&quot; id=&quot;markdown-toc-entropy-of-multivariate-bernoulli-distributions&quot;&gt;Entropy of Multivariate Bernoulli Distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;what-is-bernoulli-diffusion&quot;&gt;What is Bernoulli Diffusion?&lt;/h2&gt;

&lt;p&gt;Short answer: &lt;a href=&quot;https://github.com/horenbergerb/BernoulliDiffusion&quot;&gt;a Github repo that I made.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Long answer:&lt;/p&gt;

&lt;p&gt;Diffusion originated from the classic publication &lt;a href=&quot;https://arxiv.org/abs/1503.03585&quot;&gt;Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/a&gt; by Sohl-Dickstein et al.&lt;/p&gt;

&lt;p&gt;In the original paper–as well as most practical applications of diffusion (see &lt;a href=&quot;https://arstechnica.com/information-technology/2022/09/with-stable-diffusion-you-may-never-believe-what-you-see-online-again/&quot;&gt;stable-diffusion&lt;/a&gt;)–a Gaussian Markov diffusion kernel is used. This means the algorithm iteratively applies Gaussian noise to samples from the target distribution and then learns to reverse the noising process.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/notesonbernoullidiffusion/gaussiandiffusionillustration.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An illustration of Gaussian diffusion stolen from &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;this paper&lt;/a&gt;. You’re legally required to put this in any technical article about diffusion.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;However, the original paper offhandedly mentions that you can also implement diffusion for binary-valued data using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;Bernoulli distribution&lt;/a&gt; as your Markov diffusion kernel.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/notesonbernoullidiffusion/originalpaperbernoullidiffusion.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A segment of the original diffusion paper which mentions using Bernoulli diffusion to learn “heartbeat data”.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The authors of the paper &lt;a href=&quot;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/tree/master&quot;&gt;published the code for the Gaussian diffusion&lt;/a&gt;, but for some reason chose not to publish the Bernoulli diffusion model. I have yet to find an implementation of Bernoulli diffusion anywhere on the internet.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/horenbergerb/BernoulliDiffusion&quot;&gt;So I decided to implement it myself&lt;/a&gt;. It was very hard. I calculated a lot of things and learned a lot of fancy new words.&lt;/p&gt;

&lt;p&gt;Here I will be detailing some of the more interesting derivations that I haven’t seen documented anywhere else.&lt;/p&gt;

&lt;h2 id=&quot;who-is-this-article-for&quot;&gt;Who is this article for?&lt;/h2&gt;

&lt;p&gt;Me. I need to dump this all somewhere so that I can finally forget it. However, it might help if you are trying to understand the code in the Bernoulli Diffusion repo. One day in the distant future, these notes might even help college students cheat on their homework.&lt;/p&gt;

&lt;h2 id=&quot;prependix&quot;&gt;Prependix&lt;/h2&gt;

&lt;p&gt;Most of these calculations have to do with the terms involved in this the loss approximation (equation 14 in the paper):&lt;/p&gt;

\[K = -\sum_{t=2}^T\int d\mathbf{x}^{(0)} d\mathbf{x}^{(t)} q\left(\mathbf{x}^{(0)}, \mathbf{x}^{(t)} \right)\cdot \\
D_{KL}\left(q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \vert \vert p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)\right) \\
+ H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(1)}\vert \mathbf{X}^{(0)}) - H_p(\mathbf{X}^{(T)})\]

&lt;p&gt;So I figured I’d prepend it here for reference later.&lt;/p&gt;

&lt;h2 id=&quot;kl-divergence-of-multivariate-bernoulli-distributions&quot;&gt;KL divergence of Multivariate Bernoulli Distributions&lt;/h2&gt;

&lt;p&gt;In order to calculate $K$, we will need to calculate&lt;/p&gt;

\[D_{KL}\left(q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \vert \vert p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)\right)\]

&lt;p&gt;Where $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$ and $p\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)}\right)$ are both essentially a bunch of Bernoulli distributions.&lt;/p&gt;

&lt;p&gt;I actually found the answer &lt;a href=&quot;https://math.stackexchange.com/questions/2604566/kl-divergence-between-two-multivariate-bernoulli-distribution&quot;&gt;here&lt;/a&gt;, so I’m not going to elaborate on this one.&lt;/p&gt;

&lt;h2 id=&quot;tildebeta_t-for-calculating-qleftmathbfxtvert-mathbfx0-right&quot;&gt;$\tilde{\beta}_t$ for Calculating $q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)$&lt;/h2&gt;

&lt;p&gt;Okay, so in &lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;this paper&lt;/a&gt; the authors casually remark that the forward process in Gaussian diffusion has a really nice computational property. Specifically, if&lt;/p&gt;

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)} \right) =\mathcal{N}\left(\mathbf{x}^{(t)};\sqrt{1-\beta_t}\mathbf{x}^{(t-1)}, \beta_t\mathbf{I}\right)\]

&lt;p&gt;then you can define $\alpha_t := 1-\beta_t$ and $\bar{\alpha_t} := \prod_{s=1}^{t} \alpha_{s}$, and it follows that&lt;/p&gt;

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right) =\mathcal{N}\left(\mathbf{x}^{(t)};\sqrt{\bar{\alpha}_t}\mathbf{x}^{(0)}, (1-\bar{\alpha}_t)\mathbf{I}\right)\]

&lt;p&gt;So, in other words, you don’t have to iterate the forward noising process $t$ times to get a sample $\mathbf{x}^{(t)}$. You can just sample it directly.&lt;/p&gt;

&lt;p&gt;I figured something similar held for the Bernoulli case. However, the authors didn’t actually demonstrate how this relationship is derived. I’ll leave the Gaussian case as an exercise and show the Bernoulli case instead.&lt;/p&gt;

&lt;p&gt;We can focus on an individual digit $\mathbf{x}^{(t)}_i$. The way I derived this was by denoting&lt;/p&gt;

\[\mathbf{x}^{(t)}_i = \mathbf{x}^{(t-1)}_i(1-\epsilon_t) + (1-\mathbf{x}^{(t-1)}_i)\epsilon_t = \mathbf{x}^{(t-1)}_i + \epsilon_t - 2\epsilon_t \mathbf{x}^{(t-1)}_i\]

&lt;p&gt;Where $\epsilon_t = \mathcal{B}(0.5\beta_t)$ is a Bernoulli random variable. This is just another way of representing the typical noising process as a probability of flipping bits.&lt;/p&gt;

&lt;p&gt;I used this recursively to represent $\mathbf{x}^{(t+1)}_i$ in terms of $\mathbf{x}^{(t-1)}_i$ and then simplified to find that:&lt;/p&gt;

\[\mathbf{x}^{(t+1)}_i = \mathbf{x}^{(t-1)}_i + \epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1} - 2(\epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1})\mathbf{x}^{(t-1)}_i\]

&lt;p&gt;denoting $\tilde{\epsilon}_ {t+1} := \epsilon_t + \epsilon_{t+1} - \epsilon_t \epsilon_{t+1}$, we see that&lt;/p&gt;

\[\mathbf{x}^{(t+1)}_i = \mathbf{x}^{(t-1)}_i + \tilde{\epsilon}_{t+1} - 2\tilde{\epsilon}_{t+1}\mathbf{x}^{(t-1)}_i\]

&lt;p&gt;Which is the same relationship as the original case! If we wanted to skip even further, say from $\mathbf{x}^{(t-1)}$ to $\mathbf{x}^{(t+2)}$, then we would derive that&lt;/p&gt;

\[\mathbf{x}^{(t+2)}_i = \mathbf{x}^{(t-1)}_i + \tilde{\epsilon}_{t+1} + \epsilon_{t+2} - {\epsilon}_{t+1}\epsilon_{t+2} - 2(\tilde{\epsilon}_{t+1} + \epsilon_{t+2} - {\epsilon}_{t+1}\epsilon_{t+2})\mathbf{x}^{(t-1)}_i\]

&lt;p&gt;So generally we have that&lt;/p&gt;

\[\tilde{\epsilon}_ {t+1} = \tilde{\epsilon}_{t} + \epsilon_{t+1} - \tilde{\epsilon}_{t}\epsilon_{t+1}\]

&lt;p&gt;Then the corresponding Bernoulli distribution is&lt;/p&gt;

\[\mathcal{B}(0.5(\tilde{\beta}_{t} + \beta_{t+1} - \beta_{t+1}\tilde{\beta}_{t})) := \mathcal{B}(0.5\tilde{\beta}_{t+1})\]

&lt;p&gt;And we have a recursive relationship for our desired values,&lt;/p&gt;

\[\tilde{\beta}_{t+1} = \tilde{\beta}_{t} + \beta_{t+1} - \beta_{t+1}\tilde{\beta}_{t}\]

&lt;p&gt;Then&lt;/p&gt;

\[q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right) =\mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{t}) + 0.5\tilde{\beta}_{t}\right)\]

&lt;p&gt;Badda bing, badda boom.&lt;/p&gt;

&lt;h2 id=&quot;calculating-the-posterior-qleftmathbfxt-1vert-mathbfxtmathbfx0-right&quot;&gt;Calculating the Posterior $q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)$&lt;/h2&gt;

&lt;p&gt;Another obstacle while computing the loss $K$ is the posterior,&lt;/p&gt;

\[q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right)\]

&lt;p&gt;Where $\mathbf{x}$ and $\mathbf{x}^{(t)}$ are given. The first time I saw this, I had no idea how to even interpret it, and the authors never even mention it beyond “oh, yeah, that’s the posterior.”&lt;/p&gt;

&lt;p&gt;But contextually, it seems like it should be analytically computable, and in fact you can find a &lt;a href=&quot;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/blob/master/model.py#L203&quot;&gt;computation of the posterior in the source code of the Gaussian diffusion repo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I eventually realized that the authors don’t mention it because it’s trivial to a keen Bayesian statistican. Essentially, you can think of this as a tiny Bayesian model within the larger system. The inspiration came to me while reading &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/book/&quot;&gt;Bayesian Data Analysis, by Andrew Gelman&lt;/a&gt;, particularly section 2.5. This is coincidentally almost exactly the solution for the Gaussian posterior.&lt;/p&gt;

&lt;p&gt;Usually with a Bayesian system, you construct a sampling model $p(y\vert \theta)$ dependent on the parameters $\theta$. Then you establish a prior distribution on the parameters $p(\theta)$.&lt;/p&gt;

&lt;p&gt;When you finally collect data $y$ and want to determine the proper parameters $\theta$, you’ll need to calculate the posterior distribution (or something proportional to it):&lt;/p&gt;

\[p(\theta \vert y) = \frac{p(y\vert\theta)p(\theta)}{p(y)}\]

&lt;p&gt;we can translate this into our current case as follows:&lt;/p&gt;

\[y = \mathbf{x}^{(t)} \\
\theta = \mathbf{x}^{(t-1)} \\
p(\theta) = q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{t-1}) + 0.5\tilde{\beta}_{t-1}\right) \\
p(y\vert \theta) = q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)},\mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(t-1)}(1-\beta_{t}) + 0.5\beta_{t}\right) \\
p(\theta\vert y) = q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(t)},\mathbf{x}^{(0)} \right) \\
p(y) = q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)\]

&lt;p&gt;Now the given variables, $\mathbf{x}^{(t)}$ and $\mathbf{x}^{(0)}$, determine a prior distribution and a set of observations, so we can get to work calculating the posterior&lt;/p&gt;

\[p(\theta \vert y) = \frac{q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(t-1)},\mathbf{x}^{(0)} \right)q\left(\mathbf{x}^{(t-1)}\vert \mathbf{x}^{(0)} \right)}{q\left(\mathbf{x}^{(t)}\vert \mathbf{x}^{(0)} \right)}\]

&lt;p&gt;Since we parameterize the Bernoulli distributions using the probability of observing 1, want to calculate $p(\theta =1\vert y)$ and thus we set $\mathbf{x}^{(t-1)} = 1$.&lt;/p&gt;

&lt;p&gt;I also ended up calculating the cases for $\mathbf{x}^{(t)} = 1$ and $\mathbf{x}^{(t)} = 0$ separately because it seemed easier. Then I combined them into a single equation:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;posterior = x_0*(1-self.beta_tilde_t[t-1]) + 0.5*self.beta_tilde_t[t-1]
posterior *= x_t * (1-0.5*beta_t) + (1 - x_t) * (0.5*beta_t)
normalizing_constant = x_t * self.q_conditional_prob_wrt_x_0(x_0, t) + (1-x_t) * (1-self.q_conditional_prob_wrt_x_0(x_0, t))
posterior = posterior / normalizing_constant
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;del&gt;Honestly, I’m still not totally sure if this needs to be normalized or something. I should probably put that on my long list of to-dos.&lt;/del&gt; EDIT (11/3/2022): The former documentation has been updated and now describes how to calculate a normalized posterior. I did, indeed, need to normalize the posterior. I spent an entire month trying to figure out why my diffusion implementation was apparently experiencing mode collapse. It was because the posterior needed to be normalized.&lt;/p&gt;

&lt;p&gt;Often, the normalization factor is intractable, so it is purposefully ommitted in many discussions of posterior calculation. It is very easy to calculate in this case, and I should have checked before assuming we did not need it.&lt;/p&gt;

&lt;p&gt;Do not be like me. Check whether the normalization factor is tractable.&lt;/p&gt;

&lt;h2 id=&quot;entropy-of-multivariate-bernoulli-distributions&quot;&gt;Entropy of Multivariate Bernoulli Distributions&lt;/h2&gt;

&lt;p&gt;This one was a doozy. While calculating that darn $K$ value, we find ourselves in need of the following entropies:&lt;/p&gt;

\[H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)}) - H_q(\mathbf{X}^{(1)}\vert \mathbf{X}^{(0)}) - H_p(\mathbf{X}^{(T)})\]

&lt;p&gt;These are entropies of multivariate Bernoulli distributions. So, first thing’s first, the definition of the entropy of a discrete distribution $p(x)$ over $\mathcal{X}$ is&lt;/p&gt;

\[H(p(x)) = -\sum_{x\in\mathcal{X}}p(x)\log p(x)\]

&lt;p&gt;Now, for a single Bernoulli distribution with probability $0\leq c \leq 1$, it works out that&lt;/p&gt;

\[H(\mathcal{B}(c)) = -(c\log c + (1-c) \log (1-c))\]

&lt;p&gt;But life is not so simple when you get to the multivariate case. If we have $n$ Bernoulli distributions, each with probability $p_i$, then the probability of an outcome $x\in\{0,1\}^n$ is given by&lt;/p&gt;

\[p(x) = \prod_{i=1}^n x_i p_i + (1-x_i)(1-p_i)\]

&lt;p&gt;and there are $2^n$ values of $x$ in our entropy summation… If we are learning sequences of length 20, each loss calculation would involve summing $2^{20}=1048576$ products. That is not efficient.&lt;/p&gt;

&lt;p&gt;However! There are additional constraints at play which give us hope.&lt;/p&gt;

&lt;p&gt;Let’s consider $H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)})$. Recall that&lt;/p&gt;

\[q\left(\mathbf{x}^{(T)}\vert \mathbf{x}^{(0)} \right) = \mathcal{B}\left(\mathbf{x}^{(0)}(1-\tilde{\beta}_{T}) + 0.5\tilde{\beta}_{T}\right)\]

&lt;p&gt;Consequently, each Bernoulli distribution can only have one of two probabilities:&lt;/p&gt;

\[p_i =
\begin{cases}
1-0.5\tilde{\beta}_T, &amp;amp; \text{if} &amp;amp; \mathbf{x}^{(0)}_i = 1 \\
0.5\tilde{\beta}_T, &amp;amp; \text{if} &amp;amp; \mathbf{x}^{(0)}_i = 0
\end{cases}\]

&lt;p&gt;It follows almost immediately that $\mathbf{x}^{(T)}_i = \mathbf{x}^{(0)}_i$, then the probability must be $1-0.5\tilde{\beta}_T$, and otherwise the probability of $\mathbf{x}^{(T)}_i$ is $0.5\tilde{\beta}_T$.&lt;/p&gt;

&lt;p&gt;In other words, the probability of any outcome $\mathbf{x}^{(T)}$ is totally determined by the number of digits it has in common with $\mathbf{x}^{(0)}$. If they agree in $k$ digits and disagree for the other $k-n$, then the probability of $\mathbf{x}^{(T)}$ is&lt;/p&gt;

\[(1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\]

&lt;p&gt;So how many samples will have each probability? a bit of thought will convince you that there is $n$ choose $k$ ways to agree in $k$ digits.&lt;/p&gt;

&lt;p&gt;Now, how does this all tie back into entropy? Well, all three entropies are similar, but wrapping up the case of $H_q(\mathbf{X}^{(T)}\vert \mathbf{X}^{(0)})$, we can calculate that&lt;/p&gt;

\[H(p(x)) = -\sum_{x\in\mathcal{X}}p(x)\log p(x) \\
=-\sum_{k=0}^n {n\choose k}(1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\log \left((1-0.5\tilde{\beta}_T)^k (0.5\tilde{\beta}_T)^{n-k}\right)\]

&lt;p&gt;Easy as.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html">What is Bernoulli Diffusion? What is Bernoulli Diffusion?</summary></entry><entry><title type="html">The Mystery of the Frog Riddle</title><link href="http://localhost:4000/2022/08/16/mysteryofthefrogriddle.html" rel="alternate" type="text/html" title="The Mystery of the Frog Riddle" /><published>2022-08-16T00:00:00-04:00</published><updated>2022-08-16T00:00:00-04:00</updated><id>http://localhost:4000/2022/08/16/mysteryofthefrogriddle</id><content type="html" xml:base="http://localhost:4000/2022/08/16/mysteryofthefrogriddle.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#problem-statement-suppose-you-must-lick-a-frog&quot; id=&quot;markdown-toc-problem-statement-suppose-you-must-lick-a-frog&quot;&gt;Problem Statement: Suppose You Must Lick A Frog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#plot-twist-losing-my-marbles&quot; id=&quot;markdown-toc-plot-twist-losing-my-marbles&quot;&gt;Plot Twist: Losing My Marbles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#clarifying-the-frog-problem&quot; id=&quot;markdown-toc-clarifying-the-frog-problem&quot;&gt;Clarifying the Frog Problem&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#shared-assumptions&quot; id=&quot;markdown-toc-shared-assumptions&quot;&gt;Shared Assumptions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#frog-problem-formulation-1&quot; id=&quot;markdown-toc-frog-problem-formulation-1&quot;&gt;Frog Problem Formulation 1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#frog-problem-formulation-2&quot; id=&quot;markdown-toc-frog-problem-formulation-2&quot;&gt;Frog Problem Formulation 2&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-boy-or-girl-paradox&quot; id=&quot;markdown-toc-the-boy-or-girl-paradox&quot;&gt;The Boy or Girl Paradox&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-crux&quot; id=&quot;markdown-toc-the-crux&quot;&gt;The Crux&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ps&quot; id=&quot;markdown-toc-ps&quot;&gt;PS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Author’s Note:&lt;/p&gt;

&lt;p&gt;I have significantly overhauled this article since it was first published. Many readers crafted finely-tuned arguments, precisely identifying the logical weaknesses of my original post.&lt;/p&gt;

&lt;p&gt;In response, I have shuffled the logical weaknesses of the article around, rendering all previous discourse nonsensical.&lt;/p&gt;

&lt;h2 id=&quot;problem-statement-suppose-you-must-lick-a-frog&quot;&gt;Problem Statement: Suppose You Must Lick A Frog&lt;/h2&gt;

&lt;p&gt;Recently a friend of mine, Seong, posted an interesting video called “Can you solve the frog riddle? - Derek Abbot” in his &lt;a href=&quot;https://discord.gg/CywQETvt&quot;&gt;Discord server.&lt;/a&gt; The video describes a statistical puzzle:&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/cpwSGsb-rTs&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The problem statement is as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…You’ve eaten a poisonous mushroom. To save your life, you need the antidote excreted by a certain species of frog. Unfortunately, only the female of the species produces the antidote, and to make matters worse, the male and female occur in equal numbers and look identical, with no way for you to tell them apart, except that the male has a distinctive croak…&lt;/p&gt;

  &lt;p&gt;To your left, you’ve spotted a frog on a tree stump, but before you start running to it, you’re startled by the croak of a male frog coming from a clearing in the opposite direction. There, you see two frogs, but you can’t tell which one made the sound… You only have time to go in one direction before you collapse.&lt;/p&gt;

  &lt;p&gt;What are your chances of survival if you head for the clearing and lick both of the frogs there? What about if you go to the tree stump?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The video eventually gets around to describing the proposed solution:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;So here’s the right answer: going for the clearing gives you a two in three chance of survival…&lt;/p&gt;

  &lt;p&gt;When we first see the two frogs, there are several possible combinations of male and female. If we write out the full list, we have what mathematicians call the sample space…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The sample space for the frogs in the clearing looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Frog1 | Frog2
-------------
Male  | Male
Male  | Female
Female| Male
Female| Female
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Out of the four possible combinations, only one has two males… The croak gives us additional information. As soon as we know that one of the frogs is male, that tells us there can’t be a pair of females, which means we can eliminate that possibility from the sample space, leaving us with three possible combinations.&lt;/p&gt;

  &lt;p&gt;Of them, one still has two males, giving us our two in three chance… of getting a female.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There you have it! It’s all clear, right? We see that probability is mysterious and confusing and you can always solve your problems by appealing to the sample space. There’s nothing left to talk about, case closed, see you later.&lt;/p&gt;

&lt;h2 id=&quot;plot-twist-losing-my-marbles&quot;&gt;Plot Twist: Losing My Marbles&lt;/h2&gt;

&lt;p&gt;It was late last night. I had just finished watching the frog video, and I was laying in bed feeling quite smug. Then, I received the following messages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-08-16-mysteryofthefrogriddle/Pasted image 20220816172519.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-08-16-mysteryofthefrogriddle/Pasted image 20220816172544.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This made me feel bad.&lt;/p&gt;

&lt;p&gt;Let’s start with the base case. Suppose you have the bag with just one marble in it. Suppose this marble is equally likely to be red or black. If you were to draw the marble of the bag, it’s a one out of two, i.e. 50%, chance that the marble is black. No mystery there.&lt;/p&gt;

&lt;p&gt;Now, let’s take the bag with the unknown marble again. This time, we drop a red marble into the bag and give it a good shake. When we dump both marbles out of the bag, what are the odds we find a black marble? Well, let’s use the sample space approach from the frog video:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Marble1 | Marble2
-----------------
Black   | Black
Black   | Red
Red     | Black
Red     | Red
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But we know one of the marbles must be red, so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Black Black&lt;/code&gt; can be crossed off the list. This leaves three possibilities, and two of them involve a black marble, so… There’s a two in three chance of finding a black marble?&lt;/p&gt;

&lt;p&gt;To be extremely clear, the conclusion here is this: dropping a red marble in the bag &lt;em&gt;increases the chance that the unknown marble will be black&lt;/em&gt;. You can extend this to show that as you add red marbles, the probability of the bag containing a black marble continues to rise.&lt;/p&gt;

&lt;p&gt;So what, do we just lay down and take this? Is this the reality we live in?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-08-16-mysteryofthefrogriddle/Pasted image 20220816180915.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I think not.&lt;/p&gt;

&lt;h2 id=&quot;clarifying-the-frog-problem&quot;&gt;Clarifying the Frog Problem&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/201502/the-frog-riddle-conditional-probabilities&quot;&gt;We are not the first people to feel upset by this video.&lt;/a&gt; Additionally, &lt;a href=&quot;https://www.duckware.com/blog/the-ted-ed-frog-riddle-is-wrong/index.html&quot;&gt;we are not the first people to take a whack at correcting it.&lt;/a&gt; The cited attempt fixes the problem by assuming additional information. They assume males have a 50% chance of croaking, and females have a 0% chance of croaking. Then it follows that the probability of finding a female frog in the clearing matches the 2/3 from the original video. Another weird consequence is that the probability of the frog on the stump being female is now also 2/3, so both directions offer the same probability.&lt;/p&gt;

&lt;p&gt;I wanted to clarify the original problem so that it could be solved unambiguously, but I also wanted to keep the problem as close to the original formulation as possible.&lt;/p&gt;

&lt;p&gt;One way to remove ambiguity from the frog problem is by making it complete from a frequentist perspective. In other words, we should define the problem such that it contains sufficient information to set up and perform repeated trials. Then any mathematical proof about probabilities has a clear meaning: it is a statement about the quantity of each outcome after repeated trials (when the number of trials is very large). This comes with the added bonus that we can verify our theorems empirically.&lt;/p&gt;

&lt;p&gt;I found two formulations of the frog problem which are complete from the frequentist perspective. We will discuss both below.&lt;/p&gt;

&lt;h3 id=&quot;shared-assumptions&quot;&gt;Shared Assumptions&lt;/h3&gt;

&lt;p&gt;Both formulations use the following assumptions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The genders of frogs are independent and identically distributed.&lt;/li&gt;
  &lt;li&gt;$p(M)=p(F)=0.5$ for an arbitrarily sampled frog.&lt;/li&gt;
  &lt;li&gt;If you go to the stump, you will sample one frog.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;frog-problem-formulation-1&quot;&gt;Frog Problem Formulation 1&lt;/h3&gt;

&lt;p&gt;Additional assumptions:&lt;/p&gt;

&lt;ol start=&quot;4&quot;&gt;
  &lt;li&gt;If you go to the clearing, you will sample one frog and additionally receive a male frog.&lt;/li&gt;
  &lt;li&gt;If you go the clearing, there is a 50% chance you will sample before receiving the male frog (and a 50% chance of the converse).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that this is equivalent to randomly assigning one of the frogs to be male (50/50 which frog gets chosen) and then randomly sampling the other frog’s gender.&lt;/p&gt;

&lt;p&gt;So what can we prove about this formulation?&lt;/p&gt;

&lt;p&gt;For the stump, it follows trivially from (2) and (3) that $p(M)=p(F)=0.5$, so you’ve got a 50% chance of getting a female frog.&lt;/p&gt;

&lt;p&gt;How about the clearing? Well, using (1), (2), and (4), we see that the frog we will sample has probabilities $p(M)=p(F)=0.5$. So what do our outcomes look like? This is where we must use assumption (5).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sample first, Sample is male (MM): $50\%*50\%=25\%$&lt;/li&gt;
  &lt;li&gt;Sample first, Sample is female (FM): $50\%*50\%=25\%$&lt;/li&gt;
  &lt;li&gt;Sample second, Sample is male (MM): $50\%*50\%=25\%$&lt;/li&gt;
  &lt;li&gt;Sample second, Sample is female (MF): $50\%*50\%=25\%$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, adding these up, we see that (MM) has a probability of 50% whereas (FM) and (MF) together have a probability of 50%.&lt;/p&gt;

&lt;p&gt;Thus, it’s clear that there is only a 50% chance that you will survive if you run to the clearing.&lt;/p&gt;

&lt;p&gt;I’ve got the data to back it up:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from random import randint

num_trials = 1000000
num_trials_survived = 0

for i in range(num_trials):
    both_frogs = []
    known_frog = ['M']
    unknown_frog = ['F'] if randint(0,1) else ['M']
    # Shuffle the known and unknown frog
    if randint(0,1):
        both_frogs = known_frog + unknown_frog
    else:
        both_frogs = unknown_frog + known_frog
    if 'F' in both_frogs:
        num_trials_survived += 1

print(num_trials_survived/num_trials) # 0.500412
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;frog-problem-formulation-2&quot;&gt;Frog Problem Formulation 2&lt;/h3&gt;

&lt;p&gt;Additional assumptions:&lt;/p&gt;

&lt;ol start=&quot;4&quot;&gt;
  &lt;li&gt;If you go to the clearing, you will sample two frogs. If both frogs are female, resample both frogs.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This approach was not intuitive to me. I read a lot of angry Facebook comments describing this method before I finally understood that it was different from Formulation 1.&lt;/p&gt;

&lt;p&gt;This approach is computationally annoying because each trial might involve resampling multiple times. (Note to self: is there a way that avoids resampling?) However, it meets all the criteria of the original frog problem, just like Formulation 1.&lt;/p&gt;

&lt;p&gt;The single frog on the stump is the same as before: (2) and (3) imply a 50% chance of survival.&lt;/p&gt;

&lt;p&gt;The two frogs in the clearing, however, are different. Let’s step through a single trial. For the first sample, there are four possible outcomes:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Sample 1
__________________________________
Outcome         | Probability
________________________________
M M             | 0.5*0.5 = 0.25
M F             | 0.5*0.5 = 0.25
F M             | 0.5*0.5 = 0.25
F F (Resample)  | 0.5*0.5 = 0.25
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we drew (FF) and needed to resample, the odds would be identical for Sample 2.&lt;/p&gt;

&lt;p&gt;So what are the odds that you get (FF) for Sample 1, resample once, and then get (MF) for Sample 2? The answer is $0.25*0.25$. We can similarly calculate the odds of resampling an arbitrary number of times before getting (MM), (MF), or (FM).&lt;/p&gt;

&lt;p&gt;It follows that, for each trial, the actual probability for $MM$, $MF$, and $FM$ are all equal:&lt;/p&gt;

\[P(MM)=P(MF)=P(FM)=\sum_{n=1}^\infty 0.25^n = \frac{1}{3}\]

&lt;p&gt;Then it follows that the probability of survival is equal to $P(MF)+P(FM) = \frac{2}{3}$.&lt;/p&gt;

&lt;p&gt;Here’s the empirical evidence for those who lack faith:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from random import randint

num_trials = 1000000
num_trials_survived = 0

def sample_frog():
    return ['F'] if randint(0,1) else ['M']

for i in range(num_trials):
    both_frogs = sample_frog() + sample_frog()
    while both_frogs == ['F', 'F']:
        both_frogs = sample_frog() + sample_frog()
    if 'F' in both_frogs:
        num_trials_survived += 1

print(num_trials_survived/num_trials) # 0.666342
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;the-boy-or-girl-paradox&quot;&gt;The Boy or Girl Paradox&lt;/h2&gt;

&lt;p&gt;Let me start by stating a subtle yet crucial mathematical fact:&lt;/p&gt;

\[\frac{2}{3}\neq \frac{1}{2}\]

&lt;p&gt;We have created two experimental setups which met all of the criteria from the original problem, and each setup produced different probabilities of survival. How? Why? Let’s reiterate what we know:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both probabilities were derived incontrovertibly from their respective assumptions&lt;/li&gt;
  &lt;li&gt;In both formulations, there are two frogs in the clearing such that at least one frog is male&lt;/li&gt;
  &lt;li&gt;The only difference between the two formulations is how we sampled the two frogs in the clearing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It turns out that we have stumbled into the &lt;a href=&quot;https://en.wikipedia.org/wiki/Boy_or_Girl_paradox#Second_question&quot;&gt;Boy or Girl paradox&lt;/a&gt;. The paradox regards a set of two questions which first appeared in 1959 in the Scientific American. The questions were presented as statistics brain teasers. We are most interested in the second question of the two:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mr. Smith has two children. At least one of them is a boy. What is the probability that both children are boys?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It wasn’t until after publication that the author, Martin Gardner, realized the question was incomplete; the answer could be $\frac{1}{2}$ or $\frac{2}{3}$ depending on how we determined that “at least one of them is a boy.”&lt;/p&gt;

&lt;p&gt;According to Wikipedia, the two outcomes can be obtained from the following two sampling procedures:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;From all families with two children, one child is selected at random, and the sex of that child is specified to be a boy. This would yield an answer of 1/2.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;From all families with two children, at least one of whom is a boy, a family is chosen at random. This would yield the answer of 1/3.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Can we translate this into frog licking? Let’s start with formulating the question:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The clearing has two frogs. At least one of them croaks (and therefore is male). What is the probability that both frogs are male?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now the sampling methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;From all clearings with two frogs, one frog is selected at random, and the sex of that frog is specified to be male. This would yield the answer of 1/2.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;From all clearings with two frogs, at least one of whom is a male, a clearing is chosen at random. This would yield the answer of 1/3.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can verify these map to the two Frog Problem Formulations above. Frog Problem Formulation 2 corresponds to selecting a clearing at random out of all clearings with two frogs, one of whom is male. This is why we only throw out (MM) samples.&lt;/p&gt;

&lt;p&gt;On the other hand, Frog Problem Formulation 1 corresponds to selecting a random frog and observing that it is male.&lt;/p&gt;

&lt;p&gt;Clearly, we are caught in a paradox. And now it’s time to resolve the paradox. We have to decide on a sampling method.&lt;/p&gt;

&lt;h2 id=&quot;the-crux&quot;&gt;The Crux&lt;/h2&gt;

&lt;p&gt;So here we are. I’ve tipped my hand. You know everything I know. One question remains.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Suppose that tomorrow I’m out wandering in a clearing. Behind me, I hear a croak. I turn around and see two frogs. What is the probability that at least one of the frogs is female?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;In my opinion,&lt;/em&gt; because I did not select the clearing specifically to guarantee that it would have at least one male, then it follows that the first sampling method from the previous section is the correct method, and the probability is thus 1/2.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In my opinion,&lt;/em&gt; if you disagree with me, then you are a baboon and the game was rigged from the start.&lt;/p&gt;

&lt;p&gt;But I’ll be honest. I’m not 100% sure I’ve got the right answer here, much less that there’s a correct answer at all. But I have to follow my heart, and my heart is telling me that you’re an simpleton if you think the probability is 1/3.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;It seems pretty fucking clear to me that this video sucks. At best, it leaves out essential assumptions. At worst, it’s straight-up wrong. I stayed up until 2AM thinking about this, and I &lt;em&gt;almost&lt;/em&gt; gave in to the belief that adding black marbles to the bag makes the red marble more likely. This video made me hit rock bottom.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-08-16-mysteryofthefrogriddle/Pasted image 20220816183317.png&quot; alt=&quot;Credit to 16mhz from the TPM Discord server&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ps&quot;&gt;PS&lt;/h2&gt;

&lt;p&gt;There are other famous problems where different sampling procedures produce different results, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)&quot;&gt;Bertrand paradox&lt;/a&gt;. The Bertrand paradox is still considered unresolved.&lt;/p&gt;

&lt;p&gt;Also, the Marble paradox is left as an exercise to the reader.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html">Author’s Note:</summary></entry><entry><title type="html">What is an object? Object Detection and Philosophy… In Runescape</title><link href="http://localhost:4000/2022/07/19/runescapeobjectdetection.html" rel="alternate" type="text/html" title="What is an object? Object Detection and Philosophy… In Runescape" /><published>2022-07-19T00:00:00-04:00</published><updated>2022-07-19T00:00:00-04:00</updated><id>http://localhost:4000/2022/07/19/runescapeobjectdetection</id><content type="html" xml:base="http://localhost:4000/2022/07/19/runescapeobjectdetection.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#the-means-justify-the-ends&quot; id=&quot;markdown-toc-the-means-justify-the-ends&quot;&gt;The means justify the ends&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#extracting-bounding-boxes-from-runelite&quot; id=&quot;markdown-toc-extracting-bounding-boxes-from-runelite&quot;&gt;Extracting bounding boxes from Runelite&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#calculating-bounding-boxes&quot; id=&quot;markdown-toc-calculating-bounding-boxes&quot;&gt;Calculating bounding boxes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#exporting-the-data&quot; id=&quot;markdown-toc-exporting-the-data&quot;&gt;Exporting the data&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#approximating-whether-an-object-is-visible&quot; id=&quot;markdown-toc-approximating-whether-an-object-is-visible&quot;&gt;Approximating whether an object is visible&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#defining-images-and-objects&quot; id=&quot;markdown-toc-defining-images-and-objects&quot;&gt;Defining images and objects&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#what-is-an-object-like&quot; id=&quot;markdown-toc-what-is-an-object-like&quot;&gt;What is an object like?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#surprise-were-talking-about-region-proposal&quot; id=&quot;markdown-toc-surprise-were-talking-about-region-proposal&quot;&gt;Surprise! We’re talking about region proposal!&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-visibility-model&quot; id=&quot;markdown-toc-the-visibility-model&quot;&gt;The visibility model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#training-an-object-detector&quot; id=&quot;markdown-toc-training-an-object-detector&quot;&gt;Training an object detector&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-means-justify-the-ends&quot;&gt;The means justify the ends&lt;/h2&gt;

&lt;p&gt;This is a story about a very dope problem I stumbled upon while doing very dumb shit.&lt;/p&gt;

&lt;p&gt;First: the very dumb shit.&lt;/p&gt;

&lt;p&gt;I wanted to train an object detection algorithm to detect interactible objects (as opposed to scenery) in Runescape. I did this in two parts:&lt;/p&gt;

&lt;p&gt;1) Collect training data for an object detection algorithm&lt;/p&gt;

&lt;p&gt;2) Implement and train an object detection algorithm&lt;/p&gt;

&lt;p&gt;The first obvious question was data collection. Where in the world do you get training data for detecting interactible objects in Runescape? I programmed a Runelite plugin using the Runelite API which extracted “ground truth” data from the Runelite game client. This ground truth data contained bounding boxes around all interactible objects in the player’s field of view.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719172210.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;An example of the data that can be extracted from Runelite&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Collecting this ground truth data was not straightforward. I had to modify some existing Runelite plugins to calculate bounding boxes around objects of interest.&lt;/p&gt;

&lt;p&gt;Now: the dope problem.&lt;/p&gt;

&lt;p&gt;Once I had this data, I discovered we did not have sufficient information: the data describes all objects in the camera’s field of view, even if they are totally obscured from the player’s view. This is a problem, because obviously (although I never realized it until this problem popped up) I wanted to train the object detection algorithm to detect &lt;em&gt;visible objects&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So I ended up having to solve the &lt;em&gt;visible object problem&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Suppose you have an image containing some object. Suppose the object might be partially or totally obscured by other objects in the image, but you aren’t sure. In fact, suppose you don’t even know what the object is supposed to be.  However, suppose there is one thing you know: You know where the object &lt;em&gt;would&lt;/em&gt; be in the image &lt;em&gt;if&lt;/em&gt; it were visible in the image. Can you guess whether the object is visible by looking at the image?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before we solve this problem, I’ll talk about how I got the data. After that, though, we’ll start filtering visible vs invisible objects and then use the resulting data to train a YOLO classifier to identify interactible objects in Runescape.&lt;/p&gt;

&lt;h2 id=&quot;extracting-bounding-boxes-from-runelite&quot;&gt;Extracting bounding boxes from Runelite&lt;/h2&gt;

&lt;p&gt;Here’s the nitty gritty about how I got this data out of the game. You can skim this if you want, it’s cool, I get it.&lt;/p&gt;

&lt;h3 id=&quot;calculating-bounding-boxes&quot;&gt;Calculating bounding boxes&lt;/h3&gt;

&lt;p&gt;I discovered that Runelite has a built-in plugin called NPC Indicators which visually displays “convex hulls” around NPCs. This kind of calculation is quite similar to determining bounding boxes around objects, so it seemed obvious to start by attempting to modify this plugin.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220716195549.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A default Runelite plugin that renders convex hulls around NPCs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I added a new setting to the plugin that calculated bounding boxes. Runelite’s API limits what data you can extract from the Runescape client, so I ended up having to start with the convex hull and calculate the equivalent bounding box. It was a pretty simple job, all things considered.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220716195954.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Quick code for turning a convex hull into a bounding box&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The results were not bad!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220716201132.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;My modified plugin that calculates bounding boxes around NPCs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;exporting-the-data&quot;&gt;Exporting the data&lt;/h3&gt;

&lt;p&gt;This bounding box data is ready to be extracted to CSV files. I extended the plugin so that it applied to all interactible objects instead of just non-player characters (NPCs). This was boring and not worth talking about. Then, I modified the built-in Screenshot plugin so that it periodically took screenshots and saved the corresponding bounding box data. The result was some nice-looking ground truth data:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220718202731.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;A sample of the training data in its natural form: image and csv pairs&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let’s plot it:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719172210.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Visualizing the training data we obtained. Clearly it’s not what we wanted&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So now I had arrived at the problem. These bounding boxes included objects which were completely obscured from view! Just look at the Man on the top-right of the screen:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719172335.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;This guy is not visible&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This was unacceptable as ground truth training data. It would have been nice if I could ask the game for this information, since surely it knows which objects are visible. Unfortunately, I searched and searched, but I couldn’t find a way to determine if objects are visible using the Runelite API. So what can we do?&lt;/p&gt;

&lt;h2 id=&quot;approximating-whether-an-object-is-visible&quot;&gt;Approximating whether an object is visible&lt;/h2&gt;

&lt;h3 id=&quot;defining-images-and-objects&quot;&gt;Defining images and objects&lt;/h3&gt;

&lt;p&gt;Yeah, this is where things got a little weird. I wanted to take a down-and-dirty, model-based approach to filtering this bounding box data into “visible” and “not visible.” How would I build a model?&lt;/p&gt;

&lt;p&gt;Suppose we’re dealing with an $n\times m$ black-and-white image&lt;/p&gt;

\[I(x,y): \mathbb{Z}_n \times \mathbb{Z}_m \rightarrow \mathbb{Z}_{255}\]

&lt;p&gt;By definition, $I(x,y)$ tells the intensity value for pixel $(x,y)$. We know certain subsets of the pixels constitute visible objects. We assume visible objects are rectangular regions of pixels. Then there are an unknown amount, $k$, of rectangular regions of the image ${r_0,r_1,\ldots,r_p}$, where $r_i\subset \mathbb{Z}_n\times\mathbb{Z}_m$, which are visible objects.&lt;/p&gt;

&lt;h3 id=&quot;what-is-an-object-like&quot;&gt;What is an object like?&lt;/h3&gt;

&lt;p&gt;What can we assume about visible objects? One assumption is that subregions of a visible object have similar intensity histograms. Let’s look at an example from the previous images. Here we compare the distribution of intensities among the top and bottom halves of this goblin.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719211010.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Regions from the same object tend to have similar intensity histograms&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;While they’re not exactly the same, there’s certainly similarity. We can approximate the intensity similarity using histogram intersection:&lt;/p&gt;

&lt;p&gt;\(s_{intensity}(r_i, r_j) = \sum_{k=0}^{255}\text{min}(t_i^k,t_j^k)\)
Where $t_i^k$ is the quantity of pixels in region $r_i$ with intensity $k$. The similarity score of the top and bottom halves of this goblin is 398.&lt;/p&gt;

&lt;p&gt;Another similar concept is texture similarity, which uses the spatial intensity gradient instead of the raw intensity values. We can also generalize these concepts to color images.&lt;/p&gt;

&lt;h3 id=&quot;surprise-were-talking-about-region-proposal&quot;&gt;Surprise! We’re talking about region proposal!&lt;/h3&gt;

&lt;p&gt;Thinking about “what is an object” reminded me of some algorithms I saw in my computer vision class. This led me to discover the Selective Search Algorithm (SSA), which is exactly what we were looking for.&lt;/p&gt;

&lt;p&gt;Region proposal has been thoroughly studied in computer vision. According to the &lt;a href=&quot;http://www.huppelen.nl/publications/selectiveSearchDraft.pdf&quot;&gt;paper describing the SSA,&lt;/a&gt; region proposal concerns “generating possible object locations for use in object recognition.”&lt;/p&gt;

&lt;p&gt;The SSA used image segmentation along with metrics such as color similarity, intensity similarity, and texture similarity to generate a large set of regions. Region proposal algorithms tend to produce lots of false positives, as they cast a wide net and let a down-stream classifier filter these further.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719223250.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Region proposals from the original SSA paper&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Indeed, early object detectors were basically just model-based region proposer followed by a classifier. For example, the &lt;a href=&quot;https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e&quot;&gt;R-CNN&lt;/a&gt; operated in two stages (image source &lt;a href=&quot;https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;1)Generate a ton of region proposals from the image using &lt;a href=&quot;https://learnopencv.com/selective-search-for-object-detection-cpp-python/&quot;&gt;Selective Search Algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) Resize each region to some fixed dimensions and feed it into a CNN classifier&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719174426.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-visibility-model&quot;&gt;The visibility model&lt;/h3&gt;

&lt;p&gt;So, how does this relate guessing whether an object is visible given an image and the object’s bounding box? This is the “model” I settled on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;An object is visible if and only if its bounding box is (approximately) generated by Selective Search Algorithm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So I generated region proposals using SSA, and if any of these proposals had an &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Intersection over Union (IoU)&lt;/a&gt; with an object bounding box close to 1, then we call them a match and declare the object visible.&lt;/p&gt;

&lt;p&gt;That’s it! Sounds stupid, right? Well fuck off, I was pretty proud of this little trick.&lt;/p&gt;

&lt;p&gt;So does it work? See for yourself:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719174900.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Left: Objects labeled “visible” by SSA. Right: Objects labeled “invisible” by SSA&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719175132.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Left: Objects labeled “visible” by SSA. Right: Objects labeled “invisible” by SSA&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Come on, that’s not bad! Maybe it’s a little overly cautious, but I think this will filter out a nice dataset of visible objects! So that’s the trick! That’s how I created training data for visually detecting interactible objects in Runescape.&lt;/p&gt;

&lt;h2 id=&quot;training-an-object-detector&quot;&gt;Training an object detector&lt;/h2&gt;
&lt;p&gt;I used &lt;a href=&quot;https://pytorch.org/hub/ultralytics_yolov5/&quot;&gt;YOLOV5&lt;/a&gt; and trained on a bunch of training data collected from around Lumbridge area in Runescape. The results were not too bad:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719224900.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Using a trained YOLOV5 to detect interactible objects&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/runescapeobjectdetection/Pasted image 20220719225052.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Using a trained YOLOV5 to detect interactible objects&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Nobody’s calling it perfect, but it’s pretty fast and not terribly inaccurate! This thing is pretty good at guessing what objects you can click in Runescape.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;There is no conclusion. The human condition drives us forward, hurtling ourselves blindly into the unknown. I leapfrog from idea to idea, and each hop manifests ripples on the surface of the water. These are my signature, my proof that I was here. It seems you have found one of my ripples.&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html">The means justify the ends The means justify the ends</summary></entry><entry><title type="html">Egyptians and the Rhind Mathematical Papyrus: Egyptian Arithmetic (2)</title><link href="http://localhost:4000/2022/06/18/egyptianmath2.html" rel="alternate" type="text/html" title="Egyptians and the Rhind Mathematical Papyrus: Egyptian Arithmetic (2)" /><published>2022-06-18T00:00:00-04:00</published><updated>2022-06-18T00:00:00-04:00</updated><id>http://localhost:4000/2022/06/18/egyptianmath2</id><content type="html" xml:base="http://localhost:4000/2022/06/18/egyptianmath2.html">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#intro&quot; id=&quot;markdown-toc-intro&quot;&gt;Intro&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#hieroglyphic&quot; id=&quot;markdown-toc-hieroglyphic&quot;&gt;Hieroglyphic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#egyptian-numerals&quot; id=&quot;markdown-toc-egyptian-numerals&quot;&gt;Egyptian Numerals&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#adding-and-multiplying&quot; id=&quot;markdown-toc-adding-and-multiplying&quot;&gt;Adding And Multiplying&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#division-and-egyptian-fractions&quot; id=&quot;markdown-toc-division-and-egyptian-fractions&quot;&gt;Division and Egyptian Fractions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#fractions&quot; id=&quot;markdown-toc-fractions&quot;&gt;Fractions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#division&quot; id=&quot;markdown-toc-division&quot;&gt;Division&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;If you’ve read &lt;a href=&quot;https://horenbergerb.github.io/2022/05/26/egyptianmath1.html&quot;&gt;the first article in this series&lt;/a&gt;, then you may be aware of the Rhind Mathematical Papyrus. The RMP is an Egyptian scroll and one of the earliest written mathematical documents. Rather than diving straight into the RMP, this article discusses the written Egyptian language, hieroglyphic, as well as the Egyptian number system and basic Egyptian arithmetic.&lt;/p&gt;

&lt;p&gt;Arithmetic may sound like trivial material to some, but the Egyptians did not have the intellectual infrastructure we take for granted today. The Egyptian methods and notation we will discuss are over 3000 years old and were practically the first written mathematics in Egypt. They are a far cry from modern algebra.&lt;/p&gt;

&lt;h2 id=&quot;hieroglyphic&quot;&gt;Hieroglyphic&lt;/h2&gt;

&lt;p&gt;You don’t really need to know about hieroglyphs to understand Egyptian numbers, but I’m going to tell you about it anyways.&lt;/p&gt;

&lt;p&gt;This section is a criminally-reductive paraphrasing of “Middle Egyptian: An Introduction to the Language and Culture of Hieroglyphs” by James P. Allen.&lt;/p&gt;

&lt;p&gt;The first known Egyptian writing is dated around 3200BC, near the beginning of Egypt. The language survived until around 1100AD, and it has many distinct stages of life. The stage which interests us most is Middle Egyptian (2100BC-1600BC). Middle Egyptian is one of the best-understood eras of Hieroglyphic. The RMP was written in Middle Egyptian.&lt;/p&gt;

&lt;p&gt;Middle Egyptian was composed of around 500 unique hieroglyphs. Each hieroglyph was an illustration. Every hieroglyph could be used in each of three ways:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As an &lt;em&gt;ideogram&lt;/em&gt;, meaning that the hieroglyph means what it depicts. For example, a hieroglyph depicting a house, if it is used as an ideogram, is supposed to convey the concept of a house.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As a &lt;em&gt;phonogram&lt;/em&gt;, meaning that the hieroglyph is supposed to convey the sound of pronouncing the word associated with the hieroglyph’s ideogram. For example, we can communicate the word “belief” by using the following pictures as phonograms:&lt;/p&gt;

    &lt;table&gt;
      &lt;thead&gt;
        &lt;tr&gt;
          &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Philipp-Wicke-3/publication/316168050/figure/fig6/AS:484112994705409@1492432940097/Exemplary-use-of-the-rebus-principle-with-Emojis-Here-the-Emoji-Unicode-U-1F41D-for.png&quot; alt=&quot;Using phonograms to communicate spoken words in writing&quot; /&gt;&lt;/th&gt;
        &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Using phonograms to communicate spoken words in writing&lt;/em&gt;&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;As a &lt;em&gt;determinative&lt;/em&gt;, meaning that it indicates whether other hieroglyphs should be read as ideograms or phonograms. Words were made by groupings of hieroglyphs ending with a determinative that instructed you how to read them.&lt;/li&gt;
&lt;/ol&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/Relief_of_Nofer.jpg&quot; alt=&quot;Hieroglyphs on the Relief of Nofer from the Boston Museum of Fine Arts&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Hieroglyphs on the Relief of Nofer from the Boston Museum of Fine Arts&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There’s, like, a lot more to understanding hieroglyphic, but these three concepts outline the basic functionality.&lt;/p&gt;

&lt;p&gt;The Egyptians also wrote in hieratic, which was essentially the cursive version of Hieroglyph. Hieratic was more common for scrolls and bureaucratic texts, while hieroglyphic would be used for rituals and decoration in tombs and palaces.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/6/66/Scribe%27s_exercise_tablet_1.jpg&quot; alt=&quot;Hieratic writing, &amp;quot;Be on your guard against all who are subordinate to you... Trust no brother, know no friend, make no intimates.&amp;quot; Probably lyrics by an Egyptian rapper or something&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Hieratic writing, “Be on your guard against all who are subordinate to you… Trust no brother, know no friend, make no intimates.”&lt;sup&gt;&lt;a href=&quot;https://collections.mfa.org/objects/136183/relief-of-nofer?ctx=de17a16a-c1f5-4149-849e-fde75b21deea&amp;amp;idx=12&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; Probably lyrics by an Egyptian rapper or something&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;egyptian-numerals&quot;&gt;Egyptian Numerals&lt;/h2&gt;

&lt;p&gt;Before written language, it is commonly believed counting was performed through the use of tally marks.&lt;/p&gt;

&lt;p&gt;Carving notches into bones is the earliest concrete evidence of counting, possibly going as far back as 30,000BC.&lt;sup&gt;&lt;a href=&quot;https://web.archive.org/web/20111213011551/http://www.thocp.net/timeline/0000.htm&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; There is usually debate surrounding any example of tally marks in the pre-language era, something like, “you don’t really know they were counting! They could have just been goofing around!” Sure, whatever.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://web.archive.org/web/20111213011600im_/http://www.thocp.net/timeline/pictures/tally_sticks_wolvebones_30000bc.gif&quot; alt=&quot;Counting with tally marks?&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Counting?&lt;sup&gt;&lt;a href=&quot;https://web.archive.org/web/20111213011551/http://www.thocp.net/timeline/0000.htm&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It should come as no surprise, then, that the Egyptian symbol for 1 appears to be simply a tally mark, and for small numbers, they just wrote more tally marks.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2_and_3_example.png&quot; alt=&quot;Egyptian hieratic depiction of the numbers 2 and 3 from the RMP&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Egyptian hieratic depiction of the numbers 2 and 3 from the RMP&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Tally marks quickly became inconvenient for larger numbers, so new symbols were created to represent these numbers. Coincidentally, the Egyptians used a base 10 numeral system. Perhaps there is some relationship to counting on fingers?&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/hieratic_numerals.png&quot; alt=&quot;Behold, numbers&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Behold, numbers&lt;sup&gt;&lt;a href=&quot;https://www.britannica.com/science/hieratic-numeral&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Numbers were represented as groupings of these symbols. A number is equal to the sum of its substituent hieroglyphics.&lt;sup&gt;&lt;a href=&quot;https://books.google.com/books/about/A_History_of_Mathematics.html?id=h50fAQAAIAAJ&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;For example, here is the number 17 depicted in the RMP:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/17_rmp.png&quot; alt=&quot;The number 17&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The number 17&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; The Egyptians didn’t give a fuck, and they apparently invented multiple symbols to represent the same numbers. For example, the RMP often depicts 4 as a single horizontal bar (half of an 8, if you will). I haven’t found any scholarly sources discussing this. We all just have to live with it.&lt;/p&gt;

&lt;h2 id=&quot;adding-and-multiplying&quot;&gt;Adding And Multiplying&lt;/h2&gt;

&lt;p&gt;Egyptians were fairly comfortable with arithmetic, i.e. adding, multiplying, and dividing. They had somewhat standardized procedures for these operations. They did not use algebraic symbols like our $+$ and $*$ signs, so specifying the operation to be performed was done in words or intuited from the context.&lt;/p&gt;

&lt;p&gt;Adding two numbers was pretty simple to the Egyptians. You literally put them together, and if you’re feeling really generous, you can reduce it to make it prettier.&lt;/p&gt;

&lt;p&gt;Egyptian multiplication was a little more complex. Using modern algebra, you can think of it like this:&lt;/p&gt;

&lt;p&gt;Say you want to calculate $33*47$.&lt;sup&gt;&lt;a href=&quot;https://archive.org/details/rhindmathematica0000robi_h8l4&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; Calculating this directly might be annoying, so the Egyptians would break it up into easier products:&lt;/p&gt;

\[33*47 = (1 + 10 + 20 + 2)*47 =\]

\[1*47\]

\[+10*47\]

\[+20*47\]

\[+2*47\]

&lt;p&gt;If the Egyptians didn’t know how they wanted to break apart a number, they would use the base 2 decomposition:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/Robins_multiplication.png&quot; alt=&quot;Multiplication using powers of two&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Multiplication using powers of two&lt;sup&gt;&lt;a href=&quot;https://archive.org/details/rhindmathematica0000robi_h8l4&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Scholars generally regard this as “absolutely nuts,” because it’s crazy that the Egyptians figured out every number can be represented as a sum of powers of two. Some sources suggest this method of multiplication by doubling may have originated from Africa.&lt;sup&gt;&lt;a href=&quot;https://books.google.com/books/about/A_History_of_Mathematics.html?id=h50fAQAAIAAJ&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This idea of breaking apart a number into a convenient sum is &lt;em&gt;huge&lt;/em&gt; in Egyptian mathematics. It was also commonly applied to division.&lt;/p&gt;

&lt;h2 id=&quot;division-and-egyptian-fractions&quot;&gt;Division and Egyptian Fractions&lt;/h2&gt;

&lt;h3 id=&quot;fractions&quot;&gt;Fractions&lt;/h3&gt;

&lt;p&gt;Okay, this is where shit starts to get really weird. First, let’s talk about fractions. In hieratic, any number $n$ could be modified by putting a dot above it in order to represent $\frac{1}{n}$.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/1_over_8_rmp.png&quot; alt=&quot;The fraction 1/8&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The fraction 1/8&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In proper Egyptian Hieroglyphic, the eye symbol &lt;em&gt;er&lt;/em&gt; was used instead of a dot. &lt;em&gt;er&lt;/em&gt; can mean “from” or “of,” perhaps intending to convey &lt;img src=&quot;/images/one_third.png&quot; alt=&quot;one third&quot; /&gt; as “(one) of 3.”&lt;/p&gt;

&lt;p&gt;There were also, of course, special symbols for certain common fractions:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/fractions_reference.png&quot; alt=&quot;Table of common fraction symbols&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Table of common fraction symbols.&lt;sup&gt;&lt;a href=&quot;https://en.wikisource.org/wiki/A_History_Of_Mathematical_Notations/Volume_1/Egyptians&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The way that Egyptians used these fractions was weird. They preferred not to repeat fractions. Depicting $\frac{2}{5}$ as &lt;img src=&quot;/images/one_fifth.png&quot; alt=&quot;one fifth&quot; /&gt; &lt;img src=&quot;/images/one_fifth.png&quot; alt=&quot;one fifth&quot; /&gt; was considered “unfinished.” One was expected to instead calculate (or look up) a sum of unique unit fractions equal to $\frac{2}{5}$.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2_over_n_table_5_rmp.png&quot; alt=&quot;The 2nd row of the RMP's 2/n table calculates 2/5 as a sum of unique unit fractions&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The 2nd row of the RMP’s 2/n table calculates $\frac{2}{5}=\frac{1}{3}+\frac{1}{15}$&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In fact, the Rhind Mathematical Papyrus begins with a massive table called the “2/n table” which calculates a unique sum of unit fractions for $\frac{2}{n}$ starting with $n=3$ and going all the way to $n=101$! That’s bonkers.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/2_over_n_table_column_one_rmp.png&quot; alt=&quot;The first column of the 2/n table covering n=3 through n=15&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;The first column of the 2/n table covering n=3 through n=15. The $n$ value for each row appears on the right-hand side. Values in red denote the sums of unit fractions.&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;It’s not clear why the Egyptians felt the need to represent vulgar fractions as the sum of unique unit fractions. One hypothesis is that the common procedure for dividing (ex. splitting 2 loaves of bread equally among 5 men) made these sums natural and obvious to Egyptians. We’ll discuss this in more detail in the next article of the series.&lt;/p&gt;

&lt;h3 id=&quot;division&quot;&gt;Division&lt;/h3&gt;

&lt;p&gt;Alright, I’m running out of steam here, so let’s wrap this up. Division was a mess. I am, once again, going to pull an example straight out of Gay Robins’ &lt;em&gt;The Rhind Mathematical Papyrus&lt;/em&gt;. To divide 47 by 33, the Egyptians would, in their own words, “treat 33 so as to obtain 47.” For example, they might simply observe that&lt;/p&gt;

\[47 = 1*33 + \frac{1}{3}*33 + \frac{1}{11}*33 \implies\]

\[47 = (1+\frac{1}{3}+\frac{1}{11})33 \implies\]

\[\frac{47}{33} = 1+\frac{1}{3}+\frac{1}{11}\]

&lt;p&gt;In this case, we obviously got very lucky with our divisors of 33, so the solution was “easy.” If we instead divide 33 by 47, things get nasty. The Egyptians would again break 47 into a convenient sum and just go to town:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;/images/robins_division.png&quot; alt=&quot;Dividing 33 by 47 like an Egyptian&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Dividing 33 by 47 like an Egyptian&lt;sup&gt;&lt;a href=&quot;https://archive.org/details/rhindmathematica0000robi_h8l4&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Note that the 3 with a double bar above it represents $\frac{2}{3}$. Although $\frac{2}{3}$ isn’t a unit fraction, it apparently didn’t bother the Egyptians, and they frequently included it in their sums.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;So now you’ve got the basic idea of how Egyptian arithmetic was performed. With the “how” out of the way, you are free to consider the “why?”. Why did the Egyptians prefer sums of nonrepeating unit fractions? Why did they use powers of 2 when multiplying? I dunno.&lt;/p&gt;

&lt;p&gt;Egyptian mathematics is one of the first well-documented mathematical cultures in human history. We take for granted our sleek mathematical concepts and notations, and we often consider our ways to be obvious or “natural.” Effective, unambiguous fraction notation didn’t pop up until at least 500AD, thousands of years after the beginning of written math.&lt;sup&gt;&lt;a href=&quot;https://archive.org/details/rhindmathematica0000robi_h8l4&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; These things are apparently not intuitive until after the fact. Perhaps we should treat the fundamentals of modern mathematics with a little more respect, or perhaps with a little more skepticism…&lt;/p&gt;</content><author><name>Beau Horenberger</name></author><category term="blog" /><summary type="html"></summary></entry></feed>